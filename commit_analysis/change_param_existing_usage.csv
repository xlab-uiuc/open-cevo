Issue-id,Title,Parameter,Issue-URL,Commit-URL,Type,Note
SPARK-21871,[SQL] Check actual bytecode size when compiling generated code,spark.sql.codegen.hugeMethodLimit,https://issues.apache.org/jira/browse/SPARK-21871,https://github.com/apache/spark/commit/4a779bdac3e75c17b7d36c5a009ba6c948fa9fb6,change functional usage,change maximum lines -> maximum bytecode size
SPARK-22062,[CORE] Spill large block to disk in BlockManager's remote fetch to avoid OOM,spark.maxRemoteBlockSizeFetchToMem,https://issues.apache.org/jira/browse/SPARK-22062,https://github.com/apache/spark/commit/e1960c3d6f380b0dfbba6ee5d8ac6da4bc29a698,change functional usage,"So here leveraging the idea of shuffle fetch, to spill the large block to local disk before consumed by upstream code. The behavior is controlled by newly added configuration, if block size is smaller than the threshold, then this block will be persisted in memory; otherwise it will first spill to disk, and then read from disk file."
SPARK-21501,Change CacheLoader to limit entries based on memory footprint,spark.shuffle.service.index.cache.entries,https://issues.apache.org/jira/browse/SPARK-21501,https://github.com/apache/spark/commit/1662e93119d68498942386906de309d35f4a135f,change functional usage,We should change this cache to be memory based and only allow a certain memory size used. When I say memory based I mean the cache should have a limit of say 100MB.
SPARK-27835,Resource Scheduling: change driver config from addresses,spark.driver.resourcesFile,https://issues.apache.org/jira/browse/SPARK-27835,https://github.com/apache/spark/commit/6748b486a9afe8370786efb64a8c9f3470c62dcf#,change functional usage,"Change the Driver resource discovery argument for standalone mode to be a file rather then separate address configs per resource. This makes it consistent with how the Executor is doing it and makes it more flexible in the future, and it makes for less configs if you have multiple resources."
CASSANDRA-14297,Startup checker should wait for count rather than percentage,block_for_peers_percentage,https://issues.apache.org/jira/browse/CASSANDRA-14297,https://github.com/apache/cassandra/commit/801cb70ee811c956e987718a00695638d5bec1b6#,change functional usage,"I proposed in that ticket instead of having `block_for_peers_percentage` defaulting to 70%, we instead have `block_for_peers` as a count of nodes that are allowed to be down before the starting node makes itself available as a coordinator."
HBASE-22803,Modify CONFIG value range to enable turning off of the hb……ck chore (#466),hbase.master.hbck.chore.interval,https://issues.apache.org/jira/browse/HBASE-22580,https://github.com/apache/hbase/commit/eb92b2565254a2783b8a6bfbb043cf7d01ab7ad,change functional usage,Let's add a config to turn it off so that operators can use it and do rolling restart when they want to stop this chore.
HDFS-13653,Make dfs.client.failover.random.order a per nameservice configuration,dfs.client.failover.random.order,https://issues.apache.org/jira/browse/HDFS-13653,https://github.com/apache/hadoop/commit/2e5cfe6df338c70965cfb0212a93617de3a6bd79,change param domain,This Jira is to make this configuration per-nameservice so that this can be configured independently for each nameservice.
HDFS-13852,RBF: The DN_REPORT_TIME_OUT and DN_REPORT_CACHE_EXPIRE should be configured in RBFConfigKeys.,DN_REPORT_TIME_OUT,https://issues.apache.org/jira/browse/HDFS-13852,https://github.com/apache/hadoop/commit/04caaba4884cdea9f3b97f819fe6599ab3d6f151,change param domain,"In the NamenodeBeanMetrics the router will invokes 'getDataNodeReport' periodically. And we can set the dfs.federation.router.dn-report.time-out and dfs.federation.router.dn-report.cache-expire to avoid time out. But when we start the router, the FederationMetrics will also invoke the method to get node usage. If time out error happened, we cannot adjust the parameter time_out. And the time_out in the FederationMetrics and NamenodeBeanMetrics should be the same."
HDFS-13852,RBF: The DN_REPORT_TIME_OUT and DN_REPORT_CACHE_EXPIRE should be configured in RBFConfigKeys.,DN_REPORT_CACHE_EXPIRE,https://issues.apache.org/jira/browse/HDFS-13852,https://github.com/apache/hadoop/commit/04caaba4884cdea9f3b97f819fe6599ab3d6f151,change param domain,"In the NamenodeBeanMetrics the router will invokes 'getDataNodeReport' periodically. And we can set the dfs.federation.router.dn-report.time-out and dfs.federation.router.dn-report.cache-expire to avoid time out. But when we start the router, the FederationMetrics will also invoke the method to get node usage. If time out error happened, we cannot adjust the parameter time_out. And the time_out in the FederationMetrics and NamenodeBeanMetrics should be the same."
HDFS-9388,Decommission related code to support Maintenance State for datanodes,dfs.namenode.decommission.interval,https://issues.apache.org/jira/browse/HDFS-9388,https://github.com/apache/hadoop/commit/79df1e750ef558afed6d166ce225a23061b36aed,change param domain,"Lots of code can be shared between the existing decommission functionality and to-be-added maintenance state support for datanodes. To make it easier to add maintenance state support, let us first modify the existing code to make it more general."
HDFS-9388,Decommission related code to support Maintenance State for datanodes,dfs.namenode.decommission.blocks.per.interval,https://issues.apache.org/jira/browse/HDFS-9388,https://github.com/apache/hadoop/commit/79df1e750ef558afed6d166ce225a23061b36aed,change param domain,"In the NamenodeBeanMetrics the router will invokes 'getDataNodeReport' periodically. And we can set the dfs.federation.router.dn-report.time-out and dfs.federation.router.dn-report.cache-expire to avoid time out. But when we start the router, the FederationMetrics will also invoke the method to get node usage. If time out error happened, we cannot adjust the parameter time_out. And the time_out in the FederationMetrics and NamenodeBeanMetrics should be the same."
HDFS-9388,Decommission related code to support Maintenance State for datanodes,dfs.namenode.decommission.max.concurrent.tracked.nodes,https://issues.apache.org/jira/browse/HDFS-9388,https://github.com/apache/hadoop/commit/79df1e750ef558afed6d166ce225a23061b36aed,change param domain,"In the NamenodeBeanMetrics the router will invokes 'getDataNodeReport' periodically. And we can set the dfs.federation.router.dn-report.time-out and dfs.federation.router.dn-report.cache-expire to avoid time out. But when we start the router, the FederationMetrics will also invoke the method to get node usage. If time out error happened, we cannot adjust the parameter time_out. And the time_out in the FederationMetrics and NamenodeBeanMetrics should be the same."
SPARK-24182,[YARN] Improve error message when client AM fails.,spark.yarn.am.waitTime,https://issues.apache.org/jira/browse/SPARK-24182,https://github.com/apache/spark/commit/54032682b910dc5089af27d2c7b6efe55700f034,change param domain,Only used in <code>cluster</code> mode.
CASSANDRA-14303,Auto-expand replication_factor for NetworkTopologyStrategy,replication_factor,https://issues.apache.org/jira/browse/CASSANDRA-14303,https://github.com/apache/cassandra/commit/1f19d5f7a243cc4227da923459f5eb2f66066778,change param domain,This re-defines the 'replication_factor' configuration option for NetworkTopologyStrategy to auto-expand out to the datacenters that the cluster knows about.
SPARK-26066,Moving truncatedString to sql/catalyst,spark.sql.debug.maxToStringFields,https://issues.apache.org/jira/browse/SPARK-26066,https://github.com/apache/spark/commit/81550b38e43fb20f89f529d2127575c71a54a538#,change param domain,Moving `truncatedString` out of `core` to `sql/catalyst` because it is used only in the `sql/catalyst` packages for restricting number of fields converted to strings from `TreeNode` and expressions of`StructType`.
SPARK-28205,useV1SourceList configuration should be for all data sources,spark.sql.sources.read.useV1SourceList,https://issues.apache.org/jira/browse/SPARK-28205,https://github.com/apache/spark/commit/3ae531ebb92a1feb1500c12ae97b8d24493354c,change param domain,"We find that the useV1SourceList configuration(spark.sql.sources.read.useV1SourceList and spark.sql.sources.write.useV1SourceList) should be for all data sources, instead of file source V2 only."
SPARK-28395,Make spark.sql.function.preferIntegralDivision internal,spark.sql.function.preferIntegralDivision,https://issues.apache.org/jira/browse/SPARK-28395,https://github.com/apache/spark/commit/3586cdd24d9f5cb7d3f642a3da6a26ced1f88ce,change param domain,This PR makes `spark.sql.function.preferIntegralDivision` to internal configuration because it is only used for PostgreSQL test cases.
HBASE-18452,VerifyReplication by Snapshot should cache HDFS token before submit job for kerberos env,MRJobConfig.JOB_NAMENODES,https://issues.apache.org/jira/browse/HBASE-18452,https://github.com/apache/hbase/commit/51954359416b107ce5eda6cb710449edc98ab0e6,change value use,"After check the patch, Seems like we did not obtain the namenode token before submit the job. The feature works fine in our kerberos cluster after applied following patch:"
SPARK-26255,Apply user provided UI filters to SQL tab in yarn  mode,spark.ui.filters,https://issues.apache.org/jira/browse/SPARK-26255,https://github.com/apache/spark/commit/6d45e6ea1507943f6ee833af8ad7969294b0356a,change value use,"User specified filters are not applied to SQL tab in yarn mode, as it is overridden by the yarn AmIp filter. So we need to append user provided filters (spark.ui.filters) with yarn filter."
HDFS-14806,bootstrap standby may fail if used in-progress tailing,DFS_HA_TAILEDITS_INPROGRESS_KEY,https://issues.apache.org/jira/browse/HDFS-14806,https://github.com/apache/hadoop/commit/9d0d580031006ca6db9b4150f17ab678ce68a25,change value use,Disable using the RPC tailing mechanism for bootstrapping the standby since it is less efficient in this case
HBASE-19505,Disable ByteBufferPool by default at HM.,hbase.ipc.server.reservoir.enabled,https://issues.apache.org/jira/browse/HBASE-19505,https://github.com/apache/hbase/commit/018a65b947a7e78fb98a190d75787f50d42d0711,change value use,"The main usage of the pool is while accepting bigger sized requests ie. Mutation requests. HM do not have any regions by default. So we can make this pool OFF in HM side. Still add a config to turn this ON. If HM carry only system table or nothing, disabling the pool is ok I think. If HM can carry any regions (hbase.balancer.tablesOnMaster == true), the pool setting for HM should be same with RS."
HDFS-9914,Fix configurable WebhDFS connect/read timeout,DFS_WEBHDFS_SOCKET_CONNECT_TIMEOUT_KEY,https://issues.apache.org/jira/browse/HDFS-9914,https://github.com/apache/hadoop/commit/cd2a1dd0630e99e6696a9f0e8dc972604d38046a,param overriden,URLConnectionFactory#getSSLConnectionConfiguration() should honor webhdfs connect/read timeout even if any exception is thrown during customized SSL configuration.
HDFS-9914,Fix configurable WebhDFS connect/read timeout,DFS_WEBHDFS_SOCKET_READ_TIMEOUT_KEY,https://issues.apache.org/jira/browse/HDFS-9914,https://github.com/apache/hadoop/commit/cd2a1dd0630e99e6696a9f0e8dc972604d38046a,param overriden,URLConnectionFactory#getSSLConnectionConfiguration() should honor webhdfs connect/read timeout even if any exception is thrown during customized SSL configuration.
HDFS-12082,BlockInvalidateLimit value is incorrectly set after namenode heartbeat interval reconfigured.,dfs.block.invalidate.limit,https://issues.apache.org/jira/browse/HDFS-12082,https://github.com/apache/hadoop/commit/3e23415a92d43ce8818124f0b180227a52a33eaf,param overriden,"Proposed a patch to fix this issue. Otherwise if user reconfigures namenode interval, the value of property dfs.block.invalidate.limit will be always overwritten."
HBASE-22322,Use special pause for CallQueueTooBigException,HBASE_CLIENT_PAUSE_FOR_CQTBE,https://issues.apache.org/jira/browse/HBASE-22322,https://github.com/apache/hbase/commit/f9f63543933616ca887fa2dc954dfd7e649d0461,param overriden,"Use special pause for CallQueueTooBigException The {} setting: {} ms is less than the {} setting: {} ms, use the greater one instead"
SPARK-21786,"SQL] When acquiring 'compressionCodecClassName' in 'ParquetOptions', `parquet.compression` needs to be considered.",spark.sql.parquet.compression.codec,https://issues.apache.org/jira/browse/SPARK-21786,https://github.com/apache/spark/commit/7b78041423b6ee330def2336dfd1ff9ae8469c59,param overriden,"In Spark side, our table-level compression conf compression was added by #11464 since Spark 2.0. We need to support both table-level conf. Users might also use session-level conf spark.sql.parquet.compression.codec. The priority rule will be like"
SPARK-24860,[SQL] Support setting of partitionOverWriteMode in output options for writing DataFrame,sparkSession.sessionState.conf.partitionOverwriteMode,https://issues.apache.org/jira/browse/SPARK-24860,https://github.com/apache/spark/commit/17f469bc808e076b45fffcedb0147991fa4c41f3,param overriden,Besides spark setting spark.sql.sources.partitionOverwriteMode also allow setting partitionOverWriteMode per write
SPARK-24566,[CORE] Fix spark.storage.blockManagerSlaveTimeoutMs default CONFIG,spark.network.timeout,https://issues.apache.org/jira/browse/SPARK-24566,https://github.com/apache/spark/commit/f71e8da5efde96aacc89e59c6e27b71fffcbc25f,param overriden,This PR use spark.network.timeout in place of spark.storage.blockManagerSlaveTimeoutMs when it is not configured
SPARK-28369,Honor spark.sql.decimalOperations.nullOnOverflow in ScalaUDF result,spark.sql.decimalOperations.nullOnOverflow,https://issues.apache.org/jira/browse/SPARK-28369,https://github.com/apache/spark/commit/a783690d8a50fe42ae2ed66269814b16f0ec8d1,param overriden,"When a `ScalaUDF` returns a value which overflows, currently it returns null regardless of the value of the config `spark.sql.decimalOperations.nullOnOverflow`."
SPARK-28470,Cast to decimal throws ArithmeticException on overflow,spark.sql.decimalOperations.nullOnOverflow,https://issues.apache.org/jira/browse/SPARK-28470,https://github.com/apache/spark/commit/8617bf6ff8c1a2021bc0132b5f668aaef8aed89,param overriden,"cast long to decimal or decimal to decimal can overflow, we should respect the new config if overflow happens."
SPARK-28928,Use Kafka delegation token protocol on sources/sinks,kafka.security.protocol,https://issues.apache.org/jira/browse/SPARK-28928,https://github.com/apache/spark/commit/e516f7e09e2bfb064376370b00fcf7a0ca91218,param overriden,This default configuration can be overwritten on each source/sink independently by using `kafka.security.protocol` configuration.
SPARK-22282,[SQL] Rename OrcRelation to OrcFileFormat and remove ORC_COMPRESSION,OrcConf.COMPRESS OrcRelation.ORC_COMPRESSION,https://issues.apache.org/jira/browse/SPARK-22282,https://github.com/apache/spark/commit/561505e2fc290fc2cee3b8464ec49df773dca5eb,param replacement,"Replace `OrcRelation.ORC_COMPRESSION` with `org.apache.orc.OrcConf.COMPRESS`. Since [SPARK-21422](https://issues.apache.org/jira/browse/SPARK-21422), we can use `OrcConf.COMPRESS` instead of Hive's."
HBASE-22810,Initialize an separate ThreadPoolExecutor for taking/restoring snapshot,hbase.master.executor.snapshot.threads hbase.snapshot.master.threads,https://issues.apache.org/jira/browse/HBASE-22810,https://github.com/apache/hbase/commit/84ee378df1e96dcadd8e35c02ee66d9ee6c1706,param replacement (combine 2 to 1),"The config key#1 limit the all the snapshot request, while the key#2 only limit the snapshot procedure with RS ( it's a part of the snapshot request). Maybe we can uniform the two config keys into one ? although we will initialize two different thread pools with the same thread size for different purpose."
SPARK-28989,Add a SQLConf `spark.sql.ansi.enabled`,spark.sql.ansi.enabled,https://issues.apache.org/jira/browse/SPARK-28989,https://github.com/apache/spark/commit/b917a6593dc969b9b766259eb8cbbd6e90e0dc5,param replacement (combine 3 to 1),"This PR is to add new configuration `spark.sql.ansi.enabled` and remove the 3 options above. When the configuration is true, Spark tries to conform to the ANSI SQL specification. It will be disabled by default."
SPARK-24367,SQL] Parquet: use JOB_SUMMARY_LEVEL instead of deprecated flag ENABLE_JOB_SUMMARY,ENABLE_JOB_SUMMARY OB_SUMMARY_LEVEL,https://issues.apache.org/jira/browse/SPARK-24367,https://github.com/apache/spark/commit/3b20b34ab72c92d9d20188ed430955e1a94eac9c,param replacement (one is deprecated),"In current parquet version,the conf ENABLE_JOB_SUMMARY is deprecated."
SPARK-21673,Use the correct sandbox environment variable set by Mesos,MESOS_DIRECTORY ESOS_SANDBOX,https://issues.apache.org/jira/browse/SPARK-21673,https://github.com/apache/spark/commit/a4470bc78ca5f5a090b6831a7cdca88274eb9afc,param replacement (use wrong parameter),This change changes spark behavior to use the correct environment variable set by Mesos in the container on startup.
SPARK-10365,[SQL] Support Parquet logical type TIMESTAMP_MICROS,PARQUET_OUTPUT_TIMESTAMP_TYPE.key,https://issues.apache.org/jira/browse/SPARK-10365,https://github.com/apache/spark/commit/21a7bfd5c324e6c82152229f1394f26afeae771c,param replacement (use wrong parameter),"Deprecated since Spark 2.3, please set ${PARQUET_OUTPUT_TIMESTAMP_TYPE.key"
SPARK-22131,[MESOS] Mesos driver secrets,spark.mesos.$taskType.secret.names,https://issues.apache.org/jira/browse/SPARK-22131,https://github.com/apache/spark/commit/5415963d2caaf95604211419ffc4e29fff38e1d7,param seperation,We recently added Secrets support to the Dispatcher (SPARK-20812). In order to have Driver-to-Executor TLS we need the same support in the Mesos Driver so a secret can be disseminated to the executors. This JIRA is to move the current secrets implementation to be used by both frameworks.
SPARK-22131,[MESOS] Mesos driver secrets,spark.mesos.$taskType.secret.values,https://issues.apache.org/jira/browse/SPARK-22131,https://github.com/apache/spark/commit/5415963d2caaf95604211419ffc4e29fff38e1d7,param seperation,We recently added Secrets support to the Dispatcher (SPARK-20812). In order to have Driver-to-Executor TLS we need the same support in the Mesos Driver so a secret can be disseminated to the executors. This JIRA is to move the current secrets implementation to be used by both frameworks.
SPARK-22131,[MESOS] Mesos driver secrets,spark.mesos.$taskType.secret.envkeys,https://issues.apache.org/jira/browse/SPARK-22131,https://github.com/apache/spark/commit/5415963d2caaf95604211419ffc4e29fff38e1d7,param seperation,We recently added Secrets support to the Dispatcher (SPARK-20812). In order to have Driver-to-Executor TLS we need the same support in the Mesos Driver so a secret can be disseminated to the executors. This JIRA is to move the current secrets implementation to be used by both frameworks.
SPARK-22131,[MESOS] Mesos driver secrets,spark.mesos.$taskType.secret.filenames,https://issues.apache.org/jira/browse/SPARK-22131,https://github.com/apache/spark/commit/5415963d2caaf95604211419ffc4e29fff38e1d7,param seperation,We recently added Secrets support to the Dispatcher (SPARK-20812). In order to have Driver-to-Executor TLS we need the same support in the Mesos Driver so a secret can be disseminated to the executors. This JIRA is to move the current secrets implementation to be used by both frameworks.
SPARK-26632,Separate Thread Configurations of Driver and Executor,spark.executor.rpc.netty.dispatcher.numThreads spark.driver.rpc.netty.dispatcher.numThreads,https://issues.apache.org/jira/browse/SPARK-26632,https://github.com/apache/spark/commit/fa5dc0a45a414c34b31c5d7efe396aa04f1e66e3#,param seperation,"From the test, we find driver and executor should have different thread configurations because driver has far more RPC messages than single executor"
SPARK-26632,Separate Thread Configurations of Driver and Executor,spark.driver.rpc.io.serverThreads spark.executor.rpc.io.serverThreads,https://issues.apache.org/jira/browse/SPARK-26632,https://github.com/apache/spark/commit/fa5dc0a45a414c34b31c5d7efe396aa04f1e66e3#,param seperation,"From the test, we find driver and executor should have different thread configurations because driver has far more RPC messages than single executor"
SPARK-26632,Separate Thread Configurations of Driver and Executor,spark.executor.rpc.io.clientThreads spark.driver.rpc.io.clientThreads,https://issues.apache.org/jira/browse/SPARK-26632,https://github.com/apache/spark/commit/fa5dc0a45a414c34b31c5d7efe396aa04f1e66e3#,param seperation,"From the test, we find driver and executor should have different thread configurations because driver has far more RPC messages than single executor"
SPARK-27083,Add a new conf to control subqueryReuse,spark.sql.subquery.reuse,https://issues.apache.org/jira/browse/SPARK-27083,https://github.com/apache/spark/commit/fac31104f69daa6cf72eae1a0de995ef0777d75c#,param seperation,Subquery Reuse and Exchange Reuse are not the same feature， if we don't want to reuse subqueries，and we just want to reuse exchanges，only one configuration that cannot be done.
SPARK-27834,Make separate PySpark/SparkR vectorization configurations,spark.sql.execution.arrow.pyspark.enabled spark.sql.execution.arrow.sparkr.enabled,https://issues.apache.org/jira/browse/SPARK-27834,https://github.com/apache/spark/commit/db48da87f02e2e89710ba65fab8b07e9c85b9e74#,param seperation,"Later, in the current master, SparkR arrow optimization was added and it's controlled by the same configuration `spark.sql.execution.arrow.enabled`."
HBASE-20499,Replication/Priority executors can use specific max queue length as default value instead of general maxQueueLength,hbase.ipc.server.replication.max.callqueue.length,https://issues.apache.org/jira/browse/HBASE-20499,https://github.com/apache/hbase/commit/6d080762ef795adf02dd0ab236c4b3eb73e19a91#,param seperation,Replication/Priority executors can use specific max queue length as default value instead of general maxQueueLength
HBASE-20986,Separate the CONFIG of block size when we do log splitting and write Hlog,hbase.regionserver.hlog.blocksize hbase.regionserver.recoverededits.blocksize,https://issues.apache.org/jira/browse/HBASE-20986,https://github.com/apache/hbase/commit/bd01fa763985491e196e296f0f627274ce04e619#,param seperation,Separate the CONFIG of block size when we do log splitting and write Hlog
HBASE-22596,Separate the execution period between CompactionChecker and PeriodicMemStoreFlusher,hbase.regionserver.compaction.check.period hbase.regionserver.flush.check.period,https://issues.apache.org/jira/browse/HBASE-22596,https://github.com/apache/hbase/commit/c7c6140396528f8f9d4dff43035a516e7ba2f22a#,param seperation,Separate the execution period between CompactionChecker and PeriodicMemStoreFlusher
HDFS-13480,RBF: Separate namenodeHeartbeat and routerHeartbeat to different config key,DFS_ROUTER_HEARTBEAT_ENABLE DFS_ROUTER_NAMENODE_HEARTBEAT_ENABLE,https://issues.apache.org/jira/browse/HDFS-13480,https://github.com/apache/hadoop/commit/6915d7e13c2afbb2738176ba55ea0774f25e1264#,param seperation,RBF: Separate namenodeHeartbeat and routerHeartbeat to different config key
CASSANDRA-7544,Allow storage port to be configurable per node,storage_port,https://issues.apache.org/jira/browse/CASSANDRA-7544,https://github.com/apache/cassandra/commit/59b5b6bef0fa76bf5740b688fcd4d9cf525760d0#,param seperation,Currently storage_port must be configured identically on all nodes in a cluster and it is assumed that this is the case when connecting to a remote node.
SPARK-28574,Allow to config different sizes for event queues,spark.scheduler.listenerbus.eventqueue.${name}.capacity,https://issues.apache.org/jira/browse/SPARK-28574,https://github.com/apache/spark/commit/c212c9d9ed7375cd1ea16c118733edd84037ec0,param seperation,Add configuration spark.scheduler.listenerbus.eventqueue.${name}.capacity to allow configuration of different event queue size.
HBASE-22580,Add a table attribute to make user scan snapshot feature ……configurable for table (#336),hbase.user.scan.snapshot.enable,https://issues.apache.org/jira/browse/HBASE-22580,https://github.com/apache/hbase/commit/7294af8b1514d8bf8a2ddb8211be310b9c0e8f5,param seperation,"If a cluster enable user scan snapshot feature, it will work for all tables. Since this feature will make some operations such as grant, revoke, snapshot... slower and some tables don't use scan snaphot to make scan faster. So add a table attribute to make it configurable at table level, in general, the feature is disabled by default, and if someone use feature, must enable the attribute of the specific table firstly."
HBASE-22280,Separate read/write handler for priority request(especially for meta).,hbase.ipc.server.metacallqueue.read.ratio hbase.ipc.server.metacallqueue.scan.ratio,https://issues.apache.org/jira/browse/HBASE-22280,https://github.com/apache/hbase/commit/dff7fffe8537427cb9ed1c0dceefd2a1c30a1a8,param seperation,Separate read/write handler for priority request(especially for meta).