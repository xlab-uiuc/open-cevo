Issue-ID,Title,Parameter,Issue-URL,Commit-URL,Change type,Impact,Checking content,Handling action,Message leave,Note
SPARK-22962,Fail fast if submission client local files are used,spark.jars,https://issues.apache.org/jira/browse/SPARK-22962,https://github.com/apache/spark/commit/5d7c4ba4d73a72f26d591108db3c20b4a6c84f3f,add checking code,early termination,semantic check: local files are used,throw SparkException with message,/,"If you try to start a Spark app on kubernetes using a local file as the app resource, for example, it will fail. The k8s backend should either somehow make these files available to the cluster or error out with a more user-friendly message if that feature is not yet available."
SPARK-22962,Fail fast if submission client local files are used,spark.files,https://issues.apache.org/jira/browse/SPARK-22962,https://github.com/apache/spark/commit/5d7c4ba4d73a72f26d591108db3c20b4a6c84f3f,add checking code,early termination,semantic check: local files are used,throw SparkException with message,/,"If you try to start a Spark app on kubernetes using a local file as the app resource, for example, it will fail The k8s backend should either somehow make these files available to the cluster or error out with a more user-friendly message if that feature is not yet available."
CASSANDRA-13669,Validate supported column type with SASI analyzer,INDEX_ANALYZER_CLASS_OPTION,https://issues.apache.org/jira/browse/CASSANDRA-13669,https://github.com/apache/cassandra/commit/ea62d8862c311e3d9b64d622bea0a68d3825aa7d,add checking code,early termination: can't restart Cassandra,semantic check: whether use incompatible type,throw ConfigurationException(didn't catch) with message,/,Error when starting cassandra: Unable to make UUID from 'aa'  One way to avoid this error is to delete faulty index before stopping a node.
CASSANDRA-9375,force minumum timeout value,request_timeout_in_ms,https://issues.apache.org/jira/browse/CASSANDRA-9375,https://github.com/apache/cassandra/commit/d2dcd7f884cc997905c820d7cef8c9fc886ff4f7,add checking code,early termination: Exception encountered during startup,single value range check: < LOWEST_ACCEPTED_TIMEOUT,"auto-correction: change to the boundary value, with log message",/,"Granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong I added small logic to check lowest acceptable timeouts. Also, it might be nice to log if any of the operator's value's were too low, and we adjusted it up to the LOWEST_ACCEPTED_TIMEOUT."
CASSANDRA-9375,force minumum timeout value,read_request_timeout_in_ms,https://issues.apache.org/jira/browse/CASSANDRA-9375,https://github.com/apache/cassandra/commit/d2dcd7f884cc997905c820d7cef8c9fc886ff4f7,add checking code,early termination: Exception encountered during startup,single value range check: < LOWEST_ACCEPTED_TIMEOUT,"auto-correction: change to the boundary value, with log message",/,"Granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong I added small logic to check lowest acceptable timeouts. Also, it might be nice to log if any of the operator's value's were too low, and we adjusted it up to the LOWEST_ACCEPTED_TIMEOUT."
CASSANDRA-9375,force minumum timeout value,range_request_timeout_in_ms,https://issues.apache.org/jira/browse/CASSANDRA-9375,https://github.com/apache/cassandra/commit/d2dcd7f884cc997905c820d7cef8c9fc886ff4f7,add checking code,early termination: Exception encountered during startup,single value range check: < LOWEST_ACCEPTED_TIMEOUT,"auto-correction: change to the boundary value, with log message",/,"Granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong I added small logic to check lowest acceptable timeouts. Also, it might be nice to log if any of the operator's value's were too low, and we adjusted it up to the LOWEST_ACCEPTED_TIMEOUT."
CASSANDRA-9375,force minumum timeout value,write_request_timeout_in_ms,https://issues.apache.org/jira/browse/CASSANDRA-9375,https://github.com/apache/cassandra/commit/d2dcd7f884cc997905c820d7cef8c9fc886ff4f7,add checking code,early termination: Exception encountered during startup,single value range check: < LOWEST_ACCEPTED_TIMEOUT,"auto-correction: change to the boundary value, with log message",/,"Granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong I added small logic to check lowest acceptable timeouts. Also, it might be nice to log if any of the operator's value's were too low, and we adjusted it up to the LOWEST_ACCEPTED_TIMEOUT."
CASSANDRA-9375,force minumum timeout value,counter_write_request_timeout_in_ms,https://issues.apache.org/jira/browse/CASSANDRA-9375,https://github.com/apache/cassandra/commit/d2dcd7f884cc997905c820d7cef8c9fc886ff4f7,add checking code,early termination: Exception encountered during startup,single value range check: < LOWEST_ACCEPTED_TIMEOUT,"auto-correction: change to the boundary value, with log message",/,"Granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong I added small logic to check lowest acceptable timeouts. Also, it might be nice to log if any of the operator's value's were too low, and we adjusted it up to the LOWEST_ACCEPTED_TIMEOUT."
CASSANDRA-9375,force minumum timeout value,cas_contention_timeout_in_ms,https://issues.apache.org/jira/browse/CASSANDRA-9375,https://github.com/apache/cassandra/commit/d2dcd7f884cc997905c820d7cef8c9fc886ff4f7,add checking code,early termination: Exception encountered during startup,single value range check: < LOWEST_ACCEPTED_TIMEOUT,"auto-correction: change to the boundary value, with log message",/,"Granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong I added small logic to check lowest acceptable timeouts. Also, it might be nice to log if any of the operator's value's were too low, and we adjusted it up to the LOWEST_ACCEPTED_TIMEOUT."
CASSANDRA-9375,force minumum timeout value,truncate_request_timeout_in_ms,https://issues.apache.org/jira/browse/CASSANDRA-9375,https://github.com/apache/cassandra/commit/d2dcd7f884cc997905c820d7cef8c9fc886ff4f7,add checking code,early termination: Exception encountered during startup,single value range check: < LOWEST_ACCEPTED_TIMEOUT,"auto-correction: change to the boundary value, with log message",/,"Granted, this is a nonsensical setting, but the error message makes it tough to discern what's wrong I added small logic to check lowest acceptable timeouts. Also, it might be nice to log if any of the operator's value's were too low, and we adjusted it up to the LOWEST_ACCEPTED_TIMEOUT."
CASSANDRA-13622,Improve config validation and documentation on overflow and NPE,conf.commitlog_segment_size_in_mb,https://issues.apache.org/jira/browse/CASSANDRA-13622,https://github.com/apache/cassandra/commit/a586f6c88dab173663b765261d084ed8410efe81#,add checking code,early termination: Exiting due to error while processing commit log during initialization.,single value range check: >= 2048,throw ConfigurationException(didn't catch) with message,/,"Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user."
HBASE-18161,Incremental Load support for Multiple-Table HFileOutputFormat,hbase.mapreduce.hfileoutputformat.table.name,https://issues.apache.org/jira/browse/HBASE-18161,https://github.com/apache/hbase/commit/293cb87d52d6ccc98f0d03387f9dc07dc4522042,add checking code,give reason,not null check: .isEmpty,throw IllegalArgumentException with message,/,"The configuration parameter ""hbase.mapreduce.hfileoutputformat.table.name"" is now a REQUIRED parameter though it is normally set automatically when configureIncrementalLoad method is called within HFileOutputFormat2"
HDFS-13275,Adding log for BlockPoolManager#refreshNamenodes failures,if (newAddressMap == null || newAddressMap.isEmpty(),https://issues.apache.org/jira/browse/HDFS-13275,https://github.com/apache/hadoop/commit/0be0f1ce1da984ff3a8fd4b079297ead2dc6722d,add checking code,give reason,not null check: .isEmpty,throw IOException with message,/,This helps detect and troubleshooting misconfigured NN service RPC or NN lifeline RPC address.
CASSANDRA-13910,Deprecate background repair and probablistic read_repair_chance table option,dclocal_read_repair_chance,https://issues.apache.org/jira/browse/CASSANDRA-13910,https://github.com/apache/cassandra/commit/eaf9bf18b2ec50713170a9ca472c34586b17a5a3,add checking code,give reason,use deprecated param,add log warning,/,"However, I think it would be good to send out a discussion thread to dev@/user@ to get user input rather than just forcing this change on them. While I find it unlikely, there might be some users who find (or think they find) value from this feature. I'd also suggest to get some more user feedback on this, before dumping a working, but potentially confusing feature."
CASSANDRA-13910,Deprecate background repair and probablistic read_repair_chance table option,read_repair_chance,https://issues.apache.org/jira/browse/CASSANDRA-13910,https://github.com/apache/cassandra/commit/eaf9bf18b2ec50713170a9ca472c34586b17a5a3,add checking code,give reason,use deprecated param,add log warning,/,"However, I think it would be good to send out a discussion thread to dev@/user@ to get user input rather than just forcing this change on them. While I find it unlikely, there might be some users who find (or think they find) value from this feature. I'd also suggest to get some more user feedback on this, before dumping a working, but potentially confusing feature."
HBASE-23312,HBase Thrift SPNEGO configs (HBASE-19852) should be backwards compatible,hbase.thrift.spnego.keytab.file,https://issues.apache.org/jira/browse/HBASE-23312,https://github.com/apache/hbase/commit/ea6cea846ad612f34f79b4bcffaf9ea5760a135,add checking code,not backwards compatible,use deprecated param,auto-correction: use new param,/,The newer HBase Thrift SPNEGO configs should not be required. The hbase.thrift.spnego.keytab.file and hbase.thrift.spnego.principal configs will fall back to the hbase.thrift.keytab.file and hbase.thrift.kerberos.principal original configs. The older configs will log a deprecation warning.
HBASE-23312,HBase Thrift SPNEGO configs (HBASE-19852) should be backwards compatible,hbase.thrift.spnego.principal,https://issues.apache.org/jira/browse/HBASE-23312,https://github.com/apache/hbase/commit/ea6cea846ad612f34f79b4bcffaf9ea5760a135,add checking code,not backwards compatible,use deprecated param,auto-correction: use new param,/,The newer HBase Thrift SPNEGO configs should not be required. The hbase.thrift.spnego.keytab.file and hbase.thrift.spnego.principal configs will fall back to the hbase.thrift.keytab.file and hbase.thrift.kerberos.principal original configs. The older configs will log a deprecation warning.
HDFS-13876,HttpFS: Implement ALLOWSNAPSHOT DISALLOWSNAPSHOT.,fs.defaultFS allowSnapshot (cmd line option),https://issues.apache.org/jira/browse/HDFS-13876,https://github.com/apache/hadoop/commit/8de5c923b432a670f6822189e7131d2df7b5a336,add checking code,not given,"correlation check, control dependency",throw UnsupportedOperationException with message,/,"allowSnapshot is not supported for HttpFs on "" + fs.getClass() . Please check your fs.defaultFS configuration"
CASSANDRA-14352,Clean up parsing speculative retry params from string,TableParams.Option.SPECULATIVE_RETRY,https://issues.apache.org/jira/browse/CASSANDRA-14352,https://github.com/apache/cassandra/commit/4991ca26aa424286ebdee89742d35e813f9e9259,add checking code,not given,semantic check,throw ConfigurationException(didn't catch) with message,/,Clean up parsing speculative retry params from string
SPARK-24110,[THRIFT-SERVER] Avoid UGI.loginUserFromKeytab in STS,HIVE_SERVER2_KERBEROS_PRINCIPAL,https://issues.apache.org/jira/browse/SPARK-24110,https://github.com/apache/spark/commit/bf4352ca6c96dfab16b286c54720685e32b216f1,add checking code,not given,not null check: isEmpty,throw IOException with message,/,"Spark ThriftServer will call UGI.loginUserFromKeytab twice in initialization. This is unnecessary and will cause various potential problems, like Hadoop IPC failure after 7 days, or RM failover issue and so on."
SPARK-24110,[THRIFT-SERVER] Avoid UGI.loginUserFromKeytab in STS,HIVE_SERVER2_KERBEROS_KEYTAB,https://issues.apache.org/jira/browse/SPARK-24110,https://github.com/apache/spark/commit/bf4352ca6c96dfab16b286c54720685e32b216f1,add checking code,not given,not null check: isEmpty,throw IOException with message,/,"Spark ThriftServer will call UGI.loginUserFromKeytab twice in initialization. This is unnecessary and will cause various potential problems, like Hadoop IPC failure after 7 days, or RM failover issue and so on."
HDFS-14949,Add getServerDefaults() support to HttpFS,fs.defaultFS,https://issues.apache.org/jira/browse/HDFS-14949,https://github.com/apache/hadoop/commit/3037762b2ca2bee0a281b16455c8592173f9231,add checking code,not given,semantic check: fs instanceof DistributedFileSystem,throw UnsupportedOperationException with message,/,getServerDefaults is not supported for HttpFs on fs.getClass()  Please check your fs.defaultFS configuration
HDFS-9695,HTTPFS - CHECKACCESS operation missing,fs.defaultFS,https://issues.apache.org/jira/browse/HDFS-9695,https://github.com/apache/hadoop/commit/4ede8bce28aadc62007ad65dc6d44be550632b5,add checking code,not given,semantic check: fs instanceof DistributedFileSystem,throw UnsupportedOperationException with message,/,checkaccess is is not supported for HttpFs on fs.getClass()  Please check your fs.defaultFS configuration
CASSANDRA-13622,Improve config validation and documentation on overflow and NPE,data_file_directories,https://issues.apache.org/jira/browse/CASSANDRA-13622,https://github.com/apache/cassandra/commit/a586f6c88dab173663b765261d084ed8410efe81,add checking code,proactively,not null check: wether the dir is not set,throw ConfigurationException(didn't catch) with message,/,"Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user."
CASSANDRA-13622,Improve config validation and documentation on overflow and NPE,conf.native_transport_max_frame_size_in_mb,https://issues.apache.org/jira/browse/CASSANDRA-13622,https://github.com/apache/cassandra/commit/a586f6c88dab173663b765261d084ed8410efe81,add checking code,proactively,single value range check: >= 2048,throw ConfigurationException(didn't catch) with message,/,"Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user."
CASSANDRA-13622,Improve config validation and documentation on overflow and NPE,conf.max_value_size_in_mb,https://issues.apache.org/jira/browse/CASSANDRA-13622,https://github.com/apache/cassandra/commit/a586f6c88dab173663b765261d084ed8410efe81,add checking code,proactively,single value range check: >= 2048,throw ConfigurationException(didn't catch) with message,/,"Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user."
HBASE-22624,Should sanity check table CONFIGURation when clone snapsh……ot to a new table,hbase.hregion.max.filesize hbase.hregion.max.filesize.limit,https://issues.apache.org/jira/browse/HBASE-22624,https://github.com/apache/hbase/commit/64e732dc8bfeda39e3ea782004182de3a8936b1,add checking code,proactively,"correlation with other configuration, value dependency",throw DoNotRetryIOException with message or using log.warn,/,HBASE-12570 imporved table configuration sanity checking. But it only worked for create table or alter table. Should check table configuration too when clone snapshot to a new table.
HBASE-22624,Should sanity check table CONFIGURation when clone snapsh……ot to a new table,hbase.hregion.memstore.flush.size hbase.hregion.memstore.flush.size.limit,https://issues.apache.org/jira/browse/HBASE-22624,https://github.com/apache/hbase/commit/64e732dc8bfeda39e3ea782004182de3a8936b1,add checking code,proactively,"correlation with other configuration, value dependency",throw DoNotRetryIOException with message or using log.warn,/,HBASE-12570 imporved table configuration sanity checking. But it only worked for create table or alter table. Should check table configuration too when clone snapshot to a new table.
HBASE-22624,Should sanity check table CONFIGURation when clone snapsh……ot to a new table,checkClassLoading  (a set of params),https://issues.apache.org/jira/browse/HBASE-22624,https://github.com/apache/hbase/commit/64e732dc8bfeda39e3ea782004182de3a8936b1,add checking code,proactively,parse check,throw DoNotRetryIOException with message or using log.warn,/,HBASE-12570 imporved table configuration sanity checking. But it only worked for create table or alter table. Should check table configuration too when clone snapshot to a new table.
HBASE-22624,Should sanity check table CONFIGURation when clone snapsh……ot to a new table,HStore.BLOCKING_STOREFILES_KEY,https://issues.apache.org/jira/browse/HBASE-22624,https://github.com/apache/hbase/commit/64e732dc8bfeda39e3ea782004182de3a8936b1#,add checking code,proactively,single value range check: < 1000,throw DoNotRetryIOException with message or using log.warn,/,HBASE-12570 imporved table configuration sanity checking. But it only worked for create table or alter table. Should check table configuration too when clone snapshot to a new table.
SPARK-24241,[SUBMIT] Do not fail fast when dynamic resource allocation enabled with 0 executor,spark.dynamicAllocation.enabled,https://issues.apache.org/jira/browse/SPARK-24241,https://github.com/apache/spark/commit/4a2b15f0af400c71b7f20b2048f38a8b74d43dfa#diff-63a5d817d2d45ae24de577f6a1bd80f9L214,add checking code,runtime error,check under specific context: Do not fail fast when dynamic resource allocation enabled with 0 executor,throw error with message,/,Do not fail fast when dynamic resource allocation enabled with 0 executor
SPARK-25753,fix reading small files via BinaryFileRDD,mapreduce.input.fileinputformat.split.minsize.per.rack,https://issues.apache.org/jira/browse/SPARK-25753,https://github.com/apache/spark/commit/81a305dd0418f6e0136b4e38ffe91e0b76c8806e,add checking code,runtime error,"correlation with other configuration, value dependency",auto-correction(silent): change to correlated param value,/,"This is a follow up of #21601, `StreamFileInputFormat` and `WholeTextFileInputFormat` have the same problem. `Minimum split size pernode 5123456 cannot be larger than maximum split size 4194304 java.io.IOException: Minimum split size pernode 5123456 cannot be larger than maximum split size 4194304"
SPARK-25753,fix reading small files via BinaryFileRDD,mapreduce.input.fileinputformat.split.minsize.per.node,https://issues.apache.org/jira/browse/SPARK-25753,https://github.com/apache/spark/commit/81a305dd0418f6e0136b4e38ffe91e0b76c8806e,add checking code,runtime error,"correlation with other configuration, value dependency",auto-correction(silent): change to correlated param value,/,"This is a follow up of #21601, `StreamFileInputFormat` and `WholeTextFileInputFormat` have the same problem. `Minimum split size pernode 5123456 cannot be larger than maximum split size 4194304 java.io.IOException: Minimum split size pernode 5123456 cannot be larger than maximum split size 4194304"
SPARK-22754,[DEPLOY] Check whether spark.executor.heartbeatInterval bigger than spark.network.timeout or not,spark.network.timeout spark.executor.heartbeatInterval,https://issues.apache.org/jira/browse/SPARK-22754,https://github.com/apache/spark/commit/8eb5609d8d961e54aa1ed0632f15f5e570fa627a,add checking code,runtime error:  Job aborted due to stage failure,"correlation with other configuration, value dependency",throw Exception with message,/,"If spark.executor.heartbeatInterval bigger than spark.network.timeout,it will almost always cause exception below. Since many users do not get that point.He will set spark.executor.heartbeatInterval incorrectly.We should check this case when submit applications."
HBASE-20295,fix NullPointException in TableOutputFormat.checkOutputSpecs,context.getConfiguration (a set of configuration options),https://issues.apache.org/jira/browse/HBASE-20295,https://github.com/apache/hbase/commit/53b4507c7949362d49a1b827f2cdf2b1e8cefa68,add checking code,runtime error:  throw NPE,not null check: .isEmpty,auto-correction(silent): using default value,/,HBASE-20295 fix NullPointException in TableOutputFormat.checkOutputSpecs
SPARK-22043,[PYTHON] Improves error message for show_profiles and dump_profiles,spark.python.profile,https://issues.apache.org/jira/browse/SPARK-22043,https://github.com/apache/spark/commit/7c7266208a3be984ac1ce53747dc0c3640f4ecac,add checking code,runtime error: AttributeError: 'NoneType' object has no attribute 'dump_profiles',not null check: is not None,throw RuntimeError with message,/,I mistakenly missed spark.python.profile enabled today while profiling and met this unfriendly messages It looks we should give better information that says spark.python.profile should be enabled.
HBASE-16993,BucketCache throw java.io.IOException: Invalid HFile block magic when CONFIGuring hbase.bucketcache.bucket.sizes.,hbase.bucketcache.bucket.sizes,https://issues.apache.org/jira/browse/HBASE-16993,https://github.com/apache/hbase/commit/bc93b6610b349d38502290af27da0ae0b5fd4936,add checking code,runtime error: BucketCache throw java.io.IOException,semantic check: bucketSize % 256 != 0,throw IllegalArgumentException with message,/,"Any value for hbase.bucketcache.bucket.sizes configuration to be multiple of 256. If that is not the case, instantiation of L2 Bucket cache itself will fail throwing IllegalArgumentException."
SPARK-28634,Failed to start SparkSession with Keytab file,principal,https://issues.apache.org/jira/browse/SPARK-28634,https://github.com/apache/spark/commit/5f6eb5d20dee57ea7ba9d47b21c712dee06fa7e,add checking code,runtime error: Failed to start SparkSession with Keytab file,use deprecated param,no special action,/,"For proper backwards compatibility it needs to remove the old names too. (Or make a change in the AM instead to ignore the keytab when running in client mode, which avoids the above hack.)"
SPARK-25295,Fix executor names collision,KUBERNETES_EXECUTOR_POD_NAME_PREFIX.key,https://issues.apache.org/jira/browse/SPARK-25295,https://github.com/apache/spark/commit/3e75a9fa24f8629d068b5fbbc7356ce2603fa58d,add checking code,runtime error: naming conflicts and failures for the next job submission.,not null check: .isEmpty,auto-correction(silent): change to the default value,/,"Fixes the collision issue with spark executor names in client mode, see SPARK-25295 for the details. It follows the cluster name convention as app-name will be used as the prefix and if that is not defined we use ""spark"" as the default prefix. Eg. `spark-pi-1536781360723-exec-1` where spark-pi is the name of the app passed at the config side or transformed if it contains illegal characters."
SPARK-21418,SQL] NoSuchElementException: None.get in DataSourceScanExec with sun.io.serialization.extendedDebugInfo=true,conf == null  (a set of params),https://issues.apache.org/jira/browse/SPARK-21418,https://github.com/apache/spark/commit/ca59445adb30ed796189532df2a2898ecd33db68,add checking code,runtime error: NoSuchElementException,parse check,no special action,/,"If no SparkConf is available to Utils.redact, simply don't redact."
CASSANDRA-14525,Do not enable native transport if bootstrap is pending,DatabaseDescriptor.getAuthenticator() write_survey,https://issues.apache.org/jira/browse/CASSANDRA-14525,https://github.com/apache/cassandra/commit/a6196a3a79b67dc6577747e591456328e57c314f,add checking code,runtime error: NullPointer exception,check under specific context:Not starting client transports in write_survey mode as it's bootstrapping or auth is enabled,throw IllegalStateException(runtime exception) with message & logger.info,/,If bootstrap fails for newly joining node (most common reason is due to streaming failure) then Cassandra state remains in joining state which is fine but Cassandra also enables Native transport which makes overall state inconsistent. This further creates NullPointer exception if auth is enabled on the new node
SPARK-28843,Set OMP_NUM_THREADS to executor cores for python if not set,spark.executorEnv.OMP_NUM_THREADS,https://issues.apache.org/jira/browse/SPARK-28843,https://github.com/apache/spark/commit/31b59bd80517f17f160eb8d6665b9bf8b3372d3,add checking code,runtime error: the amount of memory required by PySpark applications increased and tracked the problem to importing numpy,not null check: .isEmpty,auto-correction(silent): fall back to other param,/,limit the OpenMP thread pool to the number of cores assigned to this executor this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool
SPARK-28843,Set OMP_NUM_THREADS to executor cores for python if not set,spark.yarn.appMasterEnv.OMP_NUM_THREADS,https://issues.apache.org/jira/browse/SPARK-28843,https://github.com/apache/spark/commit/31b59bd80517f17f160eb8d6665b9bf8b3372d3,add checking code,runtime error: the amount of memory required by PySpark applications increased and tracked the problem to importing numpy,not null check: .isEmpty,auto-correction(silent): fall back to other param,/,limit the OpenMP thread pool to the number of cores assigned to this executor this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool
SPARK-28843,Set OMP_NUM_THREADS to executor cores for python if not set,spark.mesos.driverEnv.OMP_NUM_THREADS,https://issues.apache.org/jira/browse/SPARK-28843,https://github.com/apache/spark/commit/31b59bd80517f17f160eb8d6665b9bf8b3372d3,add checking code,runtime error: the amount of memory required by PySpark applications increased and tracked the problem to importing numpy,not null check: .isEmpty,auto-correction(silent):fall back to other param,/,limit the OpenMP thread pool to the number of cores assigned to this executor this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool
SPARK-28843,Set OMP_NUM_THREADS to executor cores for python if not set,spark.kubernetes.driverEnv.OMP_NUM_THREADS,https://issues.apache.org/jira/browse/SPARK-28843,https://github.com/apache/spark/commit/31b59bd80517f17f160eb8d6665b9bf8b3372d3,add checking code,runtime error: the amount of memory required by PySpark applications increased and tracked the problem to importing numpy,not null check: .isEmpty,auto-correction(silent): fall back to other param,/,limit the OpenMP thread pool to the number of cores assigned to this executor this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool
SPARK-24610,fix reading small files via wholeTextFiles,mapreduce.input.fileinputformat.split.minsize.per.node,https://issues.apache.org/jira/browse/SPARK-24610,https://github.com/apache/spark/commit/1055c94cdf072bfce5e36bb6552fe9b148bb9d17,add checking code,runtime error: throws an exception while trying to read them,"correlation with other variable, value dependency",auto-correction(silent): change to correlated param value,/,This change checks the maxSplitSize against the minSplitSizePerNode and minSplitSizePerRack and set them if `maxSplitSize < minSplitSizePerNode/Rack`
SPARK-24610,fix reading small files via wholeTextFiles,mapreduce.input.fileinputformat.split.minsize.per.rack,https://issues.apache.org/jira/browse/SPARK-24610,https://github.com/apache/spark/commit/1055c94cdf072bfce5e36bb6552fe9b148bb9d17,add checking code,runtime error: throws an exception while trying to read them,"correlation with other variable, value dependency",auto-correction(silent): change to correlated param value,/,This change checks the maxSplitSize against the minSplitSizePerNode and minSplitSizePerRack and set them if `maxSplitSize < minSplitSizePerNode/Rack`
SPARK-23240,Better error message when extraneous data in py……spark.daemon's stdout,daemonPort (command line option),https://issues.apache.org/jira/browse/SPARK-23240,https://github.com/apache/spark/commit/862fa697d829cdddf0f25e5613c91b040f9d9652,add checking code,runtimt error: PythonWorkerFactory uses the output as the daemon’s port number and ends up throwing an exception when creating the socket,single value range check,throw SparkException with message,/,"Environmental issues or site-local customizations (i.e., sitecustomize.py present in the python install directory) can interfere with daemon.py’s output to stdout. PythonWorkerFactory produces unhelpful messages when this happens, causing some head scratching before the actual issue is determined."
HBASE-22127,Ensure that the block cached in the LRUBlockCache offheap is allocated from heap,cacheConf.getBlockCache()==null,https://issues.apache.org/jira/browse/HBASE-22127,https://github.com/apache/hbase/commit/d1eb6171f9bfdd6adf2a4805ab0b7fcccf452d26,add checking code,system wrong state: All the sizing assumptions and calc going out of control,not null check: == null,no special action,/,"It's indeed an big problem here. so we can only make the block ref to an heap area if we use LRUCache (both LruBlockCache and CombinedBlockCache case). Or we can also make the lru cache offheap?I think we can introduce an switch indicate that whether the lru block cache offheap or not, if heap, then coping those bytes from ByteBuff to heap."
SPARK-26530,Validate heartheat arguments in HeartbeatReceiver,spark.network.timeoutInterval spark.storage.blockManagerSlaveTimeoutMs,https://issues.apache.org/jira/browse/SPARK-26530,https://github.com/apache/spark/commit/8d667c511c41239033defed6dfb07414ad98935d,add checking code,system wrong state: Application may run for a while and not failed until the max executor failures reached,"correlation with other configuration, value dependency",throw IllegalArgumentException with message,/,"Currently, heartbeat related arguments is not validated in spark, so if these args are inproperly specified, the Application may run for a while and not failed until the max executor failures reached(especially with spark.dynamicAllocation.enabled=true), thus may incurs resources waste.This PR is to precheck these arguments in HeartbeatReceiver to fix this problem."
SPARK-26530,Validate heartheat arguments in HeartbeatReceiver,spark.executor.heartbeatInterval spark.storage.blockManagerSlaveTimeoutMs,https://issues.apache.org/jira/browse/SPARK-26530,https://github.com/apache/spark/commit/8d667c511c41239033defed6dfb07414ad98935d,add checking code,system wrong state: Application may run for a while and not failed until the max executor failures reached,"correlation with other configuration, value dependency",throw IllegalArgumentException with message,/,"Currently, heartbeat related arguments is not validated in spark, so if these args are inproperly specified, the Application may run for a while and not failed until the max executor failures reached(especially with spark.dynamicAllocation.enabled=true), thus may incurs resources waste.This PR is to precheck these arguments in HeartbeatReceiver to fix this problem."
SPARK-22574,[MESOS][SUBMIT] Check submission request PARAMETERs,environmentVariables,https://issues.apache.org/jira/browse/SPARK-22574,https://github.com/apache/spark/commit/7a51e71355485bb176a1387d99ec430c5986cbec,add checking code,system wrong state: Application may run for a while and not failed until the max executor failures reached  causing a bad state of Dispatcher ,not null check: is not None,throw SubmitRestMissingFieldException with message,/,"When submitting a wrong CreateSubmissionRequest to Spark Dispatcher is causing a bad state of Dispatcher and making it inactive as a mesos framework. There are some checks of these variables but not in all of them, for example in appArgs and environmentVariables."
HDFS-12498,Journal Syncer is not started in Federated + HA cluster,dfs.namenode.shared.edits.dir,https://issues.apache.org/jira/browse/HDFS-12498,https://github.com/apache/hadoop/commit/6d201f77c734d6c6a9e3e297fe3dbff251cbb8b3,add checking code,system wrong state: Application may run for a while and not failed until the max executor failures reached: Failed to start SyncJournal daemon,not null check: == null || isEmpty(),add log warning,/,"Journal Syncer is not getting started in HDFS + Federated cluster, when dfs.shared.edits.dir.<<nameserviceId>> is provided, instead of dfs.namenode.shared.edits.dir"
CASSANDRA-13006,Rely on the JVM to handle OutOfMemoryErrors,JVM_ON_OUT_OF_MEMORY_ERROR_OPT,https://issues.apache.org/jira/browse/CASSANDRA-13006,https://github.com/apache/cassandra/commit/02aba7343ce300397ab672bbb1788aa8182d8a48,add checking code,"system wrong state: can't shutdown from OutOfMemoryError, data corruption",not null check: wether the value is set,add log warning,/,Cassandra is now relying on the JVM options to properly shutdown on OutOfMemoryError. By default it will rely on the OnOutOfMemoryError option as the ExitOnOutOfMemoryError and CrashOnOutOfMemoryError options are not supported by the older 1.7 and 1.8 JVMs. A warning will be logged at startup if none of those JVM options are used. See CASSANDRA-13006 for more details.
CASSANDRA-14991,SSL Cert Hot Reloading should check for sanity of the new keystore/truststore before loading it,conf.server_encryption_options,https://issues.apache.org/jira/browse/CASSANDRA-14991,https://github.com/apache/cassandra/commit/16ef9ac37c21c4f9091cd1f3658e54abddab8ad8,add checking code,system wrong state: fail accepting new connections,not null check: serverOpts != null,throw ConfigurationException with message(didn't catch) & throw I/O excpetion and catch exception with logger.error,/,"SSL Cert Hot Reloading assumes that the keystore & truststore are valid. However, a corrupt store or a password mismatch can cause Cassandra to fail accepting new connections as we throw away the old SslContext. This patch will ensure that we check the sanity of the certificates during startup and during hot reloading. This should protect against bad key/trust stores. As part of this PR, I have cleaned up the code a bit."
CASSANDRA-14991,SSL Cert Hot Reloading should check for sanity of the new keystore/truststore before loading it,conf.client_encryption_options,https://issues.apache.org/jira/browse/CASSANDRA-14991,https://github.com/apache/cassandra/commit/16ef9ac37c21c4f9091cd1f3658e54abddab8ad8,add checking code,system wrong state: fail accepting new connections,not null check: serverOpts != null,throw ConfigurationException with message(didn't catch) & throw I/O excpetion and catch exception with logger.error,/,"SSL Cert Hot Reloading assumes that the keystore & truststore are valid. However, a corrupt store or a password mismatch can cause Cassandra to fail accepting new connections as we throw away the old SslContext. This patch will ensure that we check the sanity of the certificates during startup and during hot reloading. This should protect against bad key/trust stores. As part of this PR, I have cleaned up the code a bit."
HBASE-21031,Memory leak if replay edits failed during region opening,hbase.hregion.memstore.mslab.enabled,https://issues.apache.org/jira/browse/HBASE-21031,https://github.com/apache/hbase/commit/4a52ddb75e2d18c6b044146b03bb1dba90ec4184,add checking code,system wrong state: memory is leak forever,check under specific context: with memstore,add log.warn,/,"After this exception, the memstore did not roll back, and since MSLAB is used, all the chunk allocated won't release for ever. Those memory is leak forever"
CASSANDRA-14084,Fix imbalanced disks when replacing node with same address with JBOD,DatabaseDescriptor.getReplaceAddress(),https://issues.apache.org/jira/browse/CASSANDRA-14084,https://github.com/apache/cassandra/commit/50e6e721b2a81da7f11f60a2fa405fd46e5415d4,add checking code,system wrong state: sstables to be randomly spread across disks potentially causing imbalance,not null check:wether the value is null,no special action,/,"While investigating CASSANDRA-14083, I noticed that we use the pending ranges to calculate the disk boundarieswhen the node is bootstrapping. The problem is that when the node is replacing a node with the same address, it sets itself as normal locally (for other unrelated reasons), so the local ranges will be null and consequently the disk boundaries will be null. This will cause the sstables to be randomly spread across disks potentially causing imbalance."
HBASE-20576,Check remote WAL directory when creating peer and transiting peer to A,peerConfig.getRemoteWALDir,https://issues.apache.org/jira/browse/HBASE-20576,https://github.com/apache/hbase/commit/603110719d9d26705e782ba048db713b18cb076a,add checking code,system wrong state: stuck there forever,semantic check: isAbsolute(),throw DoNotRetryIOException with message,/,"When testing on the real cluster I typed a wrong remote wal directory. Then I start the procedure for transiting the peer to A, the procedure was stuck there for ever"
HBASE-19630,Add peer cluster key check when add new replication peer,ZKConfig.validateClusterKey  (a set of params),https://issues.apache.org/jira/browse/HBASE-19630,https://github.com/apache/hbase/commit/1025388da6df8516fbbdf0dc21249660602f0b01,add checking code,system wrong state: will retry forever and can not recover,general check,throw DoNotRetryIOException with message,/,OK. Then +1. I also hit this problem when implementing HBASE-19592. If the cluster key is error then the add peer procedure will retry forever and can not recover...
SPARK-24954,Fail fast on job submit if run a barrier stage with dynamic resource allocation enabled,spark.dynamicAllocation.enabled,https://issues.apache.org/jira/browse/SPARK-24954,https://github.com/apache/spark/commit/92b48842b944a3e430472294cdc3c481bad6b804,add checking code,system wrong state:There can be deadlock with two concurrent applications.,check under specific context: spark.dynamicAllocation.enabled (conf) is XOR with rdd.isBarrier (not conf),throw SparkException with message,/,"We don't support run a barrier stage with dynamic resource allocation enabled, it shall lead to some confusing behaviors (eg. with dynamic resource allocation enabled, it may happen that we acquire some executors (but not enough to launch all the tasks in a barrier stage) and later release them due to executor idle time expire, and then acquire again). We perform the check on job submit and fail fast if running a barrier stage with dynamic resource allocation enabled."
SPARK-28577,Ensure executorMemoryHead requested value not less than MEMORY_OFFHEAP_SIZE when MEMORY_OFFHEAP_ENABLED is true,MEMORY_OFFHEAP_SIZE,https://issues.apache.org/jira/browse/SPARK-28577,https://github.com/apache/spark/commit/a07f795aead3bd81e7cccad30a7f6148c09ed8a,add checking code,give reason,single value range check: >0,throw IllegalArgumentException with message,/,"If MEMORY_OFFHEAP_ENABLED is true, we should ensure executorOverheadMemory not less than MEMORY_OFFHEAP_SIZE, otherwise the memory resource requested for executor may be not enough."
SPARK-24761,[SQL] Adding of isModifiable() to RuntimeConfig,isModifiable  (a set of params),https://issues.apache.org/jira/browse/SPARK-24761,https://github.com/apache/spark/commit/3ab48f985c7f96bc9143caad99bf3df7cc984583,add checking code,unexpected result: does not may any effects.,read-only configuration,no special action,/,"In the PR, I propose to extend `RuntimeConfig` by new method `isModifiable()` which returns `true` if a config parameter can be modified at runtime (for current session state). For static SQL and core parameters, the method returns `false`."
SPARK-22533,[CORE] Handle deprecated names in ConfigEntry.,SparkConf.getDeprecatedConfig  (a set of params),https://issues.apache.org/jira/browse/SPARK-22533,https://github.com/apache/spark/commit/c13b60e0194c90156e74d10b19f94c70675d21ae,add checking code,unexpected result: does not respect the deprecated keys,use deprecated param,add log warning,/,"This change hooks up the config reader to `SparkConf.getDeprecatedConfig`, so that config constants with deprecated names generate the proper warnings."
SPARK-29865,Ensure client-mode executors have same name prefix,KUBERNETES_EXECUTOR_POD_NAME_PREFIX,https://issues.apache.org/jira/browse/SPARK-29865,https://github.com/apache/spark/commit/b095232f630221926a9eabb8233c20d03c9a6eb,add checking code,"unexpected result: each executor ends up with a different name prefix, which makes debugging a little bit annoying.",not null check: !sc.conf.contains(KUBERNETES_EXECUTOR_POD_NAME_PREFIX),auto-correction: use spark.app.name,/,"If KUBERNETES_EXECUTOR_POD_NAME_PREFIX is not set, initialize it so that all executors have the same prefix. This is needed for client mode, where the feature steps code that sets this  configuration is not used."
HBASE-22559,[RPC] set guard against CALL_QUEUE_HANDLER_FACTOR_CONF_KEY,hbase.ipc.server.callqueue.handler.factor,https://issues.apache.org/jira/browse/HBASE-22559,https://github.com/apache/hbase/commit/ab4453158a41e29bb75e219079584997be756cbe,add checking code,unexpected result: handlers will be expanded twice,"single value range check: [0.0, 1.0]","auto-correction: change to the default value, with log message",/,"CALL_QUEUE_HANDLER_FACTOR_CONF_KEY is hbase.ipc.server.callqueue.handler.factor, a float number, which is supposed between [0.0, 1.0]. If it is greater than 1, for example, 2.0, the call queues and handlers will be expanded twice: We already have hbase.regionserver.handler.count set, what mentioned above looks tricky. The purpose of this conf is to control how many handlers can share one call queue, expanding the number of handlers (a kind of side-effects) is not included, from my understanding."
SPARK-26060,Track SparkConf entries and make SET command reject such entries.,ConfigEntry.findEntry(key) (a set of params),https://issues.apache.org/jira/browse/SPARK-26060,https://github.com/apache/spark/commit/8edb64c1b9ee49d836e171a459dd93f524df92bf,add checking code,unexpected result: has no effect without any warnings,parsing check,throw AnalysisException(not handled) with message,/,"Currently the SET command works without any warnings even if the specified key is for SparkConf entries and it has no effect because the command does not update SparkConf, but the behavior might confuse users. We should track SparkConf entries and make the command reject for such entries."
HBASE-18784,if available query underlying outputstream capabilities where we need hflush/hsync.,hbase.procedure.store.wal.use.hsync hbase.wal.dir,https://issues.apache.org/jira/browse/HBASE-18784,https://github.com/apache/hbase/commit/e79a007dd9810b33cd508986037e17d45b55a705,add checking code,"Unexpected result: HBase RegionServer process may fail to have access to all the data it previously wrote to its write ahead log, resulting in data loss.","check under specific context the mounted fs of ""hbase.wal.dir"" should support ""hbase.procedure.store.wal.use.hsync""",throw IllegalStateException(runtime exception) with message,/,"In places where we rely on the underlying filesystem holding up the promises of hflush/hsync (most importantly the WAL), we should use the new interfaces provided by HDFS-11644 to fail loudly when they are not present (e.g. on S3, on EC mounts, etc)."
SPARK-30025,Continuous shuffle block fetching should be disabled by default when the old fetch protocol is used,SHUFFLE_USE_OLD_FETCH_PROTOCOL,https://issues.apache.org/jira/browse/SPARK-30025,https://github.com/apache/spark/commit/169415ffac3050a86934011525ea00eef7fca35,add checking code,unexpected result: only shuffle block fetcher will merge the request,check under specific context,add log debug,/,The feature tag of continuous shuffle block fetching is set to true but we can not enable the feature because other conditions are not satisfied.
HBASE-16868,Add a replicate_all flag to avoid misuse the namespaces and table-cfs CONFIG of replication peer,checkPeerConfig (a set of params),https://issues.apache.org/jira/browse/HBASE-16868,https://github.com/apache/hbase/commit/3e2941a49e58080618fd2d2e6757694c96651e0a,add checking code,unexpected result: replicate only one table,parsing check,throw ReplicationException with message,/,If we don't set namespaces and table cfs in peer config. It means replicate all tables to the peer cluster. Then this peer will only replicate table1 to the peer cluster. It changes to replicate only one table from replicate all tables in the cluster. It is very easy to misuse in production cluster. So we should avoid appending table to a peer which replicates all table.
SPARK-24783,spark.sql.shuffle.partitions=0 should throw exception,spark.sql.shuffle.partitions,https://issues.apache.org/jira/browse/SPARK-24783,https://github.com/apache/spark/commit/5ebb4b572318abf1b483b5a8f41aa9b021eb5327,add checking code,unexpected result: result join will be an empty table,single value range check,throw IllegalArgumentException with message,/,if spark.sql.shuffle.partitions=0 and trying to join tables (not broadcast join) result join will be an empty table.
HBASE-19295,"The Configuration returned by CPEnv should be read-only.Adds a ReadOnlyConfiguration that delegates gets but throws exceptionon sets/adds, etc.",Read-only Configuration (a set of params),https://issues.apache.org/jira/browse/HBASE-19295,https://github.com/apache/hbase/commit/4a2e8b852dceb8f68adf33c940621734cc6e2f12,add checking code,unexpected result: results in a call to every registered Configurations loadResource,read-only configuration,throw UnsupportedOperationException,/,"The CP should not be able to modify this config. We should throw exception if they try to write us. missing exception code, with message"
SPARK-25088,Update Rest Server docs & defaults.,spark.master.rest.enabled SPARK_AUTH_SECRET_CONF,https://issues.apache.org/jira/browse/SPARK-25088,https://github.com/apache/spark/commit/10248758438b9ff57f5669a324a716c8c6c8f17b,add checking code,Unexpected result: The RestSubmissionServer does not support authentication via ${authKey},"correlation check, control dependency",throw IllegalArgumentException with message,/,"wrong behavior The RestSubmissionServer does not support authentication via ${authKey}. interaction with other config(control), check before start, with message"
HDFS-14660,[SBN Read] ObserverNameNode should throw StandbyException for requests not from ObserverProxyProvider.,ObserverReadProxyProvider.class,https://issues.apache.org/jira/browse/HDFS-14660,https://github.com/apache/hadoop/commit/02bd02b5af761b6b24fdc4e8e7ede72a51870d5,add checking code,unexpected result: the table will only expose bar and baz columns it may send request to a observer node,check under specific context,throw StandbyException with message,/,"In a HDFS HA cluster with consistent reads enabled (HDFS-12943), clients could be using either ObserverReadProxyProvider, ConfiguredProxyProvider, or something else. Since observer is just a special type of SBN and we allow transitions between them, a client NOT using ObserverReadProxyProvider will need to have dfs.ha.namenodes.<nameservice> include all NameNodes in the cluster, and therefore, it may send request to a observer node. For this case, we should check whether the stateId in the incoming RPC header is set or not, and throw an StandbyException when it is not."
HBASE-17513,Thrift Server 1 uses different QOP settings than RPC and Thrift Server 2 and can easily be misconfigured so there is no encryption when the operator expects it.,hbase.thrift.ssl.enabled hbase.thrift.security.qop,https://issues.apache.org/jira/browse/HBASE-17513,https://github.com/apache/hbase/commit/9a45e0a9ded094d18bdcbbcaf4cf3944e7faf6d9,add checking code,Unexpected result: transport is not encrypted,"correlation check, control dependency",throw IllegalArgumentException with message,/,"This change fixes an issue where users could have unintentionally configured the HBase Thrift1 server to run without wire-encryption, when they believed they had configured the Thrift1 server to do so."
SPARK-22938,Assert that SQLConf.get is accessed only on the driver.,SparkEnv.get.executorId(a set of params),https://issues.apache.org/jira/browse/SPARK-22938,https://github.com/apache/spark/commit/247a08939d58405aef39b2a4e7773aa45474ad12,add checking code,"unexpected result: will read fallbackConf, falling back to default config values",read-only configuration,throw Exception with message,/,"Assert if code tries to access SQLConf.get on executor. This can lead to hard to detect bugs, where the executor will read fallbackConf, falling back to default config values, ignoring potentially changed non-default configs."
SPARK-22217,[SQL] ParquetFileFormat to support arbitrary OutputCommitters,parquet.enable.summary-metadata,https://issues.apache.org/jira/browse/SPARK-22217,https://github.com/apache/spark/commit/9104add4c7c6b578df15b64a8533a1266f90734e,add checking code,unexpected result: won't see the data,"correlation check, control dependency",add log.warn,/,"if the user has set parquet.enable.summary-metadata=true, and set a committer which is not a ParquetOutputCommitter, then they won't see the data."
HBASE-21277,HBASE-21277 Prevent to add same table to two sync replication peer's config,checkSyncReplicationPeerConfigConflict (a set of params),https://issues.apache.org/jira/browse/HBASE-21277,https://github.com/apache/hbase/commit/a1f28f3ca77d1cd9777d694e246b57d642dfcdb8,add checking code,unexpected result: write wal to three places,general check,throw DoNotRetryIOException with message,/,"If a table in two sync replication peer's config, it need write wal to three places: local dir and two remote dir. It is not allowed. Need to add check when add sync replication peer or modify sync replication peer's config."
SPARK-3685,[CORE] Prints explicit warnings when CONFIGured local directories are set to URIs,getConfiguredLocalDirs,https://issues.apache.org/jira/browse/SPARK-3685,https://github.com/apache/spark/commit/bc8933faf238bcf14e7976bd1ac1465dc32b8e2b,add checking code,unexpected result:it doesn't work,semantic check: path is URIs,add log,/,This PR proposes to print warnings before creating local by `java.io.File`. I think we can't just simply disallow and throw an exception for such cases of `hdfs:/tmp/foo` case because it might break compatibility. Note that `hdfs:/tmp/foo` creates a directory called `hdfs:/`.
SPARK-23476,[CORE] Generate secret in local mode when authentication on,SparkLauncher.SPARK_MASTER,https://issues.apache.org/jira/browse/SPARK-23476,https://github.com/apache/spark/commit/c5abb3c2d16f601d507bee3c53663d4e117eb8b5,change checking code constraint,early termination,change check constraint: == 'local',/,/,"If spark is run with ""spark.authenticate=true"", then it will fail to start in local mode."
HBASE-19935,Only allow table replication for sync replication for now,oldPeerConfig.isSyncReplication,https://issues.apache.org/jira/browse/HBASE-19935,https://github.com/apache/hbase/commit/00e54aae24d88ac9c2b84db5ea1eaf11c3e5d73c,change checking code constraint,give reason,change check constraint: only allow table replication,/,/,"Add pre check to only allow table replication for now, no namespace, or replicate all and exclusion. This is used to reduce the difficulty for implementing the sync replication state transition as we need to reopen all the related regions."
HDFS-13081,Datanode#checkSecureConfig should allow SASL and privileged HTTP.,checkSecureConfig,https://issues.apache.org/jira/browse/HDFS-13081,https://github.com/apache/hadoop/commit/f20e10b2dd59f99d9af009bbbb60a067b9893e69,change checking code constraint,give reason,change check constraint,/,/,"If SASL QoP does not have auth-conf, then no need to enforce HTTPS check. If SASL QoP have auth-conf, then we need to enforce HTTPS check"
CASSANDRA-13622,Improve config validation and documentation on overflow and NPE,cassandra.available_processors,https://issues.apache.org/jira/browse/CASSANDRA-13622,https://github.com/apache/cassandra/commit/a586f6c88dab173663b765261d084ed8410efe81,change checking code constraint,not given,change check constraint: ==null -> isNullorEmpty,/,/,"Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user."
SPARK-25904,Allocate arrays smaller than Int.MaxValue,spark.shuffle.file.buffer,https://issues.apache.org/jira/browse/SPARK-25904,https://github.com/apache/spark/commit/8fbc1830f962c446b915d0d8ff2b13c5c75d22fc,change checking code constraint,proactively,change check constraint: Int.MaxValue -> ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,/,/,"In a few places spark can try to allocate arrays as big as Int.MaxValue, but thats actually too big for the JVM. We should consistently use ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH instead. In some cases this is changing defaults for configs, in some cases its bounds on a config, and others its just improving error msgs for things that still won't work."
SPARK-25904,Allocate arrays smaller than Int.MaxValue,spark.shuffle.unsafe.file.output.buffer,https://issues.apache.org/jira/browse/SPARK-25904,https://github.com/apache/spark/commit/8fbc1830f962c446b915d0d8ff2b13c5c75d22fc,change checking code constraint,proactively,change check constraint: Int.MaxValue -> ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,/,/,"In a few places spark can try to allocate arrays as big as Int.MaxValue, but thats actually too big for the JVM. We should consistently use ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH instead. In some cases this is changing defaults for configs, in some cases its bounds on a config, and others its just improving error msgs for things that still won't work."
SPARK-25904,Allocate arrays smaller than Int.MaxValue,spark.buffer.write.chunkSize,https://issues.apache.org/jira/browse/SPARK-25904,https://github.com/apache/spark/commit/8fbc1830f962c446b915d0d8ff2b13c5c75d22fc,change checking code constraint,proactively,change check constraint: Int.MaxValue -> ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,/,/,"In a few places spark can try to allocate arrays as big as Int.MaxValue, but thats actually too big for the JVM. We should consistently use ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH instead. In some cases this is changing defaults for configs, in some cases its bounds on a config, and others its just improving error msgs for things that still won't work."
HBASE-19783,Change replication peer cluster key/endpoint from a not-null value to null is not allowed,peerConfig.getClusterKey(),https://issues.apache.org/jira/browse/HBASE-19783,https://github.com/apache/hbase/commit/20ccaef8418a34b909256c2a0b68f55b390c9dfb,change checking code constraint,unexpected result: confusing behavior,change check constraint:  not-null value to null is not allowed,/,/,"To reduce the confusing behavior, now when you call updatePeerConfig with empty ClusterKey or ReplicationEndpointImpl, but the value of field of the to-be-updated ReplicationPeerConfig is not null, we will throw exception instead of ignoring them."
HBASE-22531,The HFileReaderImpl#shouldUseHeap return the incorrect true when disabled BlockCache,if (!cacheConf.getBlockCache().isPresent()),https://issues.apache.org/jira/browse/HBASE-22531,https://github.com/apache/hbase/commit/ccabbdd40aff886899a12744de0f6591dd49c6af,change checking code constraint,unexpected result: so many heap allocation,change check constraint:   == null -> .isPresent(),/,/,"I'm having a benchmark with block cache disabled for HBASE-21879 branch. Just caurious about why still so many heap allocation in the heap allocation flame graph async-prof-pid-13311-alloc-4.svg , actually, I've set the following config, which means all allocation should be offheap, while it's not:Say, the CacheConfig#getBlockCache will return a Optional<BlockCache>, which is always non-null:"
SPARK-28577,Ensure executorMemoryHead requested value not less than MEMORY_OFFHEAP_SIZE when MEMORY_OFFHEAP_ENABLED is true,MEMORY_OFFHEAP_SIZE,https://issues.apache.org/jira/browse/SPARK-28577,https://github.com/apache/spark/commit/a07f795aead3bd81e7cccad30a7f6148c09ed8a,change checking code constraint,give reason,change check constraint: add executorOffHeapMemory,/,/,"If MEMORY_OFFHEAP_ENABLED is true, we should ensure executorOverheadMemory not less than MEMORY_OFFHEAP_SIZE, otherwise the memory resource requested for executor may be not enough."
HBASE-21909,Validate the put instance before executing in AsyncTable.put method,hbase.client.keyvalue.maxsize,https://issues.apache.org/jira/browse/HBASE-21909,https://github.com/apache/hbase/commit/dedab71381e14d96da61485f859e0792d8ebc3f9,change checking place,not given,early check,/,/,Move the validatePut method to ConnectionUtils to make sure that we have same behavior for both sync client and async client.
CASSANDRA-15007,Fix SimpleStrategy option validation,replication_factor,https://issues.apache.org/jira/browse/CASSANDRA-15007,https://github.com/apache/cassandra/commit/47d4971b56d97ba8a528f7c17bfd6b11f1ababa3,change checking place,runtime error: configurationexception,early check,/,/,Getting uninformative ConfigurationException when trying to create a keyspace with SimpleStrategy and no replication factor.
SPARK-27192,spark.task.cpus should be less or equal than spark.task.cpus when use static executor allocation,spark.task.cpus spark.executor.cores,https://issues.apache.org/jira/browse/SPARK-27192,https://github.com/apache/spark/commit/39f75b45889951b175c48b2e849e920d0a0eee6e,change checking place,system wrong state: spark will hang when submit new job for TaskSchedulerImpl will not schedule a task,early check,/,/,"When use dynamic executor allocation, if we set spark.executor.cores small than  spark.task.cpus, exception will be thrown as follows:But, if dynamic executor allocation not enabled, spark will hang when submit new job for TaskSchedulerImpl will not schedule a task in a executor which available cores is small than spark.task.cpus. So, when start task scheduler, spark.task.cpus should be check."
HBASE-21857,Do not need to check clusterKey if replicationEndpoint is provided when adding a peer,peerConfig.getClusterKey(),https://issues.apache.org/jira/browse/HBASE-21857,https://github.com/apache/hbase/commit/6f16836c20802ff50ed3b0ab4bc4f1bfa791465e,remove checking code,give reason,not null check ,/,/,"The clusterKey check is done in HBASE-19630, which is part of the work for HBASE-19397. In HBASE-19630 we claim that we always check clusterKey when adding a peer at RS side, but this is not true, as clusterKey could be null. And it will be strange that if we implement a ReplicationEndpoint to kafka and we still need to provide a cluster key in the hbase format."
HBASE-19772,Do not close connection to zk when there are still pending request in ReadOnlyZKClient,connectString (including HConstants.ZOOKEEPER_CLIENT_PORT and HConstants.ZOOKEEPER_QUORUM),https://issues.apache.org/jira/browse/HBASE-19772,https://github.com/apache/hbase/commit/d24fddf3ed4980bf2801b81544b25418133fd60d,error message,/,/,/,add config content: 1->3,Patch makes it so we we don't close out zk if available Tasks to run and nicer logging.
HBASE-20259,Adds logging of CompactingMemStore configuration on construction.,hbase.systemtables.compacting.memstore.type,https://issues.apache.org/jira/browse/HBASE-20259,https://github.com/apache/hbase/commit/c88ca54c16625bc7ab7ebf5eae295fc669d08bb7,error message,/,/,/,add config content: 1->3,Assigned myself. Was getting myself confused as to what was configured; needs stronger declaration in log. Let me write up how to enable/disable too since I've been exercising this over last few days.
SPARK-25140,Catch correct exceptions when expr codegen fails,spark.sql.codegen.factoryMode,https://issues.apache.org/jira/browse/SPARK-25140,https://github.com/apache/spark/commit/07737c87d6086c986785ff0edc43ca94effa4fc6,error message,/,/,/,add config content: 1->1,This pr is to fix bugs when expr codegen fails; we need to catch `java.util.concurrent.ExecutionException` instead of `InternalCompilerException` and `CompileException` . This handling is the same with the `WholeStageCodegenExec `
SPARK-24757,[SQL] Improving the error message for broadcast timeouts,spark.sql.broadcastTimeout park.sql.autoBroadcastJoinThreshold,https://issues.apache.org/jira/browse/SPARK-24757,https://github.com/apache/spark/commit/79c66894296840cc4a5bf6c8718ecfd2b08bcca8,error message,/,/,/,add config content: 1->4,"Currently, the TimeoutException that is thrown on broadcast joins doesn't give any clues to user how to resolve the issue. Need to provide such help to users by pointing out two config parameters: spark.sql.broadcastTimeout and spark.sql.autoBroadcastJoinThreshold."
SPARK-24133,[SQL] Check for integer overflows when resizing WritableColumnVectors,PARQUET_VECTORIZED_READER_BATCH_SIZE RC_VECTORIZED_READER_BATCH_SIZE,https://issues.apache.org/jira/browse/SPARK-24133,https://github.com/apache/spark/commit/8bd27025b7cf0b44726b6f4020d294ef14dbbb7e,error message,/,/,/,add config content:  guidance 3->4,"This behavior is hard to troubleshoot for the users. Spark should instead check for negative requested capacity in WritableColumnVector.reserve() and throw more informative error, instructing the user to tweak ColumnarBatch size."
HBASE-22225,Profiler tab on Master/RS UI not working w/o comprehensive message,/,https://issues.apache.org/jira/browse/HBASE-22225,https://github.com/apache/hbase/commit/4b84ab32b372121f7fec50a4c7310b06e8a86ee0,error message,/,/,/,add config content: add guidance 3->4,"As titled, when checking 1.5.0 RC3 binary package, clicking the ""Profiler"" tab on HMaster/RegionServer web UI, it complains page not found error like below:"
CASSANDRA-14134,Migrate dtests to use pytest and python3,CASSANDRA_SKIP_SYNC,https://issues.apache.org/jira/browse/CASSANDRA-14134,https://github.com/apache/cassandra/commit/d6e508f33c1a7274b5826ad9d5ce814d719bd848,error message,/,/,/,add config content: add option name 1->3,Migrate dtests to use pytest and python3
SPARK-22968,[DSTREAM] Throw an exception on partition revoking issue,group id,https://issues.apache.org/jira/browse/SPARK-22968,https://github.com/apache/spark/commit/5fccdae18911793967b315c02c058eb737e46174,error message,/,/,/,add config content: add option name 1->3,"So instead of throwing an confused exception from Kafka, improve the exception message by identifying revoked partition and directly throw an meaningful exception when partition is revoked."
HDFS-13050,[SPS]: Create start/stop script to start external SPS process.,maxQueueLimitToScan,https://issues.apache.org/jira/browse/HDFS-13050,https://github.com/apache/hadoop/commit/5845c36c16c423107183287cce3be9357dad7564,error message,/,/,/,add config content: add param value 1->3,[SPS]: Create start/stop script to start external SPS process.
HBASE-21338,Warn if balancer is an ill-fit for cluster size,hbase.master.balancer.stochastic.runMaxSteps,https://issues.apache.org/jira/browse/HBASE-21338,https://github.com/apache/hbase/commit/807736fcf153d0d83eab50fd20d096d37c651ca9,error message,/,/,/,add configuration content: 1->4,"See parent issue. Running balancer on a cluster where the max steps was way inadequate, the balancer gave little to no indication that it was ill-configured. In fact, it only logged its starting and then that there was nothing to do though the cluster was obviously out-of-whack. To overcome above mentioned issue, just need to set  ""hbase.master.balancer.stochastic.runMaxSteps"" to true."
SPARK-26529,Add debug logs for confArchive when preparing local resource,confArchive,https://issues.apache.org/jira/browse/SPARK-26529,https://github.com/apache/spark/commit/eb42bb493b1d7c79e9516660b71aec66bdde5d51,error message,/,/,/,add configuration content: add config content: add debug log message point to conf 1->3,"Currently, `Client#createConfArchive` do not handle IOException, and some detail info is not provided in logs. Sometimes, this may delay the time of locating the root cause of io error."
HDFS-12725,BlockPlacementPolicyRackFaultTolerant fails with very uneven racks,/,https://issues.apache.org/jira/browse/HDFS-12725,https://github.com/apache/hadoop/commit/b00f828d84e4e029fd4786ebe827ce704a1b2a04,error message,/,/,/,add configuration content: point to conf 1->2,It might also be useful to log numResultsOflastChoose and totalReplicaExpected.
SPARK-24128,[SQL] Mention CONFIGuration option in implicit CROSS JOIN error,spark.sql.crossJoin.enabled,https://issues.apache.org/jira/browse/SPARK-24128,https://github.com/apache/spark/commit/cd12c5c3ecf28f7b04f566c2057f9b65eb456b7d,error message,/,/,/,add param name and setting guidance 2->4,Mention spark.sql.crossJoin.enabled in implicit cartesian product error msg
SPARK-24337,[CORE] Improve error messages for Spark conf values,catchIllegalValue,https://issues.apache.org/jira/browse/SPARK-24337,https://github.com/apache/spark/commit/0053e153faaa76ea38c845adab137d5be970e5af,error message,/,/,/,add param name and setting guidance 2->4,Improve the exception messages when retrieving Spark conf values to include the key name when the value is invalid.
HBASE-18808,Ineffective config check in BackupLogCleaner,BackupRestoreConstants.BACKUP_ENABLE_KEY,https://issues.apache.org/jira/browse/HBASE-18808,https://github.com/apache/hbase/commit/9f0863ce5f25276e22ac0d3b2c03b4f6f2296578,error message,/,/,/,change log level: warn -> debug,Ineffective config check in BackupLogCleaner
SPARK-26650,Demote noisy HBase-related log message.,"logDebug(""Unable to load HBaseConfiguration."" e)",https://issues.apache.org/jira/browse/SPARK-26650,https://github.com/apache/spark/commit/94ab4901dadbf5de95a88ec5b1b77efee2e764b7,error message,/,/,/,"change log level: warning -> debug, Avoids a noisy warning",So nothing is being thrown here. It's just an exception being logged. Agree that it's unnecessarily noisy
HBASE-21289,"Remove the log ""'hbase.regionserver.maxlogs' was deprecated."" in AbstractFSWAL",hbase.regionserver.maxlogs,https://issues.apache.org/jira/browse/HBASE-21289,https://github.com/apache/hbase/commit/fa5fa6ecdd071b72b58971058ff3ab9d28c3e709,error message,/,/,/,correctness,"This log was added by HBASE-14951. And the description and release note never said this config was deprecated. I thought HBASE-14951 only changed the default value of maxlogs (Please correct me if I am wrong). And we still use this config in our hbase book. So the log ""'hbase.regionserver.maxlogs' was deprecated."" in AbstractFSWAL is confused. Let's remove it. FYI Vladimir Rodionov"
HBASE-21816,Print source cluster replication config directory,/,https://issues.apache.org/jira/browse/HBASE-21816,https://github.com/apache/hbase/commit/a155d17fa6b0afab2c21d404af7ea92f195dcef0,error message,/,/,/,correctness,Print source cluster replication config directory
HDFS-12825,Fsck report shows CONFIG key name for min replication issues,dfs.namenode.replication.min,https://issues.apache.org/jira/browse/HDFS-12825,https://github.com/apache/hadoop/commit/ef7d334d364378070880e647eaf8bac2f12561ee,error message,/,/,/,correctness,Fsck report shows config key name for min replication issues
SPARK-24558,[CORE] wrong Idle Timeout value is used in case of the cacheBlock.,executorIdleTimeoutS,https://issues.apache.org/jira/browse/SPARK-24558,https://github.com/apache/spark/commit/2603ae30be78c6cb24a67c26fb781fae8763f229,error message,/,/,/,correctness,Driver prints the wrong info in the log when the executor which holds cacheBlock is IDLE.Time-out value displayed is not as per configuration value.
SPARK-25073,AM and Executor Memory validation message is not proper while submitting spark yarn application,yarn.nodemanager.resource.memory-mb,https://issues.apache.org/jira/browse/SPARK-25073,https://github.com/apache/spark/commit/c20916a5dc4a7e771463838e797cb944569f6259,error message,/,/,/,correctness,"As the error Message is bit misleading to the user  we can modify the same, We can keep the error message same as executor memory validation message."
HDFS-14056,Fix error messages in HDFS-12716.,dfs.datanode.failed.volumes.tolerated,https://issues.apache.org/jira/browse/HDFS-14056,https://github.com/apache/hadoop/commit/68d5dfdc78d121e89eeae4e577d670028a14a955,error message,/,/,/,correctness,"Here the error message seems a bit misleading. The error comes up when the given quantity in the configuration set to volsConfigured is set lower than -1 but in that case the error should say something like ""Value configured is either less than -1 or >= ...""."
SPARK Pull-Request #23030,Make memLimitExceededLogMessage more clean,/,https://github.com/apache/spark/pull/23030,https://github.com/apache/spark/commit/a00aaf649cb5a14648102b2980ce21393804f2c7,error message,/,/,/,correctness,"It‘s not very clear, because physical memory exceeds but suggestion contains virtual memory config. This pr makes it more clear and replace  deprecated config: ```spark.yarn.executor.memoryOverhead```"
SPARK-26564,Fix wrong assertions and error messages for PARAMETER checking,spark.network.timeout spark.executor.heartbeatInterval,https://issues.apache.org/jira/browse/SPARK-26564,https://github.com/apache/spark/commit/3bd77aa9f6775645407baa4266f6d0c02a8af902,error message,/,/,/,correctness,"it's misleading since it can be read as they could be equal. So this PR replaces ""no less than"" with ""greater than"". Also, it fixes similar inconsistencies found in MLlib and SQL components."
SPARK-23997,[SQL][FOLLOWUP] Update exception message,bucketingMaxBuckets,https://issues.apache.org/jira/browse/SPARK-23997,https://github.com/apache/spark/commit/9e0f9591afccc97cd54a133d8ed10512d14f4f91,error message,/,/,/,correctness,the message in an exception is not consistent of the current behavior.
HBASE-19647,Logging cleanups; emit regionname when RegionTooBusyException inside RetriesExhausted,HREGION_COLUMNFAMILY_FLUSH_SIZE_LOWER_BOUND,https://issues.apache.org/jira/browse/HBASE-19647,https://github.com/apache/hbase/commit/16cd9169768351fcdb6876d499888bd1834cca97,error message,/,/,/,readability,"Its missing the region name which is in the root exception, RegionTooBusyException – we just skip it."
HBASE-21536,Fix completebulkload usage instructions,/,https://issues.apache.org/jira/browse/HBASE-21536,https://github.com/apache/hbase/commit/420fbba6aefa58bb6ff252ff0c33e9b4ccae64b0,error message,/,/,/,readability,Fix completebulkload usage instructions
SPARK Pull-Request #20344,Typo fixes,/,https://github.com/apache/spark/pull/20344,https://github.com/apache/spark/commit/76b8b840ddc951ee6203f9cccd2c2b9671c1b5e8,error message,/,/,/,readability,Typo fixes
HBASE-18382,add transport type info into Thrift UI (#880),hbase.thrift.security.qop,https://issues.apache.org/jira/browse/HBASE-18382,https://github.com/apache/hbase/commit/82e155eb26cddd642e151d9276e2f4f281b4df8,error message,/,/,/,add config content: 1->1,"It would be really helpful to know if the Thrift server was started using the HTTP or binary transport. Any additional info, like QOP settings for SASL etc. would be great too. Right now the UI is very limited and shows true/false"
HBASE-23055,Alter hbase:meta Make it so hbase:meta can be altered.,configuredReplicaCount,https://issues.apache.org/jira/browse/HBASE-23055,https://github.com/apache/hbase/commit/9abdb7b5ae4de0b6d839f5c5d85e9bb899b5edb,error message,/,/,/,add config content: 1->3,Alter hbase:meta Make it so hbase:meta can be altered.
HBASE-21773,rowcounter utility should respond to pleas for help,hbase.client.scanner.caching mapreduce.map.speculative,https://issues.apache.org/jira/browse/HBASE-21773,https://github.com/apache/hbase/commit/3708198817e34bf67997c812b7f1fcef8119005,error message,/,/,/,add config content: 1->4,"For performance, consider the following configuration properties: -Dhbase.client.scanner.caching=100 -Dmapreduce.map.speculative=false"
SPARK Pull-Request #25045,Add requestHeaderSize debug log,UI_REQUEST_HEADER_SIZE,https://github.com/apache/spark/pull/25045,https://github.com/apache/spark/commit/0b6c2c259c1ed109a824b678c9ccbd9fd767d2f,error message,/,/,/,add config content: 1->3,Without debug log it's hard to find out on which side what configuration is used.
SPARK-30263,Don't log potentially sensitive value of non-Spark properties ignored in spark-submit,/,https://issues.apache.org/jira/browse/SPARK-30263,https://github.com/apache/spark/commit/46e950bea883b98cd3beb7bd637bffe52265643,error message,/,/,/,security,The value of non-Spark config properties ignored in spark-submit is no longer logged.
HDFS-14258,Introduce Java Concurrent Package To DataXceiverServer Class,DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY,https://issues.apache.org/jira/browse/HDFS-14258,https://github.com/apache/hadoop/commit/dde0ab55aadcf7c9cf71dbe36d90e97da6bc9498,exception action,/,/,add throw IllegalArgumentException,/,"In the unlikely scenario that a configuration change is made in the middle of operation (re-configuration), the current code in trunk will simply update a ""max thread"" variable to the new cap and exit. All new threads will fail to create if the cap is lowered, and eventually, running threads will complete and the total number of threads will drop below the cap."
HDFS-14207,ZKFC should catch exception when ha configuration missing,DFSZKFailoverController.create(parser.getConfiguration()）,https://issues.apache.org/jira/browse/HDFS-14207,https://github.com/apache/hadoop/commit/de34fc148ca4785c2ae7204df15096b955e671c4,exception action,/,/,"add try ctach, terminate ZKFC the invoked function may thorw illegalArgumentException but doesn't catch it ",/,"When i test hdfs zkfc with wrong configurations , i can not start zkfc process, and did not find any errors in log except command errors as bellow, Debug zkfc and deep into the code, i find that zkfc exit because of HadoopIllegalArgumentException. I think we should catch this exception and log it."
HDFS-12935,Get ambiguous result for DFSAdmin command in HA mode when only one namenode is up.,refreshSuperUserGroupsConfiguration(),https://issues.apache.org/jira/browse/HDFS-12935,https://github.com/apache/hadoop/commit/01bd6ab18fa48f4c7cac1497905b52e547962599,exception action,/,/,"add try ctach, System.out.println, still throw exception the invoked function may thorw illegalArgumentException but doesn't catch it ",/,Get ambiguous result for DFSAdmin command in HA mode when only one namenode is up Command still useful but throw exception for the connection failure to the down namenode.
HDFS-14193,RBF: Inconsistency with the Default Namespace,DFS_ROUTER_DEFAULT_NAMESERVICE,https://issues.apache.org/jira/browse/HDFS-14193,https://github.com/apache/hadoop/commit/c012b09fb678db11a10d0a8e1f64ee1a7229a417,exception action,/,/,change to default -> return with log.warn,/,"In the present scenario, if the default nameservice is not explicitly mentioned.Each router fallbacks to it local namespace as Default.There in each router having different default namespaces. Which leads to inconsistencies in operations and even blocks in maintaining a global uniform state. The outputs becomes specific to which router is serving the request and is different with different routers."
CASSANDRA-12014,Avoid assertion error when IndexSummary > 2G,min_sampling_level,https://issues.apache.org/jira/browse/CASSANDRA-12014,https://github.com/apache/cassandra/commit/ae88fd6c79b066f12ad76c2c1bfc1620d86bdbc5,exception action,/,/,excpetion -> log.error,/,"strongly suggest we consider this simplified patch for 3.0, and possibly for 3.11 and trunk as well. I don't think the size of the original patch is justified just to remove this assertion. The simplified patch is basically what you suggested initially, using a configurable default key size. Instead of failing with an assertion, it will log an error and use an incomplete index summary."
HDFS-12407,Journal node fails to shutdown cleanly if JournalNodeHttp……Server or JournalNodeRpcServer fails to start.,dfs.journalnode.http-address,https://issues.apache.org/jira/browse/HDFS-12407,https://github.com/apache/hadoop/commit/68282c8eacfedb0298741e6c41ff0b2ec71b018c,exception action,/,/,"add try ctach, Shutdown JournalNode with log, but no message",/,Journal nodes fails to shutdown cleanly if JournalNodeHttpServer or JournalNodeRpcServer fails to start. Change the http port for JournalNodeHttpServerr to some port which is already in use Start the journalnode. JournalNodeHttpServer start will fail with bind exception while journalnode process will continue to run.
HBASE-18846,Accommodate the hbase-indexer/lily/SEP consumer deploy-typePatch to start a standalone RegionServer that register's itself andoptionally stands up Services.,hbase.master.buffer.for.rs.fatals,https://issues.apache.org/jira/browse/HBASE-18846,https://github.com/apache/hbase/commit/456057ef90f152315a7f244141f3fca4ff748336,exception action,/,/,"add try ctach, give log message",/,Make sure we log the exception. HMaster is often started via reflection and the cause of failed startup is lost.
HBASE-22095,Taking a snapshot fails in local mode,Path defaultWorkingDir = getDefaultWorkingSnapshotDir(FSUtils.getRootDir(conf));,https://issues.apache.org/jira/browse/HBASE-22095,https://github.com/apache/hbase/commit/94c18f065365461a9cf9077a00cc72a4747980bd,exception action,/,/,add throw I/O exception,/,"Taking a snapshot fails in local mode, @throws IOException if we can't get the root dir"
SPARK-22676,Avoid iterating all partition paths when spark.sql.hive.verifyPartitionPath=true,FileInputFormat.INPUT_DIR,https://issues.apache.org/jira/browse/SPARK-22676,https://github.com/apache/spark/commit/ed4101d29f50d54fd7846421e4c00e9ecd3599d0,exception action,/,/,"add try ctach, empty the array and add log warning",/,"In current code, it will scanning all partition paths when spark.sql.hive.verifyPartitionPath=true. It costs much time and memory cost."
HDFS-14825,[Dynamometer] Workload doesn't start unless an absolute path of Mapper class given.,getConf().getClassByName(qualifiedClassName),https://issues.apache.org/jira/browse/HDFS-14825,https://github.com/apache/hadoop/commit/54e760511a2e2f8e5ecf1eb8762434fcd041f4d,exception action,/,/,add throw IllegalArgumentException,/,"When starting a workload by start-workload.sh, unless an absolute path of Mapper is given, the workload doesn't start."
HBASE-22054,Space Quota: Compaction is not working for super user in case of NO_WRITES_COMPACTION,Superusers.initialize(newConf),https://issues.apache.org/jira/browse/HBASE-22054,https://github.com/apache/hbase/commit/e44fe4964a25644de2912a08a55784f2268f5e9c,exception action,/,/,add try catch with log.ware,/,Space Quota: Compaction is not working for super user. Compaction command is issued successfully at client but actually compaction is not happening.
SPARK-23491,[SS] Remove explicit job cancellation from ContinuousExecution reCONFIGuring,reconfiguring,https://issues.apache.org/jira/browse/SPARK-23491,https://github.com/apache/spark/commit/7ec83658fbc88505dfc2d8a6f76e90db747f1292,exception action,/,/,exception -> log,/,interrupted by reconfiguration - swallow exception so we can restart the query
HDFS-13654,Use a random secret when a secret file doesn't exist in HttpFS. This should be default.,httpfs.authentication.signature.secret.file,https://issues.apache.org/jira/browse/HDFS-13654,https://github.com/apache/hadoop/commit/35f1014b3e10eee27f1976f4af9815a0c2d7dacd,exception action,/,/,auto-correction: use a random secre without log info,/,"The patch fixes HttpFSAuthenticationFilter to use a random secret when the secret file specified by httpfs.authentication.signature.secret.file doesn't exist. And it removes the default secret file, httpfs-signature.secret. This is same as hadoop.http.authentication.signature.secret.fileis."
HBASE-18784,if available query underlying outputstream capabilities where we need hflush/hsync.,CommonFSUtils.HBASE_WAL_DIR,https://issues.apache.org/jira/browse/HBASE-18784,https://github.com/apache/hbase/commit/e79a007dd9810b33cd508986037e17d45b55a705,exception action,/,/,change exception type,/,explicit exception type(CommonFSUtils.StreamLacksCapabilityException)
HDFS-14486,The exception classes in some throw statements do not accurately describe why they are thrown.,dfs.datanode.failed.volumes.tolerated,https://issues.apache.org/jira/browse/HDFS-14486,https://github.com/apache/hadoop/commit/e1dfc060f8f0247f97127c75c9284a068fc93907,exception action,/,/,change exception type: DiskErrorException -> HadoopIllegalArgumentException,/,"Our prototype has spotted a few throw statements whose exception class does not accurately describe why they are thrown. This can be dangerous since it makes correctly handling them challenging. For example, in an old bug, HDFS-8224, throwing a general IOException makes it difficult to perform data recovery specifically when a metadata file is corrupted."
HDFS-14486,The exception classes in some throw statements do not accurately describe why they are thrown.,DFS_DATANODE_DISK_CHECK_TIMEOUT_KEY,https://issues.apache.org/jira/browse/HDFS-14486,https://github.com/apache/hadoop/commit/e1dfc060f8f0247f97127c75c9284a068fc93907,exception action,/,/,change exception type: DiskErrorException -> HadoopIllegalArgumentException,/,"Our prototype has spotted a few throw statements whose exception class does not accurately describe why they are thrown. This can be dangerous since it makes correctly handling them challenging. For example, in an old bug, HDFS-8224, throwing a general IOException makes it difficult to perform data recovery specifically when a metadata file is corrupted."
HDFS-14486,The exception classes in some throw statements do not accurately describe why they are thrown.,DFS_DATANODE_DISK_CHECK_MIN_GAP_KEY,https://issues.apache.org/jira/browse/HDFS-14486,https://github.com/apache/hadoop/commit/e1dfc060f8f0247f97127c75c9284a068fc93907,exception action,/,/,change exception type: DiskErrorException -> HadoopIllegalArgumentException,/,"Our prototype has spotted a few throw statements whose exception class does not accurately describe why they are thrown. This can be dangerous since it makes correctly handling them challenging. For example, in an old bug, HDFS-8224, throwing a general IOException makes it difficult to perform data recovery specifically when a metadata file is corrupted."
HBASE-19524,Master side changes for moving peer modification from zk watcher to procedure,IllegalArgumentException,https://issues.apache.org/jira/browse/HBASE-19524,https://github.com/apache/hbase/commit/7f4bd0d371e9faf59076468e5efe416045eb525a,exception action,/,/,change exception type: DiskErrorException -> HadoopIllegalArgumentException,/,
HBASE-22701,Disable the DynamicClassLoader when it fails to initialize,initTempDir(conf),https://issues.apache.org/jira/browse/HBASE-22701,https://github.com/apache/hbase/commit/fbd5b5e32753104f88600b0f4c803ab5659bce6,exception action,/,/,"auto-correct, add try ctach, give log message",/,"If you give HBase an hbase.local.dir (usually, ""hbase.tmp.dir/local"") which is not writable to it, you will get some weird errors on the scan path I see two options: We abort the RegionServer when the DynamicClassLoader fails to run We catch the exception and treat the DynamicClassLoader as disabled (same action as if you had set hbase.use.dynamic.jars=false). I want to do #1 so that we don't propagate bogus configuration, but it feels a bit ""harsh"" to do that. I think #2 is the right solution with a big-fat-warning."
SPARK-28778,added a fallback to `Utils.localHostName()` in Spark Executors when `--hostname` is not provided,hostname == null,https://issues.apache.org/jira/browse/SPARK-28778,https://github.com/apache/spark/commit/f17f1d01e2dca76113d1d39ec1d03fcec9d72b6,exception action,/,/,auto-correction: use '$hostname' to advertise itself,/,added a fallback to `Utils.localHostName()` in Spark Executors when `--hostname` is not provided
SPARK-29675,Add exception when isolationLevel is Illegal,,https://issues.apache.org/jira/browse/SPARK-29675,https://github.com/apache/spark/commit/888cc4601a33f7b2479fa40d05dc23a3d05575e,exception action,/,/,add throw IllegalArgumentException,/,"Now we use JDBC api and set an Illegal isolationLevel option, spark will throw a `scala.MatchError`, it's not friendly to user. So we should add an IllegalArgumentException."