Issue-id,Title,Parameter,Issue-URL,Commit-URL,Type,Note
HBASE-18108,Procedure WALs are archived but not cleaned,hbase.master.logcleaner.plugins,https://issues.apache.org/jira/browse/HBASE-18108,https://github.com/apache/hbase/commit/023d4f1ae8081da3cb9ff54e6b2e545799704ce7#,acceptble value change: new class,The TimeToLiveProcedureWALCleaner is now added to hbase.master.logcleaner.plugins to clean the 2 WALs in one run.
CASSANDRA-14482,ZSTD Compressor support in Cassandra,commitlog_compression,https://issues.apache.org/jira/browse/CASSANDRA-14482,https://github.com/apache/cassandra/commit/dccf53061a61e7c632669c60cd94626e405518e9#,acceptble value change: new class,ZSTD Compressor support in Cassandra
HBASE-19187,Remove option to create on heap bucket cache.,hbase.bucketcache.ioengine,https://issues.apache.org/jira/browse/HBASE-19187,https://github.com/apache/hbase/commit/bff619ef7b100e8b09f7f5eb0f6e289ca51de096#,acceptble value change: new mode,"Removing the on heap Bucket cache feature. The config ""hbase.bucketcache.ioengine"" no longer support the 'heap' value. Its supported values now are 'offheap', 'file:<path>', 'files:<path>' and 'mmap:<path>"
SPARK-24360,Support Hive 3.1 metastore,spark.sql.hive.metastore.version,https://issues.apache.org/jira/browse/SPARK-24360,https://github.com/apache/spark/commit/aeff69bd879661367367f39b5dfecd9a76223c0b#,acceptble value change: new version,Hive 3.1.1 is released. This PR aims to support Hive 3.1.x metastore.
SPARK-27418,[SQL] Migrate Parquet to File Data Source V2,spark.sql.sources.write.useV1SourceList,https://issues.apache.org/jira/browse/SPARK-27418,https://github.com/apache/spark/commit/23ebd389b5cb528a7ba04113a12929bebfaf1e9a#,acceptble value change: value range,Support parquet
SPARK-17788,[SQL] fix the potential OOM in UnsafeExternalSorter and ShuffleExternalSorter,spark.shuffle.spill.numElementsForceSpillThreshold,https://issues.apache.org/jira/browse/SPARK-17788,https://github.com/apache/spark/commit/079a2609d7ad0a7dd2ec3eaa594e6ed8801a8008#,type change: long -> int,"The Double values I'm trying to sort are mostly in the range [0,1] (~70% of the data which roughly equates 1 billion records), other numbers in the dataset are as high as 2000."
HBASE-18511,Default no regions on master,hbase.balancer.tablesOnMaster,https://issues.apache.org/jira/browse/HBASE-18511,https://github.com/apache/hbase/commit/473446719b7b81b56216862bf2a94a576ff90f60#,type change: mode -> bool,Changes the configuration hbase.balancer.tablesOnMaster from list of table names to instead be a boolean; true if master carries tables/regions and false if it does not.
CASSANDRA-13990,Remove obsolete OldNetworkTopologyStrategy,replication_strategies,https://issues.apache.org/jira/browse/CASSANDRA-13990,https://github.com/apache/cassandra/commit/7c5904753f4ede492f1a5a5e68edfe37651a5be6,acceptble value change: value range,"Removed the strategy from cqlsh autocomplete, including an array for replication_factor autocomplete that was only used for SimpleStrategy and OldNetworkTopologyStrategy."
SPARK-30074,The maxNumPostShufflePartitions config should obey reducePostShufflePartitions enabled,spark.sql.adaptive.shuffle.maxNumPostShufflePartitions,https://issues.apache.org/jira/browse/SPARK-30074,https://github.com/apache/spark/commit/d1465a1b0dea690fcfbf75edb73ff9f8a015c0d,dependency,The maxNumPostShufflePartitions config should obey reducePostShufflePartitions enabled
SPARK-21012,[SUBMIT] Add glob support for resources adding to Spark,spark.jars,https://issues.apache.org/jira/browse/SPARK-21012,https://github.com/apache/spark/commit/5800144a54f5c0180ccf67392f32c3e8a51119b1#,acceptble value change: glob path,"Current ""--jars (spark.jars)"", ""--files (spark.files)"", ""--py-files (spark.submit.pyFiles)"" and ""--archives (spark.yarn.dist.archives)"" only support non-glob path. This is OK for most of the cases, but when user requires to add more jars, files into Spark, it is too verbose to list one by one. So here propose to add glob path support for resources."
SPARK-21012,[SUBMIT] Add glob support for resources adding to Spark,spark.files,https://issues.apache.org/jira/browse/SPARK-21012,https://github.com/apache/spark/commit/5800144a54f5c0180ccf67392f32c3e8a51119b1#,acceptble value change: glob path,"Current ""--jars (spark.jars)"", ""--files (spark.files)"", ""--py-files (spark.submit.pyFiles)"" and ""--archives (spark.yarn.dist.archives)"" only support non-glob path. This is OK for most of the cases, but when user requires to add more jars, files into Spark, it is too verbose to list one by one. So here propose to add glob path support for resources."
SPARK-21012,[SUBMIT] Add glob support for resources adding to Spark,spark.submit.pyFiles,https://issues.apache.org/jira/browse/SPARK-21012,https://github.com/apache/spark/commit/5800144a54f5c0180ccf67392f32c3e8a51119b1#,acceptble value change: glob path,"Current ""--jars (spark.jars)"", ""--files (spark.files)"", ""--py-files (spark.submit.pyFiles)"" and ""--archives (spark.yarn.dist.archives)"" only support non-glob path. This is OK for most of the cases, but when user requires to add more jars, files into Spark, it is too verbose to list one by one. So here propose to add glob path support for resources."
SPARK-21012,[SUBMIT] Add glob support for resources adding to Spark,spark.yarn.dist.archives,https://issues.apache.org/jira/browse/SPARK-21012,https://github.com/apache/spark/commit/5800144a54f5c0180ccf67392f32c3e8a51119b1#,acceptble value change: glob path,"Current ""--jars (spark.jars)"", ""--files (spark.files)"", ""--py-files (spark.submit.pyFiles)"" and ""--archives (spark.yarn.dist.archives)"" only support non-glob path. This is OK for most of the cases, but when user requires to add more jars, files into Spark, it is too verbose to list one by one. So here propose to add glob path support for resources."
SPARK-24646,[CORE] Minor change to spark.yarn.dist.forceDownloadSchemes to support wildcard '*',spark.yarn.dist.forceDownloadSchemes,https://issues.apache.org/jira/browse/SPARK-24646,https://github.com/apache/spark/commit/e2c7e09f742a7e522efd74fe8e14c2620afdb522#,acceptble value change: surpport *,"Minor change to spark.yarn.dist.forceDownloadSchemes to support wildcard '*', For the ease of using this configuration, here propose to add wildcard '*' support to `spark.yarn.dist.forceDownloadSchemes`"
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_CLIENT_CACHE_READAHEAD,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_NAMENODE_MAX_XATTR_SIZE_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_NAMENODE_MAX_COMPONENT_LENGTH_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_NAMENODE_MIN_BLOCK_SIZE_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_IMAGE_TRANSFER_RATE_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_IMAGE_TRANSFER_BOOTSTRAP_STANDBY_RATE_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_NAMENODE_DU_RESERVED_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
HDFS-9872,HDFS bytes-default configurations should accept multiple size unit,DFS_IMAGE_TRANSFER_CHUNKSIZE_KEY,https://issues.apache.org/jira/browse/HDFS-9872,https://github.com/apache/hadoop/commit/88cce32551e6d52fd1c5a5bfd6c41499bf6ab1ab#,type change,HDFS bytes-default configurations should accept multiple size unit
SPARK-27256,"If the configuration is used to set the number of bytes, we'd better use `bytesConf`",spark.sql.files.maxPartitionBytes,https://issues.apache.org/jira/browse/SPARK-27256,https://github.com/apache/spark/commit/e4b36df2c0ae3bdba4484f9f92461dbb528d8fb9#,type change: long -> byte,"Currently, if we want to configure `spark. sql. files. maxPartitionBytes` to 256 megabytes, we must set `spark. sql. files. maxPartitionBytes=268435456`, which is very unfriendly to users."
SPARK-27256,"If the configuration is used to set the number of bytes, we'd better use `bytesConf`",spark.files.maxPartitionBytes,https://issues.apache.org/jira/browse/SPARK-27256,https://github.com/apache/spark/commit/e4b36df2c0ae3bdba4484f9f92461dbb528d8fb9#,type change: long -> byte,"Currently, if we want to configure `spark. sql. files. maxPartitionBytes` to 256 megabytes, we must set `spark. sql. files. maxPartitionBytes=268435456`, which is very unfriendly to users."
SPARK-27256,"If the configuration is used to set the number of bytes, we'd better use `bytesConf`",spark.files.openCostInBytes,https://issues.apache.org/jira/browse/SPARK-27256,https://github.com/apache/spark/commit/e4b36df2c0ae3bdba4484f9f92461dbb528d8fb9#,type change: long -> byte,"Currently, if we want to configure `spark. sql. files. maxPartitionBytes` to 256 megabytes, we must set `spark. sql. files. maxPartitionBytes=268435456`, which is very unfriendly to users."
SPARK-27256,"If the configuration is used to set the number of bytes, we'd better use `bytesConf`",spark.shuffle.sort.initialBufferSize,https://issues.apache.org/jira/browse/SPARK-27256,https://github.com/apache/spark/commit/e4b36df2c0ae3bdba4484f9f92461dbb528d8fb9#,type change: long -> byte,"Currently, if we want to configure `spark. sql. files. maxPartitionBytes` to 256 megabytes, we must set `spark. sql. files. maxPartitionBytes=268435456`, which is very unfriendly to users."
SPARK-27256,"If the configuration is used to set the number of bytes, we'd better use `bytesConf`",spark.shuffle.spill.initialMemoryThreshold,https://issues.apache.org/jira/browse/SPARK-27256,https://github.com/apache/spark/commit/e4b36df2c0ae3bdba4484f9f92461dbb528d8fb9#,type change: long -> byte,"Currently, if we want to configure `spark. sql. files. maxPartitionBytes` to 256 megabytes, we must set `spark. sql. files. maxPartitionBytes=268435456`, which is very unfriendly to users."
SPARK-22845,Modify spark.kubernetes.allocation.batch.delay to take time instead of int,spark.kubernetes.allocation.batch.delay,https://issues.apache.org/jira/browse/SPARK-22845,https://github.com/apache/spark/commit/0114c89d049724b95f7823b957bf33790216316b#,type change: long -> time,Fixing configuration that was taking an int which should take time. Made the granularity milliseconds as opposed to seconds since there's a use-case for sub-second reactions to scale-up rapidly especially with dynamic allocation.
SPARK-29151,Support fractional resources for task resource scheduling,spark.task.resource.{resourceName}.amount,https://issues.apache.org/jira/browse/SPARK-21287,https://github.com/apache/spark/commit/3cb18d90c441bbaa64c693e276793b670213e59,acceptble value change: surpport fractional,There is a configuration change where `spark.task.resource.[resource type].amount` can now be fractional.
HDFS-14719,Correct the safemode threshold value in BlockManagerSafeMode.,DFS_NAMENODE_REPL_QUEUE_THRESHOLD_PCT_KEY,https://issues.apache.org/jira/browse/HDFS-14719,https://github.com/apache/hadoop/commit/34681643e92774da6f74826c468ecec4dcbedf5,type change: double -> float,"BlockManagerSafeMode is doing wrong parsing for safemode threshold. It is storing float value in double, which will give different result some time."
HDFS-14158,Checkpointer ignores configured time period > 5 minutes,dfs.namenode.checkpoint.period,https://issues.apache.org/jira/browse/HDFS-14158,https://github.com/apache/hadoop/commit/9aa3dc872ca9a528cb98ef56d9a33ab9d4531aa1#,acceptble value change: value range,"Are you running BackupNode? It has received little to no attention since people have not used it much for a long time. The standard way of checkpointing until the HA feature was to use secondary namenode, which has its own checks. ""periodMsec is always 5 minutes or lower"" might have been intentional and reasonable long time ago when BackupNode was first created."
HDFS-12716,dfs.datanode.failed.volumes.tolerated' to support minimum number of volumes to be available.,dfs.datanode.failed.volumes.tolerated,https://issues.apache.org/jira/browse/HDFS-12716,https://github.com/apache/hadoop/commit/3108d27edde941d153a58f71fb1096cce2995531#,acceptble value change: value range,"Support 'dfs.datanode.failed.volumes.tolerated' to accept special 'negative value 'x' to tolerate failures of upto ""n-x"""
SPARK-21287,Remove requirement of fetch_size>=0 from JDBCOptions,JDBC_BATCH_FETCH_SIZE,https://issues.apache.org/jira/browse/SPARK-21287,https://github.com/apache/spark/commit/92b25295ca0dc5b80aaddb1c8f8d5ef0a250d11,acceptble value change: value range,Remove the requirement of fetch_size>=0 from JDBCOptions to allow negative fetch size.