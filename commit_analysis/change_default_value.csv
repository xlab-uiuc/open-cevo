Issue-id,Param,Title,Issue-URL,Commit-URL,Param-type,new scenario/use case,Old/New value,Impact,Note,How to choose new value
HBASE-19148,hbase.regionserver.hlog.blocksize,Reevaluate default values of CONFIGurations,https://issues.apache.org/jira/browse/HBASE-19148,https://github.com/apache/hbase/commit/4d6b928682cc2a17f3dfd0179fb3fd46fd9e0a1f,block size,workload: write load was significant,DefaultBlockSize -> 2*DefaultBlockSize,resource overuse: extra fractional blocks,"We found it better to just bump up the block size, and make this low enough to be sure we never exceeded the block. Then in all the files were no larger or more frequent, but less extra fractional blocks. Maybe we should just increase the block size for the WAL files along with turning down the logroll multiplier.",fix bugs
HBASE-19148,hbase.regionserver.logroll.multiplier,Reevaluate default values of CONFIGurations,https://issues.apache.org/jira/browse/HBASE-19148,https://github.com/apache/hbase/commit/4d6b928682cc2a17f3dfd0179fb3fd46fd9e0a1f,multiplier,workload: write load was significant,0.95 - 0.5,resource overuse: many log files that had a little bit more than 1 HDFS block worth of data,"With the default of 0.95, whenever write load was significant, we saw many log files that had a little bit more than 1 HDFS block worth of data. Seemed a waste.",no discussion
SPARK-25904,spark.storage.memoryMapLimitForTests,Allocate arrays smaller than Int.MaxValue,https://issues.apache.org/jira/browse/SPARK-25904,https://github.com/apache/spark/commit/8fbc1830f962c446b915d0d8ff2b13c5c75d22fc,chuck size,workload: Replicating a block > 2GB,Int.MaxValue -> Integer.MAX_VALUE - 15,failure: OOM Requested array size exceeds VM limit,"In a few places spark can try to allocate arrays as big as Int.MaxValue, but thats actually too big for the JVM. We should consistently use ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH instead. Replicating a block > 2GB currently fails because it tries to allocate a bytebuffer that is just a bit too large",fix bugs
SPARK-25904,spark.sql.sortMergeJoinExec.buffer.in.memory.threshold,Allocate arrays smaller than Int.MaxValue,https://issues.apache.org/jira/browse/SPARK-25904,https://github.com/apache/spark/commit/8fbc1830f962c446b915d0d8ff2b13c5c75d22fc,threshold,workload: Replicating a block > 2GB,Int.MaxValue -> Integer.MAX_VALUE - 15,failure: OOM Requested array size exceeds VM limit,"In a few places spark can try to allocate arrays as big as Int.MaxValue, but thats actually too big for the JVM. We should consistently use ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH instead. Replicating a block > 2GB currently fails because it tries to allocate a bytebuffer that is just a bit too large",fix bugs
SPARK-25904,spark.sql.execution.topKSortFallbackThreshold,Allocate arrays smaller than Int.MaxValue,https://issues.apache.org/jira/browse/SPARK-25904,https://github.com/apache/spark/commit/8fbc1830f962c446b915d0d8ff2b13c5c75d22fc,threshold,workload: Replicating a block > 2GB,Int.MaxValue -> Integer.MAX_VALUE - 15,failure: OOM Requested array size exceeds VM limit,"In a few places spark can try to allocate arrays as big as Int.MaxValue, but thats actually too big for the JVM. We should consistently use ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH instead.",fix bugs
SPARK-24805,avro.mapred.ignore.inputs.without.extension,[SQL] Do not ignore avro files without extensions by default,https://issues.apache.org/jira/browse/SPARK-24805,https://github.com/apache/spark/commit/ba437fc5c73b95ee4c59327abf3161c58f64cb12,feature selection,workload: Other systems can create avro files without extensions,true -> false,unexpected result: get just partitial results silently,"Currently to read files without .avro extension, users have to set the flag avro.mapred.ignore.inputs.without.extension to false (by default it is true). Other systems can create avro files without extensions. When users try to read such files, they get just partitial results silently. The behaviour may confuse users. Current behavior is different behavior from another supported datasource CSV and JSON.",/
HBASE-19148,hbase.master.fileSplitTimeout,Reevaluate default values of CONFIGurations,https://issues.apache.org/jira/browse/HBASE-19148,https://github.com/apache/hbase/commit/4d6b928682cc2a17f3dfd0179fb3fd46fd9e0a1f,timeout,workload: large regions under load,30s -> 10m,perf: gets stuck in a cycle where they fail worse,"Painful when the largest region splits are the ones that fail, and gets stuck in a cycle where they fail worse. suggested by production experience(not enough for some large regions under load)",no discussion
HBASE-21000,hbase.hstore.compaction.throughput.higher.bound,Default limits for PressureAwareCompactionThroughputController are too low,https://issues.apache.org/jira/browse/HBASE-21000,https://github.com/apache/hbase/commit/6d08ffcfc6e646751b35b111389403329b121c89,threshold,workload: In fairly light load scenarios,20L * 1024 * 1024 -> 100L * 1024 * 1024,failure: RegionTooBusy exceptions,"In fairly light load scenarios we see compaction quickly falls behind and write clients are backed off or failing due to RegionTooBusy exceptions. Although compaction throughput becomes unbounded after the store reaches the blocking file count, in the lead up to this the default settings do not provide enough bandwidth to stave off blocking.",performance test
HBASE-21000,hbase.hstore.compaction.throughput.lower.bound,Default limits for PressureAwareCompactionThroughputController are too low,https://issues.apache.org/jira/browse/HBASE-21000,https://github.com/apache/hbase/commit/6d08ffcfc6e646751b35b111389403329b121c89,threshold,workload: In fairly light load scenarios,10L * 1024 * 1024 -> 50L * 1024 * 1024,failure: RegionTooBusy exceptions,"In fairly light load scenarios we see compaction quickly falls behind and write clients are backed off or failing due to RegionTooBusy exceptions. Although compaction throughput becomes unbounded after the store reaches the blocking file count, in the lead up to this the default settings do not provide enough bandwidth to stave off blocking.",performance test
HBASE-19148,hbase.master.loadbalance.bytable,Reevaluate default values of CONFIGurations,https://issues.apache.org/jira/browse/HBASE-19148,https://github.com/apache/hbase/commit/4d6b928682cc2a17f3dfd0179fb3fd46fd9e0a1f,feature selection,workload: If running with tables with different workloads or profiles,false -> true -> false,resource problem,"If running with tables with different workloads or profiles, it can be surprising to find a single table very unbalanced on the cluster",/
HBASE-18023,hbase.rpc.rows.warning.threshold,Update row threshold warning from 1k to 5k (addendum),https://issues.apache.org/jira/browse/HBASE-18023,https://github.com/apache/hbase/commit/5cd7f630c21c32b183ec23adcfcd16595d7e72a3,threshold,workload: do something like a large multi-put,1000 -> 5000,failure: crash a region server,"HBASE-18023 introduces a warning message in the RegionServer log when an RPC is received from a client that has more than 5000 ""actions"" (where an ""action"" is a collection of mutations for a specific row) in a single RPC. Misbehaving clients who send large RPCs to RegionServers can be malicious, causing temporary pauses via garbage collection or denial of service via crashes. The threshold of 5000 actions per RPC is defined by the property ""hbase.rpc.rows.warning.threshold"" in hbase-site.xml.",fix bugs
HDFS-14675,dfs.datanode.balance.bandwidthPerSec,Increase Balancer Defaults Further.,https://issues.apache.org/jira/browse/HDFS-14675,https://github.com/apache/hadoop/commit/93daf69f90df650a6c5fb33f79e51878ad8985c,bandwidth,workload: dfs.datanode.balance.max.concurrent.moves = 50,10m -> 100m,perf: users find the balancer operates too slowly with 10MB/s.,"HDFS-10297 increased the balancer defaults to 50 for dfs.datanode.balance.max.concurrent.moves and to 10MB/s for dfs.datanode.balance.bandwidthPerSec. We have found that these settings often have to be increased further as users find the balancer operates too slowly with 50 and 10MB/s. We often recommend moving concurrent moves to between 200 and 300 and setting the bandwidth to 100 or even 1000MB/s, and these settings seem to work well in practice. I would like to suggest we increase the balancer defaults further. I would suggest 100 for concurrent moves and 100MB/s for the bandwidth, but I would like to know what others think on this topic too",/
SPARK-24297,spark.maxRemoteBlockSizeFetchToMem,[CORE] Fetch-to-disk by default for > 2gb,https://issues.apache.org/jira/browse/SPARK-24297,https://github.com/apache/spark/commit/15fff79032f6d708d8570b5e83144f1f84519552,fetch size,workload: Any network request which does not use stream-to-disk that is sending over 2GB,Long.MaxValue -> Int.MaxValue - 512,failure: request failed,"Any network request which does not use stream-to-disk that is sending over 2GB is doomed to fail, so we might as well at least set the default value of spark.maxRemoteBlockSizeFetchToMem to something < 2GB. It probably makes sense to set it to something even lower still, but that might require more careful testing; this is a totally safe first step.",fix bugs
HBASE-19660,hbase.client.retries.number,Up default retries from 10 to 15 and blocking store files limit from 10 to 16,https://issues.apache.org/jira/browse/HBASE-19660,https://github.com/apache/hbase/commit/b3a4fca48ffcac44ec1e1d53a99caa18b7e14cbf,retry num,workload: 40-odd WALs to split,10 -> 15,failure: can't ride over a crashed server,10 doesn't seem to be enough to make it across a server crash when there are 40-odd WALs to split. Upping it until we do more work on MTTR.,fix bugs: Upping it until we do more work on MTTR.
CASSANDRA-14855,native_transport_flush_in_batches_legacy,Disable immediate flusher by default for cassandra-3.0 and cassandra-3.11,https://issues.apache.org/jira/browse/CASSANDRA-14855,https://github.com/apache/cassandra/commit/b82a42fd9ae99dc115ec04339f4265096bb45044,feature selection,workload: `queued` data member of the Flusher got too big,true -> false,/,"I would personally find any of the options somewhat acceptable, including introducing this change, or doing so without changing the default property. This would leave users exposed to this (presumably uncommon) bug, but with an easy option for fixing it by switching the config property. I also like the suggestion of keeping the existing flusher ON by default, and making ImmediateFlusher usage optional (through yaml property like native_transport_flush_immediate which is set to false by default) - I can work on a patch for that. Let me know.",/
SPARK-23310,spark.unsafe.sorter.spill.read.ahead.enabled,[CORE] Turn off read ahead input stream for unshafe shuffle reader,https://issues.apache.org/jira/browse/SPARK-23310,https://github.com/apache/spark/commit/03b7e120dd7ff7848c936c7a23644da5bd7219ab,feature selection,workload While running all TPC-DS queries with SF set to 1000,true -> false,perf: lock congestion issue,"Specially, ReadAheadInputStream gets lock congestion. After setting spark.unsafe.sorter.spill.read.ahead.enabled set to false, the regression disappear and the overall performance of all TPC-DS queries has improved.",/
SPARK-23366,spark.unsafe.sorter.spill.read.ahead.enabled,Improve hot reading path in ReadAheadInputStream,https://issues.apache.org/jira/browse/SPARK-23366,https://github.com/apache/spark/commit/7539ae59d6c354c95c50528abe9ddff6972e960f,feature selection,workload reading small amounts of data,false -> true,resource overuse,"However, investigating flamegraphs of profiles from investigating some regressed workloads after switch to Spark 2.3, it seems that the hot path of reading small amounts of data (like readInt) is inefficient - it involves taking locks, and multiple checks.",/
SPARK-26709,spark.sql.optimizer.metadataOnly,[SQL] OptimizeMetadataOnlyQuery does not handle empty records correctly,https://issues.apache.org/jira/browse/SPARK-26709,https://github.com/apache/spark/commit/f5b9370da2745a744f8b2f077f1690e0e7035140,feature selection,workload files are empty,true -> false,unexpected result,"By default the optimization is disabled, since it may return incorrect results when the files are empty.",/
HDFS-10326,dfs.client.socket.send.buffer.size,Disable setting tcp socket send/receive buffers for write pipelines,https://issues.apache.org/jira/browse/HDFS-10326,https://github.com/apache/hadoop/commit/71b8dda4f6ff6006410f3a9fe7717aa096004b1b,buffer size,workload 10Gb networks,128k ->  0,recourse underutilized,"The hardcoded value will saturate a 1Gb with 1ms RTT. 105Mbs at 10ms. Paltry 11Mbs over a 100ms long haul. 10Gb networks are underutilized. The reason why the original patches didn't set the default was basically that we wanted to be conservative. Basically, we were adding the option to use auto-tuning, but not making it the default. If we strongly believe that auto-tuning should be the default, we should make these options default to 0 unless set by the admin.",no discussion
HDFS-10326,dfs.datanode.transfer.socket.send.buffer.size,Disable setting tcp socket send/receive buffers for write pipelines,https://issues.apache.org/jira/browse/HDFS-10326,https://github.com/apache/hadoop/commit/71b8dda4f6ff6006410f3a9fe7717aa096004b1b,buffer size,workload 10Gb networks,128k ->  0,recourse underutilized,"The hardcoded value will saturate a 1Gb with 1ms RTT. 105Mbs at 10ms. Paltry 11Mbs over a 100ms long haul. 10Gb networks are underutilized. The reason why the original patches didn't set the default was basically that we wanted to be conservative. Basically, we were adding the option to use auto-tuning, but not making it the default. If we strongly believe that auto-tuning should be the default, we should make these options default to 0 unless set by the admin.",no discussion
HDFS-10326,dfs.datanode.transfer.socket.recv.buffer.size,Disable setting tcp socket send/receive buffers for write pipelines,https://issues.apache.org/jira/browse/HDFS-10326,https://github.com/apache/hadoop/commit/71b8dda4f6ff6006410f3a9fe7717aa096004b1b,buffer size,workload 10Gb networks,128k ->  0,recourse underutilized,"The hardcoded value will saturate a 1Gb with 1ms RTT. 105Mbs at 10ms. Paltry 11Mbs over a 100ms long haul. 10Gb networks are underutilized. The reason why the original patches didn't set the default was basically that we wanted to be conservative. Basically, we were adding the option to use auto-tuning, but not making it the default. If we strongly believe that auto-tuning should be the default, we should make these options default to 0 unless set by the admin.",no discussion
HDFS-10326,DEFAULT_DATA_SOCKET_SIZE,Disable setting tcp socket send/receive buffers for write pipelines,https://issues.apache.org/jira/browse/HDFS-10326,https://github.com/apache/hadoop/commit/71b8dda4f6ff6006410f3a9fe7717aa096004b1b,sockect size,workload 10Gb networks,128k ->  0,recourse underutilized,"The hardcoded value will saturate a 1Gb with 1ms RTT. 105Mbs at 10ms. Paltry 11Mbs over a 100ms long haul. 10Gb networks are underutilized. The reason why the original patches didn't set the default was basically that we wanted to be conservative. Basically, we were adding the option to use auto-tuning, but not making it the default. If we strongly believe that auto-tuning should be the default, we should make these options default to 0 unless set by the admin.",no discussion
SPARK-28885,spark.sql.storeAssignmentPolicy,[SQL] Follow ANSI store assignment rules in table insertion by default,https://issues.apache.org/jira/browse/SPARK-28885,https://github.com/apache/spark/commit/322ec0ba9ba75708cfe679368a43655de7b0e4f,feature selection,proactive improvement (Intelligibility),Strict -> ANSI,/,"[old value]: As far as I know, no mainstream DBMS is using this policy by default. [new value]: the behavior is mostly the same as PostgreSQL",/
SPARK-29753,spark.sql.defaultCatalog,[SQL] refine the default catalog CONFIG###,https://issues.apache.org/jira/browse/SPARK-29753,https://github.com/apache/spark/commit/942753a44beeae5f0142ceefa307e90cbc1234c,name,proactive improvement (Intelligibility),(Null) -> SESSION_CATALOG_NAME,/,"improve the config description, provide a default value to simplify the code.",/
HBASE-16417,hbase.hregion.compacting.pipeline.segments.limit,In-memory MemStore Policy for Flattening and Compactions,https://issues.apache.org/jira/browse/HBASE-16417,https://github.com/apache/hbase/commit/17e7aff37e6b69cae0fb6d15ebeb2037d1ca6acc,segment number,performance exploration,1 -> 4,/,"The submitted patch sets some system properties at the values yielding optimal performance. We investigate several settings of hardware (SSD, HDD), key distribution (Zipf, uniform), with multiple settings of the system, and compare measures like write throughput, read latency, write volume, total gc time, etc. But it should be set by us and we should not give it to the user's decision. We'll tests various workloads, large scale, to find the policy that is most beneficial under common workloads, and is not causing performance degradation in all workloads. ",performance test
HBASE-16417,hbase.memstore.inmemoryflush.threshold.factor,In-memory MemStore Policy for Flattening and Compactions,https://issues.apache.org/jira/browse/HBASE-16417,https://github.com/apache/hbase/commit/17e7aff37e6b69cae0fb6d15ebeb2037d1ca6acc,threshold,performance exploration,0.25 -> 0.02,/,"The submitted patch sets some system properties at the values yielding optimal performance. We investigate several settings of hardware (SSD, HDD), key distribution (Zipf, uniform), with multiple settings of the system, and compare measures like write throughput, read latency, write volume, total gc time, etc. But it should be set by us and we should not give it to the user's decision. We'll tests various workloads, large scale, to find the policy that is most beneficial under common workloads, and is not causing performance degradation in all workloads. ",performance test
HBASE-19282,hbase.memstore.inmemoryflush.threshold.factor,Making CellChunkMap the default index,https://issues.apache.org/jira/browse/HBASE-19282,https://github.com/apache/hbase/commit/8d0da1a77f50b730b366c28b5b477141aa83cc55,threshold,performance exploration,0.02 -> 0.1,/,So in mem flush at 10% of memstore flush size gives better result than 2% right? So why he had gone with 2% as def? Or this is only with CCM and with CAM (which is the def as of now) gives best result with 2%,performance test
HBASE-19919,hbase.cleaner.scan.dir.concurrent.size,Tidying up logging,https://issues.apache.org/jira/browse/HBASE-19919,https://github.com/apache/hbase/commit/06dec205826a6e96e2286180460e5fe014b46fc8,thread num,performance exploration,count of CPUs -> 1/4 of count of CPUs.,/,"There is a bunch of work to be done still cutting down thread counts, connections.. etc. Our defaults are sloppy and in need of another tidying.",no discussion
HBASE-19919,hbase.master.cleaner.interval,Tidying up logging,https://issues.apache.org/jira/browse/HBASE-19919,https://github.com/apache/hbase/commit/06dec205826a6e96e2286180460e5fe014b46fc8,interval,performance exploration,60s -> 600s,/,"There is a bunch of work to be done still cutting down thread counts, connections.. etc. Our defaults are sloppy and in need of another tidying.",no discussion
HBASE-20390,hbase.memstore.inmemoryflush.threshold.factor,IMC Default Parameters for 2.0.0,https://issues.apache.org/jira/browse/HBASE-20390,https://github.com/apache/hbase/commit/1eabbb42954b94308f85434ed2a9a80ba3e03c5e,flush factor,performance exploration,0.1 -> 0.014,/,I tried this. It is a bit better than default on writes but seems worse for mixed load. Following HBASE-20188 we realized in-memory compaction combined with MSLABs may suffer from heap under-utilization due to internal fragmentation.,performance test
HBASE-20390,hbase.hregion.compacting.pipeline.segments.limit,IMC Default Parameters for 2.0.0,https://issues.apache.org/jira/browse/HBASE-20390,https://github.com/apache/hbase/commit/1eabbb42954b94308f85434ed2a9a80ba3e03c5e,segment number,performance exploration,4 -> 2,/,Setting new default parameters for in-memory compaction based on performance tests done in HBASE-20188,performance test
HBASE-20542,hbase.memstore.inmemoryflush.threshold.factor,Better heap utilization for IMC with MSLABs,https://issues.apache.org/jira/browse/HBASE-20542,https://github.com/apache/hbase/commit/d822ee3a7ccc4959ed5a4b85bb54ff6142aa7d6e,flush factor,performance exploration,0.014 -> 0,/,Following HBASE-20188 we realized in-memory compaction combined with MSLABs may suffer from heap under-utilization due to internal fragmentation.,performance test
HBASE-23107,hbase.procedure.store.region.flush.size,Add a stress test tool for region based procedure store,https://issues.apache.org/jira/browse/HBASE-23617,https://github.com/apache/hbase/commit/0ba84d8e95f10e17edd3b18a87b0e130038d54d,flush size,performance exploration,16L * 1024 * 1024 -> 1024 * 1024 * 128L,/,"a larger flush size will help when we schedule a lot of procedures, for example, a full cluster restart.",performance test
HDFS-11998,dfs.use.dfs.network.topology,Enable DFSNetworkTopology as default.,https://issues.apache.org/jira/browse/HDFS-11998,https://github.com/apache/hadoop/commit/9ae9467f920e95ca989d7d51775b39e1b9fee300,feature selection,performance exploration,false -> true,/,"Given the stress testing in HDFS-11923 which shows the correctness of DFSNetworkTopology, and the performance testing in HDFS-11535 which shows how DFSNetworkTopology can outperform NetworkTopology. I think we are at the point where I can and should enable DFSNetworkTopology as default.",/
HDFS-12303,DEFAULT_CELLSIZE,Change default EC cell size to 1MB for better performance,https://issues.apache.org/jira/browse/HDFS-12303,https://github.com/apache/hadoop/commit/f29a0fc288a625522ba910e61b63fd5f10418b3d,cell size,performance exploration,64k -> 1024k,/,The cell size of the provided HDFS erasure coding policies has been changed from 64k to 1024k for better performance. 1MB cell size shows better performance than others during the tests.,performance test
CASSANDRA-13656,start_native_transport,Change default start_native_transport to true and remove from jvm.options,https://issues.apache.org/jira/browse/CASSANDRA-13656,https://github.com/apache/cassandra/commit/12d4e2f189fb228250edc876963d0c74b5ab0d4f,feature selection,No concrete,false -> true,/,When you don't specify the start_native_transport option in the cassandra.yaml config file the default value is set to false. So far I did not find any good reason for setting it this way so I'm proposing to set it to true as default.,/
HDFS-13153,dfs.disk.balancer.enabled,Enable HDFS diskbalancer by default.,https://issues.apache.org/jira/browse/HDFS-13153,https://github.com/apache/hadoop/commit/8d5ea7470a3225319e3bef5626b837572c2e0d3c,feature selection,No concrete,false -> true,/,Enable HDFS diskbalancer by default,/
HDFS-13947,dfs.datanode.directoryscan.throttle.limit.ms.per.sec,Review of DirectoryScanner Class.,https://issues.apache.org/jira/browse/HDFS-13947,https://github.com/apache/hadoop/commit/1dc0adfac0ee4821c67366728c70be9b59477b0f,time,No concrete,1000 -> -1,/,"DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_DEFAULT was previously 1000. 1000 indicated that the class should be unthrottled, which is confusing to say the least. I've simply changed it here so that a value less than 0 (default: -1) indicates unthrottled.",no discussion
SPARK-23146,spark.kubernetes.submitInDriver,Support client mode.,https://issues.apache.org/jira/browse/SPARK-23146,https://github.com/apache/spark/commit/571a6f0574e50e53cea403624ec3795cd03aa204,feature selection,No concrete,(Null)-> false,/,"Client mode works more or less identically to cluster mode. However, in client mode, the Spark Context needs to be manually bootstrapped with certain properties which would have otherwise been set up by spark-submit in cluster mode.",/
SPARK-26304,spark.kafka.sasl.kerberos.service.name,Add default value to spark.kafka.sasl.kerberos.service.name,https://issues.apache.org/jira/browse/SPARK-26304,https://github.com/apache/spark/commit/9b1f6c8bab5401258c653d4e2efb50e97c6d282f,service name,No concrete,(Null) -> kafka,/,It would be easier for spark users by providing less configuration,/
CASSANDRA-14498,excluded_keyspaces,Audit log allows system keyspaces to be audited via configuration options,https://issues.apache.org/jira/browse/CASSANDRA-14498,https://github.com/apache/cassandra/commit/f46762eeca9f5d7e32e731573a8c3e521b70fc05#,mode,new use case,"(Null) -> system, system_schema, system_virtual_schema",/,"It may be a common use case to whitelist queries on these keyspaces, but Cassandra should not make assumptions. Users who don't want these statements in their audit log are still able to whitelist them with configuration.",/
HBASE-18224,hbase.http.max.threads,Upgrade Jetty,https://issues.apache.org/jira/browse/HBASE-18224,https://github.com/apache/hbase/commit/3b444a066c0c699aff749713209950198f1b21e4,thread num,new use case,10(test) -> 16,failure: java.io.IOException: Problem starting http server,"Jetty returns more correct HTTP code when Header is too long, 431 instead of 413, and it requires more threads to start up (made default 16 instead of 10).  The failures are because the 9.4 jetty needs more threads to start; it doesn't like the 10 we stipulate as max in test context. Let me put up a patch.",fix bugs
HBASE-19359,hbase.client.serverside.retries.multiplier,Revisit the default CONFIG of hbase client retries number,https://issues.apache.org/jira/browse/HBASE-19359,https://github.com/apache/hbase/commit/abb535eef661dc9cf0f72f18b96151ae7d5d0179,multipiler,new use case,10 -> 3,,no discussion,no discussion
HBASE-19359,hbase.client.retries.number,Revisit the default CONFIG of hbase client retries number,https://issues.apache.org/jira/browse/HBASE-19359,https://github.com/apache/hbase/commit/abb535eef661dc9cf0f72f18b96151ae7d5d0179,retry num,new use case,35 -> 10,perf: test may hang a very long time.,"As the retries number effect too many unit tests. So I open this issue to see the Hadoop QA result.The default value of hbase.client.retries.number is 35. Plan to reduce this to 10. And for server side, the default hbase.client.serverside.retries.multiplier is 10. So the server side retries number is 35 * 10 = 350. It is too big! Plan to reduce hbase.client.serverside.retries.multiplier to 3.",fix bugs
HBASE-19483,hbase.security.authorization,Add proper privilege check for rsgroup commands,https://issues.apache.org/jira/browse/HBASE-19483,https://github.com/apache/hbase/commit/7ddf79946da4bc8f0dfa673b48f29023e5ff5bcf,feature selection,new use case,true -> false,unexpected result: AccessController is not added to the server,"it may happen that the value of hbase.security.authorization is true, but AccessController is not added to the server.",/
HBASE-19660,hbase.hstore.blockingStoreFiles,Up default retries from 10 to 15 and blocking store files limit from 10 to 16,https://issues.apache.org/jira/browse/HBASE-19660,https://github.com/apache/hbase/commit/b3a4fca48ffcac44ec1e1d53a99caa18b7e14cbf,file num,new use case,10 -> 16,failure: stop all writes,When we hit the blocking store file limit we stop all writes for 90 seconds. Can happen a few times back-to-back. Kills the running ITBLL.,no discussion
HDFS-12290,dfs.cblock.jscsi.server.address,Block Storage: Change dfs.cblock.jscsi.server.address default bind address to 0.0.0.0.,https://issues.apache.org/jira/browse/HDFS-12290,https://github.com/apache/hadoop/commit/f9bce29dccf6c8f9d3155fdff42981750c2d20da,ip address,new use case,127.0.0.1 -> 0.0.0.0,/,"dfs.cblock.jscsi.server.address currently default to 127.0.0.1 which only allows localhost to mount cblock drives, for external connections this config needs to be changed to 0.0.0.0 to allow external connections.",/
HDFS-12990,DFS_NAMENODE_RPC_PORT_DEFAULT,Change default NameNode RPC port back to 8020,https://issues.apache.org/jira/browse/HDFS-12990,https://github.com/apache/hadoop/commit/4304fcd5bdf9fb7aa9181e866eea383f89bf171f,port,new use case,9820 -> 8020,/,"With more integration going on, it appears that all the other ephemeral port changes are fine, but the NN RPC port change is painful for downstream on migrating to Hadoop 3. for compatible(downstream) reason",/
HDFS-13099,dfs.federation.router.store.driver.class,RBF: Use the ZooKeeper as the default State Store.,https://issues.apache.org/jira/browse/HDFS-13099,https://github.com/apache/hadoop/commit/543f3abbee79d7ec70353f0cdda6397ee001324e,Class implementation,new use case,test implmentation -> production implementation,failure: doesn't handle multiple writers concurrently.,Change default State Store from local file to ZooKeeper. This will require additional zk address to be configured. This implementation doesn't handle multiple writers concurrently.,/
HDFS-14317,dfs.namenode.edit.log.autoroll.multiplier.threshold,Standby does not trigger edit log rolling when in-progress edit log tailing is enabled,https://issues.apache.org/jira/browse/HDFS-14317,https://github.com/apache/hadoop/commit/1bc282e0b3f74968c92751f0972746b012e72810,threshold,new use case,2.0f -> 0.2f -> 0.5f,unexpected result: irrecoverably losing a few minutes worth of metadata,"It seems that NNStorageRetentionManager should never delete an in-progress edit log, since this implies it is still in use. Also, probably you need to turn down your dfs.namenode.edit.log.autoroll.multiplier.threshold to avoid the possibility of getting into that situation?",fix bugs
SPARK-25088,spark.master.rest.enabled,Update Rest Server docs & defaults.,https://issues.apache.org/jira/browse/SPARK-25088,https://github.com/apache/spark/commit/10248758438b9ff57f5669a324a716c8c6c8f17b,feature selection,new use case,true -> false,unexpected result:left a backdoor wide open,"if you're allowing unauthed rest, what is the point of auth on standard submission? For most users, they'd just think they had a secure setup with auth on standard submission, and not realize they'd left a backdoor wide open. Its not worth that security risk",/
SPARK-25641,spark.shuffle.server.chunkFetchHandlerThreadsPercent,Change the spark.shuffle.server.chunkFetchHandlerThreadsPercent default to 100,https://issues.apache.org/jira/browse/SPARK-25641,https://github.com/apache/spark/commit/6353425af76f9cc9de7ee4094f41df7a7390d898,percentage,new use case,0% -> 100%,/,"currently this is set to 0. Which means currently if server.ioThreads > 0, the default number of threads would be 2 * #cores instead of server.io.Threads. We want the default to server.io.Threads in case this is not set at all. Also here a default of 0 would also mean 2 * #cores",no discussion
SPARK-27528,spark.sql.parquet.outputTimestampType,Use Parquet logical type TIMESTAMP_MICROS by default,https://issues.apache.org/jira/browse/SPARK-27528,https://github.com/apache/spark/commit/43a73e387cb843486adcf5b8bbd8b99010ce6e02,timestamp type,new use case,INT96 -> TIMESTAMP_MICROS,/,"In the PR, I propose to use the TIMESTAMP_MICROS logical type for timestamps written to parquet files. The type matches semantically to Catalyst's TimestampType, and stores microseconds since epoch in UTC time zone. This will allow to avoid conversions of microseconds to nanoseconds and to Julian calendar. Also this will reduce sizes of written parquet files.",/
HDFS-14653,dfs.federation.router.namenode.heartbeat.enable,RBF: Correct the default value for dfs.federation.router.namenode.heartbeat.enable,https://issues.apache.org/jira/browse/HDFS-14653,https://github.com/apache/hadoop/commit/9792f58662a256f79d2c8352e11d7368ac49698,feature selection,incorrect,false -> true,/,Correct the default value for dfs.federation.router.namenode.heartbeat.enable,/
HDFS-14821,dfs.image.parallel.load,Make HDFS-14617 (fsimage sub-sections) off by default.,https://issues.apache.org/jira/browse/HDFS-14821,https://github.com/apache/hadoop/commit/06ad6540b3cdb8fb957f900aaebc9d1d289faba,feature selection,incorrect,true -> false,/,"Even though the steps to downgrade are simple, we cannot expect people to be aware of this, and hence it is safest to disable the feature by default.",/
HBASE-18662,hbase.client.keyvalue.maxsize,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,-1 -> 10485760,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,zookeeper.session.timeout,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,180000 -> 90000,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.client.retries.number,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,31 -> 35,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.snapshot.restore.take.failsafe.snapshot,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,false -> true,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.lease.recovery.dfs.timeout,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,61000 -> 64000,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.regionserver.logroll.errors.tolerated,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,0 -> 2,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.hstore.blockingStoreFiles,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,7 -> 10,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.ipc.server.callqueue.handler.factor,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,0 -> 0.1,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.normalizer.period,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,1800000 -> 300000,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.client.max.perserver.tasks,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,5 -> 2,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.client.max.total.tasks,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,5 -> 2,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
HBASE-18662,hbase.security.visibility.mutations.checkauths,The default values for many configuration items in the code are not consistent with hbase-default.xml,https://issues.apache.org/jira/browse/HBASE-18662,https://github.com/apache/hbase/commit/90c15bae8d5e2acb4d58977bdfbb3425f80466a2,/,inconsistent,10 ->16,/,The default values for many configuration items in the code are not consistent with hbase-default.xml,/
SPARK-24926,spark.rpc.io.threads,Ensure numCores is used consistently in all netty configurations,https://issues.apache.org/jira/browse/SPARK-24926,https://github.com/apache/spark/commit/70462f291bf046c648f36063b0161861e6d11898,/,inconsistent,0 -> numUsableCores,/,Ensure numCores is used consistently in all netty configurations,/
HDFS-11957,dfs.namenode.posix.acl.inheritance.enabled,Enable POSIX ACL inheritance by default.,https://issues.apache.org/jira/browse/HDFS-11957,https://github.com/apache/hadoop/commit/312e57b95477ec95e6735f5721c646ad1df019f8,feature selection,enable/disable feature,false -> true,/,"This change is considered backward-incompatible, so the new behavior is off by default and must be explicitly configured by setting dfs.namenode.posix.acl.inheritance.enabled to true in hdfs-site.xml. It is time to enable POSIX ACL inheritance by default.",/
HDFS-12214,dfs.storage.policy.satisfier.enabled,[SPS]: Fix review comments of StoragePolicySatisfier feature.,https://issues.apache.org/jira/browse/HDFS-12214,https://github.com/apache/hadoop/commit/0e820f16af309cc8476edba448dd548686431133,feature selection,enable/disable feature,true -> false,/,"Yea, I think that'd make sense for additional safety, we often ship new features disabled by default.",/
HDFS-12376,dfs.journalnode.enable.sync,Enable JournalNode Sync by default.,https://issues.apache.org/jira/browse/HDFS-12376,https://github.com/apache/hadoop/commit/5ff74167ddd378a738898425c4462f3432f9c31d,feature selection,enable/disable feature,true -> false -> true,/,"In the meanwhile, the current patch may still hit an issue while HA upgrade is going on. If the segment downloading is happening while the admin tries to rollback, the deletion of the current directory may fail on Windows. As a fix we can disable the sync while there is prev directory on JN (which means the upgrade is still going on). Or we can download the segment first into another directory. Currently I'm thinking maybe we can disable this feature in the configuration by default, then use separate jiras to track remaining issues. This also allows us to do more testing. Thoughts? I have tested this feature extensively on a long running test for more than 10 days. Ran a script which keeps generating load, deleting edit logs from journals and verifying that the deleted edit logs are synced back.at the beginning is true, however, there remains some bugs so change it to false. After the bug is fixed, change it back to true. “I have tested this feature extensively on a long running test for more than 10 days. Ran a script which keeps generating load, deleting edit logs from journals and verifying that the deleted edit logs are synced back.”",/
HDFS-12603,dfs.namenode.edits.asynclogging,Enable async edit logging by default,https://issues.apache.org/jira/browse/HDFS-12603,https://github.com/apache/hadoop/commit/afb42aeabf1317b755ab79e0265bc90920c896ac,feature selection,enable/disable feature,false -> true,/,we think it's safe to turn this on by default for better out-of-the-box performance. It was off by default due to concerns about correctness. We have been running it in production for quite a while with no issues so far,/
HDFS-13505,dfs.namenode.acls.enabled,Turn on HDFS ACLs by default.,https://issues.apache.org/jira/browse/HDFS-13505,https://github.com/apache/hadoop/commit/e9b6b81de44ff5fb9f833cfc32c69b644eb46ba,feature selection,enable/disable feature,false -> true,/,ACLs is basic functionality required by most of the use cases and clients. Currently turning it on involves additional step of reconfiguring cluster and restarting service. We can avoid this by enabling it by default. This feature is in use for quite some time now and it is safe to make it default.,/
SPARK-21568,spark.ui.showConsoleProgress,ConsoleProgressBar should only be enabled in shells,https://issues.apache.org/jira/browse/SPARK-21568,https://github.com/apache/spark/commit/f31e11404d6d5ee28b574c242ecbee94f35e9370#,feature selection,enable/disable feature,True -> False,/,[CORE] ConsoleProgressBar should only be enabled in shells,/
SPARK-21783,spark.sql.orc.filterPushdown,[SQL] Turn on ORC filter push-down by default,https://issues.apache.org/jira/browse/SPARK-21783,https://github.com/apache/spark/commit/0f8a28617a0742d5a99debfbae91222c2e3b5cec,feature selection,enable/disable feature,false -> true,/,"We turned off Parquet filter push-down by default in Spark 1.4.0 and prior versions because of some Parquet side bugs in Parquet 1.6.0rc3. Now we've upgraded to 1.7.0, which fixed all those bugs. Should turn on Parquet filter push-down by default now.",/
SPARK-26700,spark.maxRemoteBlockSizeFetchToMem,enable fetch-big-block-to-disk by default,https://issues.apache.org/jira/browse/SPARK-26700,https://github.com/apache/spark/commit/ed71a825c56920327533ebb741707871848ccd6d,threshold,enable/disable feature,Int.MaxValue - 512 -> 200m,/,"The fetch-big-block-to-disk feature is disabled by default, because it's not compatible with external shuffle service prior to Spark 2.2. The client sends stream request to fetch block chunks, and old shuffle service can't support it.After 2 years, Spark 2.2 has EOL, and now it's safe to turn on this feature by default",no discussion
SPARK-27119,spark.sql.hive.caseSensitiveInferenceMode,Do not infer schema when reading Hive serde table with native data source,https://issues.apache.org/jira/browse/SPARK-27119,https://github.com/apache/spark/commit/31878c9daafb2c05e380b83cb4a04b32d5ff648f,mode,enable/disable feature,INFER_AND_SAVE -> NEVER_INFER,unexpected result: query returns 0 results,"To be safe, this PR just changes the default value to NEVER_INFER, so that users can set it back to INFER_AND_SAVE. If we don't receive any bug reports for it, we can remove the related code in the next release.",/
SPARK-27677,spark.shuffle.service.fetch.rdd.enabled,Disable by default fetching of disk persisted RDD blocks via external shuffle service,https://issues.apache.org/jira/browse/SPARK-27677,https://github.com/apache/spark/commit/1e87694f2bc0dea3eaaef936dc8552b3ba421539,feature selection,enable/disable feature,true -> false,"unexpected result: dynamic allocation to reclaim executors with only disk-cached blocks more rapidly, but still keep the cached data available.",In the PR the config `spark.shuffle.service.fetch.rdd.enabled` default is changed to false to avoid breaking any compatibility with older external shuffle service installations. As external shuffle service is deployed separately and disk persisted RDD block fetching had even introduced new network messages (`RemoveBlocks` and `BlocksRemoved`) and changed the behaviour of the already existing fetching: extended it for RDD blocks.,/
HDFS-12106,dfs.storage.policy.satisfier.low.max-streams.preference,Improve storage policy satisfier CONFIGurations.,https://issues.apache.org/jira/browse/HDFS-12106,https://github.com/apache/hadoop/commit/c561cb316e365ef674784cd6cf0b12c0fbc271a3,/,code error,false -> true,/,"Change dfs.storage.policy.satisfier.low.max-streams.preference's value to true and modify the default value to true as well. If user wants equal share then it should be false, but presently it is true which is not correct. Thanks Uma Maheswara Rao G for pointing out this case. code mistake, should be true(ideally), but false",/
HDFS-14676,CommonConfigurationKeys.FS_DF_INTERVAL_KEY,Wrong default value for fs.df.interval.,https://issues.apache.org/jira/browse/HDFS-14676,https://github.com/apache/hadoop/commit/72eba25e09e69d35c541603844dacf70a8499327,/,code error,3 * 1000 -> 60000,/,Wrong default value for fs.df.interval.,/
SPARK-27868,SPARK_NETWORK_IO_BACKLOG_KEY,[CORE] Better default value and documentation for socket server backlog.,https://issues.apache.org/jira/browse/SPARK-27868,https://github.com/apache/spark/commit/09ed64d795d3199a94e175273fff6fcea6b52131,/,code error,-1 -> 64,/,"The default value actual causes the default value from the JRE to be used, which is 50 according to the docs",/