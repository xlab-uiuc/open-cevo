Issue-id,Title,Parameter,Issue-URL,Commit-URL,Inconsistency
CASSANDRA-13664,Only optimize large ranges when figuring out where to stream from,DatabaseDescriptor.getPartitioner,https://issues.apache.org/jira/browse/CASSANDRA-13664,https://github.com/apache/cassandra/commit/ff06424faccc8acedd027c71e955a38fd8ddee6c,/
CASSANDRA-13740,Delay hints store excise by write timeout to avoid race with decommission,DatabaseDescriptor.getMinRpcTimeout(),https://issues.apache.org/jira/browse/CASSANDRA-13740,https://github.com/apache/cassandra/commit/b2f6ce961f38a3e4cd744e102026bf7a471056c9,/
CASSANDRA-13740,Delay hints store excise by write timeout to avoid race with decommission,DatabaseDescriptor.getWriteRpcTimeout(),https://issues.apache.org/jira/browse/CASSANDRA-13740,https://github.com/apache/cassandra/commit/b2f6ce961f38a3e4cd744e102026bf7a471056c9,/
CASSANDRA-14153,Delete temp test files on exit,java.io.tmpdir,https://issues.apache.org/jira/browse/CASSANDRA-14153,https://github.com/apache/cassandra/commit/8587b0ceb47fa54308dfa9b0bfdc320e6afdc311,/
CASSANDRA-14225,Fix comparison of address and port for repair and messages,DatabaseDescriptor.getStoragePort(),https://issues.apache.org/jira/browse/CASSANDRA-14225,https://github.com/apache/cassandra/commit/834f2a6ecdb8974839762bf4e9c5fed32163f9c8,/
CASSANDRA-14435,Add JMX query support for diagnostic events,DatabaseDescriptor.diagnosticEventsEnabled(),https://issues.apache.org/jira/browse/CASSANDRA-14435,https://github.com/apache/cassandra/commit/a79e5903b552e40f77c151e23172f054ffb7f39e,/
CASSANDRA-14619,Add fqltool compare,DatabaseDescriptor.clientInitialization(),https://issues.apache.org/jira/browse/CASSANDRA-14619,https://github.com/apache/cassandra/commit/f83bd5ac2bbc6755213a6ad0675e7e5400c79670,/
CASSANDRA-14938,Add specialized IndexRegistry for offline tools/clients,DatabaseDescriptor.isDaemonInitialized(),https://issues.apache.org/jira/browse/CASSANDRA-14938,https://github.com/apache/cassandra/commit/3ddfbc8f5871c78bde26e96a936e96deeccb4366,/
HBASE-18347,Implement a BufferedMutator for async client,hbase.client.write.buffer,https://issues.apache.org/jira/browse/HBASE-18347,https://github.com/apache/hbase/commit/d12eb7a4aae5c2dc7b230bf2a12d2313b93b8ba9,/
HBASE-19747,Introduce a special WALProvider for synchronous replication,hbase.hlog.open.timeout,https://issues.apache.org/jira/browse/HBASE-19747,https://github.com/apache/hbase/commit/274b813e1221c366e2c4773910d202b65298768f,/
HBASE-19747,Introduce a special WALProvider for synchronous replication,hbase.regionserver.hlog.reader.impl,https://issues.apache.org/jira/browse/HBASE-19747,https://github.com/apache/hbase/commit/274b813e1221c366e2c4773910d202b65298768f,/
HBASE-20466,Consistently use override mechanism for exempt classes in CoprocessorClassloader,CP_HTD_ATTR_INCLUSION_KEY,https://issues.apache.org/jira/browse/HBASE-20466,https://github.com/apache/hbase/commit/a8be3bb814378805a1b5eae85826c9eff3768534,/
HBASE-21247,Custom WAL Provider cannot be specified by CONFIGuration whose value is outside the enums in Providers,hbase.wal.provider,https://issues.apache.org/jira/browse/HBASE-21247,https://github.com/apache/hbase/commit/7395ffac447bdb159b5c9a2bc65f95af0eea11fe,/
HBASE-21809,Add retry thrift client for ThriftTable/Admin,HBASE_CLIENT_RETRIES_NUMBER,https://issues.apache.org/jira/browse/HBASE-21809,https://github.com/apache/hbase/commit/2776bc0151051c8d20d9b1c2ac6142ade6a31b62,/
HBASE-22261,Make use of ClusterStatusListener for async client,STATUS_PUBLISHED,https://issues.apache.org/jira/browse/HBASE-22261,https://github.com/apache/hbase/commit/eb912bb1dab793248f47d6ed8ee6f8abb467d10b,/
HDFS-11576,Block recovery will fail indefinitely if recovery time >heartbeat interval,DFS_HEARTBEAT_INTERVAL_KEY,https://issues.apache.org/jira/browse/HDFS-11576,https://github.com/apache/hadoop/commit/5304698dc8c5667c33e6ed9c4a827ef57172a723,/
HDFS-12158,Secondary Namenode's web interface lack configs for X-FRAME-OPTIONS protection.,DFS_XFRAME_OPTION_ENABLED,https://issues.apache.org/jira/browse/HDFS-12158,https://github.com/apache/hadoop/commit/413b23eb04eee24275257ab462133e0818f87449,/
HDFS-12158,Secondary Namenode's web interface lack configs for X-FRAME-OPTIONS protection.,DFS_XFRAME_OPTION_VALUE,https://issues.apache.org/jira/browse/HDFS-12158,https://github.com/apache/hadoop/commit/413b23eb04eee24275257ab462133e0818f87449,/
HDFS-12976,Introduce ObserverReadProxyProvider.,HdfsClientConfigKeys.Failover.RANDOM_ORDER,https://issues.apache.org/jira/browse/HDFS-12976,https://github.com/apache/hadoop/commit/64b7cf59bde66bc58f67d2c3a97324ef679fb60a,/
HDFS-13328,Abstract ReencryptionHandler recursive logic in separate class.,DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY,https://issues.apache.org/jira/browse/HDFS-13328,https://github.com/apache/hadoop/commit/f89594f0b80e8efffdcb887daa4a18a2b0a228b3,/
HDFS-13608,Add ability for JournalNode to serve edits via RPC,DFS_HA_TAILEDITS_INPROGRESS_KEY,https://issues.apache.org/jira/browse/HDFS-13608,https://github.com/apache/hadoop/commit/151c8ddbe4c05fcb5f251fa4450edc452f6c735a,/
HDFS-13782,ObserverReadProxyProvider should work with IPFailoverProxyProvider.,DFS_NAMENODE_RPC_ADDRESS_KEY,https://issues.apache.org/jira/browse/HDFS-13782,https://github.com/apache/hadoop/commit/f9fc01cd7fef2fab1a6f696653b5de1d821b4d2a,/
HDFS-13856,RBF: RouterAdmin should support dfsrouteradmin -refreshRouterArgs command,DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY,https://issues.apache.org/jira/browse/HDFS-13856,https://github.com/apache/hadoop/commit/f4e2bfce585d762eaf26096613d135203f080eb3,/
HDFS-14001,bootstrapStandby should manage the InMemoryAliasMap.,DFS_NAMENODE_PROVIDED_ENABLED,https://issues.apache.org/jira/browse/HDFS-14001,https://github.com/apache/hadoop/commit/8fc0d04517912766a3102f3e611f7d0fabd2f815,/
HDFS-14001,bootstrapStandby should manage the InMemoryAliasMap.,DFS_PROVIDED_ALIASMAP_INMEMORY_ENABLED,https://issues.apache.org/jira/browse/HDFS-14001,https://github.com/apache/hadoop/commit/8fc0d04517912766a3102f3e611f7d0fabd2f815,/
HDFS-14052,RBF: Use Router keytab for WebHDFS. Contributed by CR Hota.,DFS_ROUTER_KEYTAB_FILE_KEY,https://issues.apache.org/jira/browse/HDFS-14052,https://github.com/apache/hadoop/commit/c4b1fa91faf5ada6eea6b39d52e92c06816bd2c8,/
HDFS-14388,RBF: Prevent loading metric system when disabled.,DFS_ROUTER_METRICS_ENABLE,https://issues.apache.org/jira/browse/HDFS-14388,https://github.com/apache/hadoop/commit/0dbd87874a16403f537ef31f45ab0fe05924af6f,/
HDFS-14460,DFSUtil#getNamenodeWebAddr should return HTTPS address based on policy configured,DFS_NAMENODE_HTTPS_ADDRESS_KEY,https://issues.apache.org/jira/browse/HDFS-14460,https://github.com/apache/hadoop/commit/865c3289308327788f3bed355864c510deb40956,/
HDFS-14560,Allow block replication parameters to be refreshable.,DFS_NAMENODE_REPLICATION_STREAMS_HARD_LIMIT_KEY,https://issues.apache.org/jira/browse/HDFS-14560,https://github.com/apache/hadoop/commit/4f455290b15902e7e44c4b1a762bf915414b2bb6,/
HDFS-14560,Allow block replication parameters to be refreshable.,DFS_NAMENODE_REPLICATION_WORK_MULTIPLIER_PER_ITERATION,https://issues.apache.org/jira/browse/HDFS-14560,https://github.com/apache/hadoop/commit/4f455290b15902e7e44c4b1a762bf915414b2bb6,/
SPARK-13704,Reduce rack resolution time,NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,https://issues.apache.org/jira/browse/SPARK-13704,https://github.com/apache/spark/commit/52838e74afdd58a7c09707284b4e0232dc01ef26,/
SPARK-24307,[CORE] Support reading remote cached partitions > 2gb,spark.storage.memoryMapLimitForTests,https://issues.apache.org/jira/browse/SPARK-24307,https://github.com/apache/spark/commit/7e847646d1f377f46dc3154dea37148d4e557a03,/
SPARK-24737,[SQL] Type coercion between StructTypes,SQLConf.get.resolver,https://issues.apache.org/jira/browse/SPARK-24737,https://github.com/apache/spark/commit/01fcba2c685be0603a404392685e9d52fb4cb82a,/
SPARK-24743,EXAMPLES] Update the JavaDirectKafkaWordCount example to support the new API of kafka,BOOTSTRAP_SERVERS_CONFIG,https://issues.apache.org/jira/browse/SPARK-24743,https://github.com/apache/spark/commit/ac78bcce00ff8ec8e5b7335c2807aa0cd0f5406a,/
SPARK-24743,EXAMPLES] Update the JavaDirectKafkaWordCount example to support the new API of kafka,GROUP_ID_CONFIG,https://issues.apache.org/jira/browse/SPARK-24743,https://github.com/apache/spark/commit/ac78bcce00ff8ec8e5b7335c2807aa0cd0f5406a,/
SPARK-24743,EXAMPLES] Update the JavaDirectKafkaWordCount example to support the new API of kafka,KEY_DESERIALIZER_CLASS_CONFIG,https://issues.apache.org/jira/browse/SPARK-24743,https://github.com/apache/spark/commit/ac78bcce00ff8ec8e5b7335c2807aa0cd0f5406a,/
SPARK-24743,EXAMPLES] Update the JavaDirectKafkaWordCount example to support the new API of kafka,VALUE_DESERIALIZER_CLASS_CONFIG,https://issues.apache.org/jira/browse/SPARK-24743,https://github.com/apache/spark/commit/ac78bcce00ff8ec8e5b7335c2807aa0cd0f5406a,/
SPARK-24794,DriverWrapper should have both master addresses in -Dspark.master,spark.master.rest.port,https://issues.apache.org/jira/browse/SPARK-24794,https://github.com/apache/spark/commit/002f9c169eb20b0d71b6d0296595f343c7f5bab2,/
SPARK-24794,DriverWrapper should have both master addresses in -Dspark.master,spark.master,https://issues.apache.org/jira/browse/SPARK-24794,https://github.com/apache/spark/commit/002f9c169eb20b0d71b6d0296595f343c7f5bab2,/
SPARK-24960,[K8S] explicitly expose ports on driver container,DRIVER_BLOCK_MANAGER_PORT,https://issues.apache.org/jira/browse/SPARK-24960,https://github.com/apache/spark/commit/f5113ea8d79de724ec1579bc81a7abb61e44eeef,/
SPARK-24960,[K8S] explicitly expose ports on driver container,spark.driver.port,https://issues.apache.org/jira/browse/SPARK-24960,https://github.com/apache/spark/commit/f5113ea8d79de724ec1579bc81a7abb61e44eeef,/
SPARK-25207,Case-insensitve field resolution for filter pushdown when reading Parquet,sqlConf.caseSensitiveAnalysis,https://issues.apache.org/jira/browse/SPARK-25207,https://github.com/apache/spark/commit/8d9495a8f1e64dbc42c3741f9bcbd4893ce3f0e9,/
SPARK-25407,Allow nested access for non-existent field for Parquet file when nested pruning is enabled,SQLConf.NESTED_SCHEMA_PRUNING_ENABLED.key,https://issues.apache.org/jira/browse/SPARK-25407,https://github.com/apache/spark/commit/215609def22da14c464b37374ceae4f53a39a145#diff-ee26d4c4be21e92e92a02e9f16dbc285R313,/
SPARK-25950,from_csv should respect to spark.sql.columnNameOfCorruptRecord,SQLConf.COLUMN_NAME_OF_CORRUPT_RECORD,https://issues.apache.org/jira/browse/SPARK-25950,https://github.com/apache/spark/commit/76813cfa1e2607ea3b669a79e59b568e96395b2e,/
CASSANDRA-14726,ReplicaCollection follow-up,DatabaseDescriptor.getEndpointSnitch(),https://issues.apache.org/jira/browse/CASSANDRA-14726,https://github.com/apache/cassandra/commit/e645b9172c5d50fc2af407de724e46121edfe109,/
HBASE-17165,Make use of retry setting in LoadIncrementalHFiles & fix test,HBASE_CLIENT_RETRIES_NUMBER,https://issues.apache.org/jira/browse/HBASE-17165,https://github.com/apache/hbase/commit/b16e03c130bacf71a22b1c0e61c502e13831e94b,/
HBASE-18375,"Fix the bug where the pool chunks from ChunkCreator are deallocated and not returned to pool, because there is no reference to them",COMPACTING_MEMSTORE_INDEX_KEY,https://issues.apache.org/jira/browse/HBASE-18375,https://github.com/apache/hbase/commit/75a6b36849c58d6a751f57226ab0c8f7884a9e87,/
HBASE-19568,Restore of HBase table using incremental backup doesn't restore rows from an earlier incremental backup,BACKUP_SYSTEM_TABLE_NAME_KEY,https://issues.apache.org/jira/browse/HBASE-19568,https://github.com/apache/hbase/commit/a5601c8eac6bfcac7d869574547f505d44e49065,/
HBASE-19568,Restore of HBase table using incremental backup doesn't restore rows from an earlier incremental backup,BACKUP_SYSTEM_TTL_KEY,https://issues.apache.org/jira/browse/HBASE-19568,https://github.com/apache/hbase/commit/a5601c8eac6bfcac7d869574547f505d44e49065,/
HBASE-19841,LocalHTU to not enforce stream capabilities,CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE,https://issues.apache.org/jira/browse/HBASE-19841,https://github.com/apache/hbase/commit/9ea152d2351cfb2607a5a861358470969ce24548,/
HBASE-19923,Reset peer state and CONFIG when refresh replication source failed,peer.setPeerConfig(oldConfig),https://issues.apache.org/jira/browse/HBASE-19923,https://github.com/apache/hbase/commit/2b63af376edbca53eb0475e929478bd38b580aae,/
HBASE-20530,Composition of backup directory containing namespace when restoring is different from the actual hfile location,writeMultipleTables,https://issues.apache.org/jira/browse/HBASE-20530,https://github.com/apache/hbase/commit/acbc3a225338fd1ff82226ebbd937f7b15ef5b60,/
HBASE-20590,REST Java client is not able to negotiate with the server in the secure mode,REST_SSL_ENABLED,https://issues.apache.org/jira/browse/HBASE-20590,https://github.com/apache/hbase/commit/7da0015a3b58a28ccbae0b03ba7de9ce62b751e1,/
HBASE-20833,Modify pre-upgrade coprocessor validator to support table level coprocessors,MASTER_COPROCESSOR_CONF_KEY,https://issues.apache.org/jira/browse/HBASE-20833,https://github.com/apache/hbase/commit/ad5b4af2c4ae822fc892974fd7e2e9e2106998a6,/
HBASE-20833,Modify pre-upgrade coprocessor validator to support table level coprocessors,REGION_COPROCESSOR_CONF_KEY,https://issues.apache.org/jira/browse/HBASE-20833,https://github.com/apache/hbase/commit/ad5b4af2c4ae822fc892974fd7e2e9e2106998a6,/
HBASE-21570,Add write buffer periodic flush support for AsyncBufferedMutator,WRITE_BUFFER_PERIODIC_FLUSH_TIMEOUT_MS,https://issues.apache.org/jira/browse/HBASE-21570,https://github.com/apache/hbase/commit/b09b87d143730db00ec56114a752d3a74f8982c4,/
HBASE-22244,Make use of MetricsConnection in async client,CLIENT_SIDE_METRICS_ENABLED_KEY,https://issues.apache.org/jira/browse/HBASE-22244,https://github.com/apache/hbase/commit/a3d2a2df3a0837f39d586f5f2018fd630883fa10,/
HDFS-10880,Federation Mount Table State Store internal API.,DFS_ROUTER_DEFAULT_NAMESERVICE,https://issues.apache.org/jira/browse/HDFS-10880,https://github.com/apache/hadoop/commit/6f0de2731806628b5b01bd1350225692147590da,/
HDFS-11640,[READ] Datanodes should use a unique identifier when reading from external stores,IO_FILE_BUFFER_SIZE_KEY,https://issues.apache.org/jira/browse/HDFS-11640,https://github.com/apache/hadoop/commit/4531588a94dcd2b4141b12828cb60ca3b953a58c,/
HDFS-12877,Add open(PathHandle) with default buffersize,IO_FILE_BUFFER_SIZE_KEY,https://issues.apache.org/jira/browse/HDFS-12877,https://github.com/apache/hadoop/commit/0780fdb1ebdddd19744fbbca7fb05f8fe4bf4d28,/
HDFS-12982,Reduce the locking and cleanup the Namesystem access.,DFS_STORAGE_POLICY_SATISFIER_SELF_RETRY_TIMEOUT_MILLIS_KEY,https://issues.apache.org/jira/browse/HDFS-12982,https://github.com/apache/hadoop/commit/05d4daf6ba3e5bd40f46e8003ee12fc7c613453d,/
HDFS-12982,Reduce the locking and cleanup the Namesystem access.,DFS_STORAGE_POLICY_SATISFIER_RECHECK_TIMEOUT_MILLIS_KEY,https://issues.apache.org/jira/browse/HDFS-12982,https://github.com/apache/hadoop/commit/05d4daf6ba3e5bd40f46e8003ee12fc7c613453d,/
HDFS-13025,[SPS]: Implement a mechanism to scan the files for extern……al SPS.,DFS_STORAGE_POLICY_SATISFIER_QUEUE_LIMIT_KEY,https://issues.apache.org/jira/browse/HDFS-13025,https://github.com/apache/hadoop/commit/3159b39cf8ef704835325263154fb1a1cecc109d,/
HDFS-13033,[SPS]: Implement a mechanism to do file block movements for external SPS. ,DFS_MOVER_MOVERTHREADS_KEY,https://issues.apache.org/jira/browse/HDFS-13033,https://github.com/apache/hadoop/commit/b0cb8d9bb44c963ae686d2b5c1b70bc76b955e10,/
HDFS-13062,Provide support for JN to use separate journal disk per namespace.,DFS_INTERNAL_NAMESERVICES_KEY,https://issues.apache.org/jira/browse/HDFS-13062,https://github.com/apache/hadoop/commit/dd50f53997239bf9078481cf46592ca3e41520b5,/
HDFS-13165,[SPS]: Collects successfully moved block details via IBR,DFS_STORAGE_POLICY_SATISFIER_MODE_KEY,https://issues.apache.org/jira/browse/HDFS-13165,https://github.com/apache/hadoop/commit/2acc50b826fa8b00f2b09d9546c4b3215b89d46d,/
HDFS-12778,[READ] Report multiple locations for PROVIDED blocks,DFS_REPLICATION_KEY,https://issues.apache.org/jira/browse/HDFS-12778,https://github.com/apache/hadoop/commit/3d3be87e301d9f8ab1a220bc5dbeae0f032c5a86,/
SPARK-16944,[Mesos] Improve data locality when launching new executors when dynamic allocation is enabled,spark.dynamicAllocation.enabled,https://issues.apache.org/jira/browse/SPARK-16944,https://github.com/apache/spark/commit/4329eb2e73181819bb712f57ca9c7feac0d640ea,/
SPARK-20396,[SQL][PYSPARK] groupby().apply() with pandas udf,spark.buffer.size,https://issues.apache.org/jira/browse/SPARK-20396,https://github.com/apache/spark/commit/bfc7e1fe1ad5f9777126f2941e29bbe51ea5da7c,/
SPARK-20396,[SQL][PYSPARK] groupby().apply() with pandas udf,spark.python.worker.reuse,https://issues.apache.org/jira/browse/SPARK-20396,https://github.com/apache/spark/commit/bfc7e1fe1ad5f9777126f2941e29bbe51ea5da7c,/
SPARK-21417,[SQL] Infer join conditions using propagated constraints,SQLConf.get.constraintPropagationEnabled,https://issues.apache.org/jira/browse/SPARK-21417,https://github.com/apache/spark/commit/6ac57fd0d1c82b834eb4bf0dd57596b92a99d6de,/
SPARK-21701,Enable RPC client to use ` SO_RCVBUF` and ` SO_SN……DBUF` in SparkConf.,SO_SNDBUF,https://issues.apache.org/jira/browse/SPARK-21701,https://github.com/apache/spark/commit/763b83ee84cbb6f263218c471dd9198dd6bee411,/
SPARK-21701,Enable RPC client to use ` SO_RCVBUF` and ` SO_SN……DBUF` in SparkConf.,SO_RCVBUF,https://issues.apache.org/jira/browse/SPARK-21701,https://github.com/apache/spark/commit/763b83ee84cbb6f263218c471dd9198dd6bee411,/
SPARK-21728,"[CORE] Follow up: fix user CONFIG, auth in SparkSubmit logging",SecurityManager.SPARK_AUTH_SECRET_CONF,https://issues.apache.org/jira/browse/SPARK-21728,https://github.com/apache/spark/commit/0bdbefe9dd1e7c95c58ea6b52d3b264794abbc0e,/
SPARK-21842,[MESOS] Support Kerberos ticket renewal and creation in Mesos,config.KEYTAB,https://issues.apache.org/jira/browse/SPARK-21842,https://github.com/apache/spark/commit/1e82335413bc2384073ead0d6d581c862036d0f5,/
SPARK-21842,[MESOS] Support Kerberos ticket renewal and creation in Mesos,config.PRINCIPAL,https://issues.apache.org/jira/browse/SPARK-21842,https://github.com/apache/spark/commit/1e82335413bc2384073ead0d6d581c862036d0f5,/
SPARK-21866,[ML][PYSPARK] Adding spark image reader,FileInputFormat.INPUT_DIR_RECURSIVe,https://issues.apache.org/jira/browse/SPARK-21866,https://github.com/apache/spark/commit/1edb3175d8358c2f6bfc84a0d958342bd5337a62,/
SPARK-21866,[ML][PYSPARK] Adding spark image reader,SamplePathFilter.ratioParam,https://issues.apache.org/jira/browse/SPARK-21866,https://github.com/apache/spark/commit/1edb3175d8358c2f6bfc84a0d958342bd5337a62,/
SPARK-21866,[ML][PYSPARK] Adding spark image reader,SamplePathFilter.seedParam,https://issues.apache.org/jira/browse/SPARK-21866,https://github.com/apache/spark/commit/1edb3175d8358c2f6bfc84a0d958342bd5337a62,/
SPARK-22203,[SQL] Add job description for file listing Spark jobs,SparkContext.SPARK_JOB_DESCRIPTION,https://issues.apache.org/jira/browse/SPARK-22203,https://github.com/apache/spark/commit/c8affec21c91d638009524955515fc143ad86f20,/
SPARK-22238,Fix plan resolution bug caused by EnsureStatefulOpPartitioning,SQLConf.SHUFFLE_PARTITIONS.key,https://issues.apache.org/jira/browse/SPARK-22238,https://github.com/apache/spark/commit/e8547ffb49071525c06876c856cecc0d4731b918,/
SPARK-22239,[SQL][PYTHON] Enable grouped aggregate pandas UDFs as window functions with unbounded window frames,spark.buffer.size,https://issues.apache.org/jira/browse/SPARK-22239,https://github.com/apache/spark/commit/9786ce66c52d41b1d58ddedb3a984f561fd09ff3,/
SPARK-22239,[SQL][PYTHON] Enable grouped aggregate pandas UDFs as window functions with unbounded window frames,spark.python.worker.reuse,https://issues.apache.org/jira/browse/SPARK-22239,https://github.com/apache/spark/commit/9786ce66c52d41b1d58ddedb3a984f561fd09ff3,/
SPARK-22239,[SQL][PYTHON] Enable grouped aggregate pandas UDFs as window functions with unbounded window frames,conf.sessionLocalTimeZone,https://issues.apache.org/jira/browse/SPARK-22239,https://github.com/apache/spark/commit/9786ce66c52d41b1d58ddedb3a984f561fd09ff3,/
SPARK-22239,[SQL][PYTHON] Enable grouped aggregate pandas UDFs as window functions with unbounded window frames,conf.pandasRespectSessionTimeZone,https://issues.apache.org/jira/browse/SPARK-22239,https://github.com/apache/spark/commit/9786ce66c52d41b1d58ddedb3a984f561fd09ff3,/
SPARK-22274,[PYTHON][SQL] User-defined aggregation functions with pandas udf,spark.buffer.size,https://issues.apache.org/jira/browse/SPARK-22274,https://github.com/apache/spark/commit/b2ce17b4c9fea58140a57ca1846b2689b15c0d61,/
SPARK-22274,[PYTHON][SQL] User-defined aggregation functions with pandas udf,spark.python.worker.reuse,https://issues.apache.org/jira/browse/SPARK-22274,https://github.com/apache/spark/commit/b2ce17b4c9fea58140a57ca1846b2689b15c0d61,/
SPARK-22290,[CORE] Avoid creating Hive delegation tokens when not necessary.,spark.submit.deployMode,https://issues.apache.org/jira/browse/SPARK-22290,https://github.com/apache/spark/commit/dc2714da50ecba1bf1fdf555a82a4314f763a76e,/
SPARK-22463,[YARN][SQL][HIVE] add hadoop/hive/hbase/etc CONFIGuration files in SPARK_CONF_DIR to distribute archive,SPARK_CONF_DIR,https://issues.apache.org/jira/browse/SPARK-22463,https://github.com/apache/spark/commit/c755b0d910d68e7921807f2f2ac1e3fac7a8f357,/
SPARK-22529,[SQL] Relation stats should be consistent with other plans based on cbo CONFIG,conf.cboEnabled,https://issues.apache.org/jira/browse/SPARK-22529,https://github.com/apache/spark/commit/1ff4a77be498615ee7216fd9cc2d510ecbd43b27,/
SPARK-24137,[K8S] Mount local directories as empty dir volumes.,spark.local.dir,https://issues.apache.org/jira/browse/SPARK-24137,https://github.com/apache/spark/commit/6282fc64e32fc2f70e79ace14efd4922e4535dbb,/
SPARK-24149,Retrieve all federated namespaces tokens,dfs.nameservices,https://issues.apache.org/jira/browse/SPARK-24149,https://github.com/apache/spark/commit/3159ee085b23e2e9f1657d80b7ae3efe82b5edb9,/
SPARK-24149,Retrieve all federated namespaces tokens,dfs.namenode.rpc-address,https://issues.apache.org/jira/browse/SPARK-24149,https://github.com/apache/spark/commit/3159ee085b23e2e9f1657d80b7ae3efe82b5edb9,/
SPARK-24149,Retrieve all federated namespaces tokens,dfs.ha.namenodes,https://issues.apache.org/jira/browse/SPARK-24149,https://github.com/apache/spark/commit/3159ee085b23e2e9f1657d80b7ae3efe82b5edb9,/
SPARK-24324,[PYTHON] Pandas Grouped Map UDF should assign result columns by name,SQLConf.SESSION_LOCAL_TIMEZONE,https://issues.apache.org/jira/browse/SPARK-24324,https://github.com/apache/spark/commit/a5849ad9a3e5d41b5938faa7c592bcc6aec36044,/
SPARK-24377,[SPARK SUBMIT] make --py-files work in non pyspark application,spark.submit.pyFiles,https://issues.apache.org/jira/browse/SPARK-24377,https://github.com/apache/spark/commit/2ced6193b39dc63e5f74138859f2a9d69d3cfd11,/
SPARK-24396,[SS][PYSPARK] Add Structured Streaming ForeachWriter for python,spark.buffer.size,https://issues.apache.org/jira/browse/SPARK-24396,https://github.com/apache/spark/commit/b5ccf0d3957a444db93893c0ce4417bfbbb11822,/
SPARK-24396,[SS][PYSPARK] Add Structured Streaming ForeachWriter for python,spark.python.worker.reuse,https://issues.apache.org/jira/browse/SPARK-24396,https://github.com/apache/spark/commit/b5ccf0d3957a444db93893c0ce4417bfbbb11822,/
SPARK-27872,[K8S] Fix executor service account inconsistency,KUBERNETES_SERVICE_ACCOUNT_NAME,https://issues.apache.org/jira/browse/SPARK-27872,https://github.com/apache/spark/commit/7912ab85a6fc086b814d93e7c71af1f50515517a,/
HBASE-22274,Cell size limit check on append considers cell's previous size,this.maxCellSize,https://issues.apache.org/jira/browse/HBASE-22274,https://github.com/apache/hbase/commit/987d36879c310d567f3b0b0e04a91960f14f453f,/
HDFS-12052,Set SWEBHDFS delegation token kind when ssl is enabled in HttpFS,httpfs.ssl.enabled,https://issues.apache.org/jira/browse/HDFS-12052,https://github.com/apache/hadoop/commit/12c8fdceaf263425661169cba25402df89d444c1,/
SPARK-22788,"HdfsUtils.getOutputStream uses non-existent Hadoop conf ""hdfs.append.support""",dfs.support.append,https://issues.apache.org/jira/browse/SPARK-22788,https://github.com/apache/spark/commit/7570eab6bee57172ee3746207261307690a57b72,/
SPARK-22860,Redact command line arguments,SECRET_REDACTION_PATTERN,https://issues.apache.org/jira/browse/SPARK-22860,https://github.com/apache/spark/commit/c17150a5f5a6c4f4a83ce8c055eab9fea78df08e#,/
SPARK-28366,Refine logging in driver when loading single large unsplittable file,IO_WARNING_LARGEFILETHRESHOLD,https://issues.apache.org/jira/browse/SPARK-28366,https://github.com/apache/spark/commit/0f2efe6825a5b50b50bd1aeb8ee970fd190824f,/
SPARK-23098,Migrate Kafka Batch source to v2.,NETWORK_TIMEOUT,https://issues.apache.org/jira/browse/SPARK-23098,https://github.com/apache/spark/commit/0a4f985ca0218ec34a249a7d9d655f060b9debc,/
SPARK-28200,Decimal overflow handling in ExpressionEncoder,SQLConf.get.decimalOperationsNullOnOverflow,https://issues.apache.org/jira/browse/SPARK-28200,https://github.com/apache/spark/commit/683e270c16258490c7c1deedb05fe44d45c3961,/
SPARK-28224,Check overflow in decimal Sum aggregate,decimalOperationsNullOnOverflow,https://issues.apache.org/jira/browse/SPARK-28224,https://github.com/apache/spark/commit/b79cf0d14351c741efe4f27523919a0e24b8b2e,/
SPARK-29048,Improve performance on Column.isInCollection() with a large size collection,optimizerInSetConversionThreshold,https://issues.apache.org/jira/browse/SPARK-29048,https://github.com/apache/spark/commit/5631a96367d2576e1e0f95d7ae529468da8f5fa,/
SPARK-29008,Define an individual method for each common subexpression in HashAggregateExec,methodSplitThreshold,https://issues.apache.org/jira/browse/SPARK-29008,https://github.com/apache/spark/commit/95073fb62b646c3e8394941c5835a396b9d48c0,/
SPARK-29247,Redact sensitive information in when construct HiveClientHive.state,SQLConf.get.redactOptions,https://issues.apache.org/jira/browse/SPARK-29247,https://github.com/apache/spark/commit/1d4b2f010b8e1224985f4204c3e1d88c65d71f6,/
SPARK-29415,Stage Level Sched: Add base ResourceProfile and Request classes,EXECUTOR_CORES,https://issues.apache.org/jira/browse/SPARK-29415,https://github.com/apache/spark/commit/2d5de25a999e0e5580cf4024937b61e6c926567,/
SPARK-29415,Stage Level Sched: Add base ResourceProfile and Request classes,EXECUTOR_MEMORY,https://issues.apache.org/jira/browse/SPARK-29415,https://github.com/apache/spark/commit/2d5de25a999e0e5580cf4024937b61e6c926567,/
SPARK-29415,Stage Level Sched: Add base ResourceProfile and Request classes,CPUS_PER_TASK,https://issues.apache.org/jira/browse/SPARK-29415,https://github.com/apache/spark/commit/2d5de25a999e0e5580cf4024937b61e6c926567,/
SPARK-30107,Expose nested schema pruning to all V2 sources,spark.sql.optimizer.nestedSchemaPruning.enabled,https://issues.apache.org/jira/browse/SPARK-30107,https://github.com/apache/spark/commit/5114389aef2cacaacc82e6025696b33d6d20b2a,/
HDFS-12979,StandbyNode should upload FsImage to ObserverNode,DFS_NAMENODE_CHECKPOINT_TXNS_KEY,https://issues.apache.org/jira/browse/HDFS-12979,https://github.com/apache/hadoop/commit/5e6cc6fe8a11a638ba98913ca402efdc988fe73,/
HBASE-22578,HFileCleaner should not delete empty ns/table directories used for user san snapshot feature (#337),MASTER_COPROCESSOR_CONF_KEY,https://issues.apache.org/jira/browse/HBASE-22578,https://github.com/apache/hbase/commit/a399a1452b4840f1a147f84566a76a85f599490,/
HBASE-22578,HFileCleaner should not delete empty ns/table directories used for user san snapshot feature (#337),USER_SCAN_SNAPSHOT_ENABLE,https://issues.apache.org/jira/browse/HBASE-22578,https://github.com/apache/hbase/commit/a399a1452b4840f1a147f84566a76a85f599490,/
HBASE-23107,Avoid temp byte array creation when doing cacheDataOnWrite (#678),HFILEBLOCK_HEADER_SIZE,https://issues.apache.org/jira/browse/HBASE-23107,https://github.com/apache/hbase/commit/0043dfebc5e43705818071c3de062211943829f,/
HBASE-23197,IllegalArgumentException: Wrong FS' on edits replay when… (#740),HBASE_DIR,https://issues.apache.org/jira/browse/HBASE-23197,https://github.com/apache/hbase/commit/b08697ae4a347f34273253e33ba91bb6b7ade5e,/
HBASE-23239,Reporting on status of backing MOB files from client-facing cells (#785),HBASE_DIR,https://issues.apache.org/jira/browse/HBASE-23239,https://github.com/apache/hbase/commit/bc2f16274930e272767ce20ede27eb8fa411724,/
CASSANDRA-15295,Avoid deadlock during CommitLog initialization,cdc_enabled,https://issues.apache.org/jira/browse/CASSANDRA-15295,https://github.com/apache/cassandra/commit/3a8300e0b86c4acfb7b7702197d36cc39ebe94bc,/
CASSANDRA-15295,Avoid deadlock during CommitLog initialization,commitlog_directory,https://issues.apache.org/jira/browse/CASSANDRA-15295,https://github.com/apache/cassandra/commit/3a8300e0b86c4acfb7b7702197d36cc39ebe94bc,/
HBASE-20590,REST Java client is not able to negotiate with the server in the secure mode,REST_KERBEROS_PRINCIPAL,https://issues.apache.org/jira/browse/HBASE-20590,https://github.com/apache/hbase/commit/7da0015a3b58a28ccbae0b03ba7de9ce62b751e1,check code check condition different: != null && ! isEmpty
HBASE-19258,IntegrationTest for Backup and Restore,MASTER_HFILE_CLEANER_PLUGINS,https://issues.apache.org/jira/browse/HBASE-19258,https://github.com/apache/hbase/commit/64ef12080899dad4ab60652a76c68fb59ded04f2,"check code check condition different: contains(cleanerClass),  ==null"
HBASE-16261,MultiHFileOutputFormat Enhancement,TEMPORARY_FS_DIRECTORY_KEY,https://issues.apache.org/jira/browse/HBASE-16261,https://github.com/apache/hbase/commit/c7a7f880dd99a29183e54f0092c10e7a70186d9d,check code check point different
SPARK-22998,[K8S] Set missing value for SPARK_MOUNTED_CLASSPATH in the executors,JARS_DOWNLOAD_LOCATION,https://issues.apache.org/jira/browse/SPARK-22998,https://github.com/apache/spark/commit/6a4206ff04746481d7c8e307dfd0d31ff1402555,"check code: require(downloadDir.isDirectory, errMessage)"
HBASE-21809,Add retry thrift client for ThriftTable/Admin,HBASE_CLIENT_PAUSE,https://issues.apache.org/jira/browse/HBASE-21809,https://github.com/apache/hbase/commit/2776bc0151051c8d20d9b1c2ac6142ade6a31b62,default value inconsistent
SPARK-24926,Ensure numCores is used consistently in all netty configurations,spark.driver.cores,https://issues.apache.org/jira/browse/SPARK-24926,https://github.com/apache/spark/commit/70462f291bf046c648f36063b0161861e6d11898,default value inconsistent
HBASE-23239,Reporting on status of backing MOB files from client-facing cells (#785)*,HBASE_CLIENT_SCANNER_CACHING,https://issues.apache.org/jira/browse/HBASE-24926,https://github.com/apache/hbase/commit/bc2f16274930e272767ce20ede27eb8fa411724,default value inconsistent
SPARK-18061,[THRIFTSERVER] Add spnego auth support for ThriftServer thrift/http protocol,ConfVars.HIVE_SERVER2_SPNEGO_PRINCIPAL,https://issues.apache.org/jira/browse/SPARK-18061,https://github.com/apache/spark/commit/6a2325448000ba431ba3b982d181c017559abfe3,Message: no log info
SPARK-18061,[THRIFTSERVER] Add spnego auth support for ThriftServer thrift/http protocol,ConfVars.HIVE_SERVER2_SPNEGO_KEYTAB,https://issues.apache.org/jira/browse/SPARK-18061,https://github.com/apache/spark/commit/6a2325448000ba431ba3b982d181c017559abfe3,Message: no log info
HBASE-20561,The way we stop a ReplicationSource may cause the RS down,sleepForRetries,https://issues.apache.org/jira/browse/HBASE-20561,https://github.com/apache/hbase/commit/ec66434380aee62289ccf7b173d765bbe7083718,Message: no log info
SPARK-20791,[PYSPARK] Use Arrow to create Spark DataFrame from Pandas,spark.sql.execution.arrow.enabled,https://issues.apache.org/jira/browse/SPARK-20791,https://github.com/apache/spark/commit/209b9361ac8a4410ff797cff1115e1888e2f7e66,"Message: warn quality not good, no explicit option name"
SPARK-20652,[SQL] Store SQL UI data in the new app status store.,LIVE_ENTITY_UPDATE_PERIOD,https://issues.apache.org/jira/browse/SPARK-20652,https://github.com/apache/spark/commit/0ffa7c488fa8156e2a1aa282e60b7c36b86d8af8,Message:code comment quality are not good
HBASE-23197,IllegalArgumentException: Wrong FS' on edits replay when… (#740),HBASE_WAL_DIR,https://issues.apache.org/jira/browse/HBASE-23197,https://github.com/apache/hbase/commit/b08697ae4a347f34273253e33ba91bb6b7ade5e,Message:error message is different
HBASE-22261,Make use of ClusterStatusListener for async client,STATUS_LISTENER_CLASS,https://issues.apache.org/jira/browse/HBASE-22261,https://github.com/apache/hbase/commit/eb912bb1dab793248f47d6ed8ee6f8abb467d10b,Message:log message is a little bit different
HDFS-13977,Override shouldForceSync in QuorumOutputStream to allow for proper auto-sync behavior,IPC_MAXIMUM_DATA_LENGTH,https://issues.apache.org/jira/browse/HDFS-13977,https://github.com/apache/hadoop/commit/d699022fce756d25956d33e022100111aa0dd22,Message:log message is a little bit different
HDFS-13165,[SPS]: Collects successfully moved block details via IBR,DFS_STORAGE_POLICY_ENABLED_KEY,https://issues.apache.org/jira/browse/HDFS-13165,https://github.com/apache/hadoop/commit/2acc50b826fa8b00f2b09d9546c4b3215b89d46d,"Message:log quality not good, no explicit option name"
HDFS-13033,[SPS]: Implement a mechanism to do file block movements for external SPS. ,DFS_CLIENT_USE_DN_HOSTNAME,https://issues.apache.org/jira/browse/HDFS-13033,https://github.com/apache/hadoop/commit/b0cb8d9bb44c963ae686d2b5c1b70bc76b955e10,use Deprecated param
HDFS-12553,Add nameServiceId to QJournalProtocol,DFS_NAMESERVICE_ID,https://issues.apache.org/jira/browse/HDFS-12553,https://github.com/apache/hadoop/commit/8dd1eeb94fef59feaf19182dd8f1fcf1389c7f34,use deprecated param HdfsClientConfigKeys.DeprecatedKeys.DFS_NAMESERVICE_ID
HDFS-12979,StandbyNode should upload FsImage to ObserverNode,DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,https://issues.apache.org/jira/browse/HDFS-12979,https://github.com/apache/hadoop/commit/5e6cc6fe8a11a638ba98913ca402efdc988fe73,use deprecated param use HdfsClientConfigKeys.DeprecatedKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY
HDFS-14560,Allow block replication parameters to be refreshable.,DFS_NAMENODE_REPLICATION_MAX_STREAMS_KEY,https://issues.apache.org/jira/browse/HDFS-14560,https://github.com/apache/hadoop/commit/4f455290b15902e7e44c4b1a762bf915414b2bb6,use deprecated param use HdfsClientConfigKeys.DeprecatedKeys.DFS_NAMENODE_REPLICATION_MAX_STREAMS_KEY
HDFS-12895,RBF: Add ACL support for mount table,DFS_PERMISSIONS_ENABLED_KEY,https://issues.apache.org/jira/browse/HDFS-12895,https://github.com/apache/hadoop/commit/ee028bfdf1c88a27cd925bed93ebb599a164dd2e,use deprecated param use HdfsClientConfigKeys.DeprecatedKeys.DFS_PERMISSIONS_ENABLED_KEY
HDFS-12895,RBF: Add ACL support for mount table,DFS_PERMISSIONS_SUPERUSERGROUP_KEY,https://issues.apache.org/jira/browse/HDFS-12895,https://github.com/apache/hadoop/commit/ee028bfdf1c88a27cd925bed93ebb599a164dd2e,use deprecated param use HdfsClientConfigKeys.DeprecatedKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY
SPARK-24926,Ensure numCores is used consistently in all netty configurations,spark.submit.deployMode,https://issues.apache.org/jira/browse/SPARK-24926,https://github.com/apache/spark/commit/70462f291bf046c648f36063b0161861e6d11898,use statement  == or contain
HBASE-20586,SyncTable tool: Add support for cross-realm remote clusters,hbase.security.authentication,https://issues.apache.org/jira/browse/HBASE-20586,https://github.com/apache/hbase/commit/cd61bcc01eb29eb3509f72cf72326605afefabc8,use statement: case-sensitive
SPARK-29558,ResolveTables and ResolveRelations should be order-insensitive,spark.sql.globalTempDatabase (static configuration),https://issues.apache.org/jira/browse/SPARK-29558,https://github.com/apache/spark/commit/6b4b6a87cde8e29da5cbc2ee00242ec74d5477b,use statement: case-sensitive
HDFS-12486,GetConf to get journalnodeslist.,DFS_NAMENODE_SHARED_EDITS_DIR_KEY,https://issues.apache.org/jira/browse/HDFS-12486,https://github.com/apache/hadoop/commit/cda3378659772f20fd951ae342dc7d9d6db29534,use statement: conf.getTrimmed
SPARK-29347,Add JSON serialization for external Rows,spark.sql.session.timeZone,https://issues.apache.org/jira/browse/SPARK-29347,https://github.com/apache/spark/commit/1f1443ebb2a52d311e599880c4065977ec1ee9d,use statement: DateFormatter or DateFormatter.apply
SPARK-30112,Allow insert overwrite same table if using dynamic partition overwrite,spark.sql.sources.partitionOverwriteMode,https://issues.apache.org/jira/browse/SPARK-30112,https://github.com/apache/spark/commit/c1a5f94973213b1cad15388f3ef8a488424c34a,use statement: get api
SPARK-23049,[SQL] `spark.sql.files.ignoreCorruptFiles` should work for ORC files,spark.sql.files.ignoreCorruptFiles,https://issues.apache.org/jira/browse/SPARK-23049,https://github.com/apache/spark/commit/9a96bfc8bf021cb4b6c62fac6ce1bcf87affcd43,use statement: pre-condition: runtime exception or IO exception