Issue-ID,Parameter,Title,Issue-URL,Commit-URL,Param type,Rationale,New scenario/use case,Impact,Note,discussion,Whether documented
SPARK-20728,spark.sql.orc.impl,[SQL] Make OrcFileFormat CONFIGurable between sql/hive and sql/core,https://issues.apache.org/jira/browse/SPARK-20728,https://github.com/apache/spark/commit/326f1d6728a7734c228d8bfaa69442a1c7b92e9b,class implementation,compatibility,concrete,/,This issue depends on SPARK-20682 and aims to provide a configuration to choose the default ORCFileFormat from legacy `sql/hive` module or new `sql/core` module.,/,no
SPARK-25129,spark.sql.legacy.replaceDatabricksSparkAvro.enabled,Make the mapping of com.databricks.spark.avro to built-in module configurable,https://issues.apache.org/jira/browse/SPARK-25129,https://github.com/apache/spark/commit/ac0174e55af2e935d41545721e9f430c942b3a0c,feature selection,compatibility,concrete,/,"In https://issues.apache.org/jira/browse/SPARK-24924, the data source provider com.databricks.spark.avro is mapped to the new package org.apache.spark.sql.avro. make the mapping of com.databricks.avro => internal avro configurable, that would allow them to continue use their version of com.databricks.avro until they can update api.",/,no
SPARK-28730,spark.sql.storeAssignmentPolicy,Configurable type coercion policy for table insertion,https://issues.apache.org/jira/browse/SPARK-28730,https://github.com/apache/spark/commit/895c90b582cc2b2667241f66d5b733852aeef9e,policy,compatibility,concrete,/,"To ensure backward compatibility with existing queries, the default store assignment policy for data source V1 is ""legacy"" before ANSI mode is implemented.",Balancing flexibility and simplicity: You can't deny this large class of use cases out there (probably makes up half of Spark use cases actually),no
HBASE-23081,hbase.balancer.rsgroup.enabled,Add an option to enable/disable rs group feature,https://issues.apache.org/jira/browse/HBASE-23081,https://github.com/apache/hbase/commit/0f4a87c24dc8d5c29e31df27ec741fbca9981c9,feature selection,compatibility,Concrete task: if you want the pre 3.0.0 hbase client/shell to communicate with the new hbase cluster,/,"The coprocessor org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint is deprected, but for compatible, if you want the pre 3.0.0 hbase client/shell to communicate with the new hbase cluster, you still need to add this coprocessor to master.",/,no
SPARK-28395,spark.sql.function.preferIntegralDivision,Division operator support integral division,https://issues.apache.org/jira/browse/SPARK-28395,https://github.com/apache/spark/commit/69268492471137dd7a3da54c218026c3b1fa7db,feature selection,compatibility,unknow,/,This pr add a flag(`spark.sql.function.preferIntegralDivision`) to control whether to use integral division with the `/` operator.,/,no
SPARK-20331,spark.sql.hive.advancedPartitionPredicatePushdown.enabled,[SQL][FOLLOW-UP] Add a SQLConf for enhanced Hive partition pruning predicate pushdown,https://issues.apache.org/jira/browse/SPARK-20331,https://github.com/apache/spark/commit/d8cada8d1d3fce979a4bc1f9879593206722a3b9#,feature selection,compatibility,vague,/,we don't backport such risky changes to maintenance branches.,/,no
SPARK-20923,spark.taskMetrics.trackUpdatedBlockStatuses,turn tracking of TaskMetrics._updatedBlockStatuses off,https://issues.apache.org/jira/browse/SPARK-20923,https://github.com/apache/spark/commit/5b5a69bea9de806e2c39b04b248ee82a7b664d7b#,feature selection,compatibility,vague,/,We leave the api there as well as a config to turn it back on just in case anyone is using it.,Balancing flexibility and simplicity:If its not being used we should remove it,no
SPARK-24307,spark.network.remoteReadNioBufferConversion,[CORE] Add conf to revert to old code.,https://issues.apache.org/jira/browse/SPARK-24307,https://github.com/apache/spark/commit/2c82745686f4456c4d5c84040a431dcb5b6cb60b,feature selection,compatibility,vague,/,"In case there are any issues in converting FileSegmentManagedBuffer to ChunkedByteBuffer, add a conf to go back to old code path.",/,no
SPARK-27253,spark.sql.legacy.sessionInitWithConfigDefaults,Add a legacy flag to restore old session init behavior,https://issues.apache.org/jira/browse/SPARK-27253,https://github.com/apache/spark/commit/83f628b57da39ad9732d1393aebac373634a2eb9,feature selection,compatibility,vague,/,"Add a legacy flag to restore the old session init behavior, where SparkConf defaults take precedence over configs in a parent session.",/,no
CASSANDRA-14659,native_transport_allow_older_protocols,Disable old native protocol versions on demand,https://issues.apache.org/jira/browse/CASSANDRA-14659,https://github.com/apache/cassandra/commit/7b61b0be88ef1fcc29646ae8bdbb05da825bc1b2,feature selection,compatibility,concrete,/,This will help operators selectively reject connections from clients that do not support the latest protoocol.,/,yes
SPARK-29444,ignoreNullFields (API configuration whose normal configuration is: spark.sql.jsonGenerator.ignoreNullFields),[FOLLOWUP] add doc and python PARAMETER for ignoreNullFields in json generating,https://issues.apache.org/jira/browse/SPARK-29444,https://github.com/apache/spark/commit/dcf5eaf1a6c0330a9460e168c1c3fee21998ba6,feature selection,compatibility,concrete environment: Enable user to use ignoreNullFields in pyspark (as in Spark),/,"DataSet.toJSON will lost some column when field data is null. Maybe it is better to keep null data in some scenarios. Such as sparkmagic, which is widely used in jupyter with livy, we use toJSON to toJSON is used to get sql results. with toJSON sparkmagic may return empty results, which confused users.Maybe adding a config is the best choice. This configuration retains the current semantics and will remain fields with null until the configuration is modified.",/,yes
SPARK-30113,mergeSchema (API configuration whose normal configuration is: spark.sql.parquet.mergeSchema),Expose mergeSchema OPTION in PySpark's ORC APIs,https://issues.apache.org/jira/browse/SPARK-30113,https://github.com/apache/spark/commit/c8922d9145a9bc60c0f423a6c1b7d4f0bfa2e58,feature selection,compatibility,unknow,/,Why are the changes needed? So the Python API matches the Scala API.,/,yes
SPARK-30098,spark.sql.legacy.respectHiveDefaultProvider.enabled,Use default datasource as provider for CREATE TABLE syntax,https://issues.apache.org/jira/browse/SPARK-30098,https://github.com/apache/spark/commit/58be82ad4b98fc17e821e916e69e77a6aa36209,feature selection,compatibility,unknow,/,"Changing the default provider from `hive` to the value of `spark.sql.sources.default` for ""CREATE TABLE"" syntax to make it be consistent with DataFrameWriter.saveAsTable API. Also, it brings more friendly to end users since Spark is well know of using parquet(default value of `spark.sql.sources.default`) as its default I/O format.",/,yes
HBASE-20942,hbase.ipc.trace.log.max.length,Improve RpcServer TRACE logging,https://issues.apache.org/jira/browse/HBASE-20942,https://github.com/apache/hbase/commit/fcd883b5dd7f3e2987646407f8f03c5513d93a9b,threshold,debuggability,unknow,/,We truncate RpcServer output to 1000 characters for trace logging. Would be better if that value was configurable.,/,no
SPARK-25415,spark.sql.optimizer.planChangeLog.level,Make plan change log in RuleExecutor configurable by SQLConf,https://issues.apache.org/jira/browse/SPARK-25415,https://github.com/apache/spark/commit/8b702e1e0aba1d3e4b0aa582f20cf99f80a44a09,Log level,debuggability,vague task: helpful for debugging,/,"such information can be very helpful for debugging, so making the log level configurable in SQLConf would allow users to turn on the plan change log independently and save the trouble of tweaking log4j settings. Meanwhile, filtering plan change log for specific rules can also be very useful.",/,no
SPARK-22959,spark.python.daemon.module,[PYTHON] Configuration to select the modules for daemon and worker in PySpark,https://issues.apache.org/jira/browse/SPARK-22959,https://github.com/apache/spark/commit/afae8f2bc82597593595af68d1aa2d802210ea8b,class implementation,debuggability,vague task: it's sometimes hard to debug what happens,/,We are now forced to use pyspark/daemon.py and pyspark/worker.py in PySpark tests.,/,no
SPARK-22959,spark.python.worker.module,[PYTHON] Configuration to select the modules for daemon and worker in PySpark,https://issues.apache.org/jira/browse/SPARK-22959,https://github.com/apache/spark/commit/afae8f2bc82597593595af68d1aa2d802210ea8b,class implementation,debuggability,vague task: it's sometimes hard to debug what happens,/,We are now forced to use pyspark/daemon.py and pyspark/worker.py in PySpark tests.,/,no
SPARK-26389,spark.sql.streaming.forceDeleteTempCheckpointLocation,Add force delete temp checkpoint configuration,https://issues.apache.org/jira/browse/SPARK-26389,https://github.com/apache/spark/commit/701b06a7e2e76e5d9ed020c62e0ed3464fa2818b,feature selection,debuggability,vague workload: running the stream application multiple times,Failure: job failed,Not all users wants to keep temporary checkpoint directories.,/,no
SPARK-22050,spark.eventLog.logBlockUpdates.enabled,[CORE] Allow BlockUpdated events to be optionally logged to the event log,https://issues.apache.org/jira/browse/SPARK-22050,https://github.com/apache/spark/commit/1437e344ec0c29a44a19f4513986f5f184c44695,feature selection,debuggability,vague Task: get a better understanding of caching for a job,/,"I see that block updates are not logged to the event log. This makes sense as a default for performance reasons. However, I find it helpful when trying to get a better understanding of caching for a job to be able to log these updates.",/,yes
HDFS-11060,dfs.namenode.max-corrupt-file-blocks-returned,make DEFAULT_MAX_CORRUPT_FILEBLOCKS_RETURNED configurable,https://issues.apache.org/jira/browse/HDFS-11060,https://github.com/apache/hadoop/commit/e95c5e9f62452ee848875ec2f8642eab4992cd23,threshold,debuggability,vague task: helpful for debugging,/,"Current, the easiest way to determine which blocks is missing is using NN web UI or JMX. Unfortunately, because the DEFAULT_MAX_CORRUPT_FILEBLOCKS_RETURNED=100 is hard code in FSNamesystem, only 100 missing blocks can be return by UI and JMX.",/,yes
SPARK-23820,spark.eventLog.longForm.enabled,Enable use of long form of callsite in logs,https://issues.apache.org/jira/browse/SPARK-23820,https://github.com/apache/spark/commit/ab25c967905ca0973fc2f30b8523246bb9244206,feature selection,debuggability,vague Task: some performance debugging,/,It would be nice if the long form of the callsite information could be included in the log. I don't expect it will be widely used but I've personally found it helpful in some performance debugging and it's a fairly low impact change.,"Balancing flexibility and simplicity: Although maybe not widely used, I could see allowing control of this via an undocumented param",yes
SPARK-25515,spark.kubernetes.executor.deleteOnTermination,Adds a config option to keep executor pods for debugging,https://issues.apache.org/jira/browse/SPARK-25515,https://github.com/apache/spark/commit/0c2935b01def8a5f631851999d9c2d57b63763e6,feature selection,debuggability,concrete usecase: if a pod fails to start due to some failure,"Failure: These sequence of events go on, until the app is killed","Currently, if a pod fails to start due to some failure, it gets removed and new one is attempted. These sequence of events go on, until the app is killed. Given the speed of creation and deletion, it becomes difficult to debug the reason for failure. So adding a configuration parameter to disable auto-deletion of pods, will be helpful for debugging.",/,yes
CASSANDRA-13518,storage-port,sstableloader doesn't support non default storage_port and ssl_storage_port,https://issues.apache.org/jira/browse/CASSANDRA-13518,https://github.com/apache/cassandra/commit/428eaa3e37cab7227c81fdf124d29dfc1db4257c,port,env setting,unknow,/,The proposed fix is to add command line option for these two ports like what NATIVE_PORT_OPTION currently does,/,no
CASSANDRA-13518,ssl-storage-port,sstableloader doesn't support non default storage_port and ssl_storage_port,https://issues.apache.org/jira/browse/CASSANDRA-13518,https://github.com/apache/cassandra/commit/428eaa3e37cab7227c81fdf124d29dfc1db4257c,port,env setting,unknow,/,The proposed fix is to add command line option for these two ports like what NATIVE_PORT_OPTION currently does,/,no
HBASE-20469,TEMPORARY_FS_DIRECTORY_KEY (hbase.fs.tmp.dir),Directory used for sidelining old recovered edits files should be made configurable,https://issues.apache.org/jira/browse/HBASE-20469,https://github.com/apache/hbase/commit/092efb42749bf7fc6ad338c96aae8e7b9d3a2c74,file directory,env setting,unknow,/,"Currently the directory used for sidelining of old recovered edit files is hardcoded to be ""/tmp"" Instead, we can use some configurable directory in the following manner:",/,no
SPARK-24203,#NAME?,Make executor's bindAddress CONFIGURABLE ,https://issues.apache.org/jira/browse/SPARK-24203,https://github.com/apache/spark/commit/833a9f12e2d46b5741e0522d5c86c9f6d88bb9d,Ip address,env setting,concrete Environment: Kernel-based Virtual Machine (KVM)'s are used inside Linux container,Failure:  wouldn't have worked for multiple executors.,/,/,no
HBASE-19572,REGIONSERVER_PORT (hbase.regionserver.port),RegionMover should use the configured default port number and not the one from HConstants,https://issues.apache.org/jira/browse/HBASE-19572,https://github.com/apache/hbase/commit/ce82fd0f4765f40ff44357719ffd47cd803f07fb,port,env setting,vague usecase: bug,Failure: java.lang.Exception,it was an error due the argument passed to the RegionMover.,/,no
SPARK-20025,spark.driver.port,[CORE] Ignore SPARK_LOCAL* env while deploying via cluster mode.,https://issues.apache.org/jira/browse/SPARK-20025,https://github.com/apache/spark/commit/23af2d79ad9a3c83936485ee57513b39193a446b,port,env setting,concrete Environment: in a bare metal system with No DNS setup,Failure:driver fail over will not work,"In a bare metal system with No DNS setup, spark may be configured with SPARK_LOCAL* for IP and host properties.",/,no
HBASE-19394,hbase.status.multicast.ni.name ,Support multi-homing env for the publication of RS status with multicast,https://issues.apache.org/jira/browse/HBASE-19394,https://github.com/apache/hbase/commit/11467ef11126c77b3692479730752ee38dff20a6,Interface name,env setting,concrete usecase: host has the multiple network interfaces and the unreachable one to the other nodes is selected.,uxepected result: won't work,"Currently, when the publication feature is enabled (hbase.status.published=true), it uses the interface which is found first.This won't work when the host has the multiple network interfaces and the unreachable one to the other nodes is selected.",/,no
HBASE-19394,hbase.status.multicast.publisher.bind.address.ip,Support multi-homing env for the publication of RS status with multicast,https://issues.apache.org/jira/browse/HBASE-19394,https://github.com/apache/hbase/commit/11467ef11126c77b3692479730752ee38dff20a6,Ip address,env setting,concrete usecase: host has the multiple network interfaces and the unreachable one to the other nodes is selected.,uxepected result: won't work,"Currently, when the publication feature is enabled (hbase.status.published=true), it uses the interface which is found first.This won't work when the host has the multiple network interfaces and the unreachable one to the other nodes is selected.",/,no
HDFS-13462,dfs.journalnode.rpc-bind-host,Add BIND_HOST configuration for JournalNode's HTTP and RPC Servers,https://issues.apache.org/jira/browse/HDFS-13462,https://github.com/apache/hadoop/commit/c9b33514b8d11db44b5f95b4df43789ed45c47a7,host name,env setting,vague Environment: multihomed environments,/,Allow configurable bind-host for JournalNode's HTTP and RPC servers to allow overriding the hostname for which the server accepts connections.,/,yes
HDFS-13462,dfs.journalnode.http-bind-host,Add BIND_HOST configuration for JournalNode's HTTP and RPC Servers,https://issues.apache.org/jira/browse/HDFS-13462,https://github.com/apache/hadoop/commit/c9b33514b8d11db44b5f95b4df43789ed45c47a7,host name,env setting,vague Environment: multihomed environments,/,Allow configurable bind-host for JournalNode's HTTP and RPC servers to allow overriding the hostname for which the server accepts connections.,/,yes
HDFS-13462,dfs.journalnode.https-bind-host,Add BIND_HOST configuration for JournalNode's HTTP and RPC Servers,https://issues.apache.org/jira/browse/HDFS-13462,https://github.com/apache/hadoop/commit/c9b33514b8d11db44b5f95b4df43789ed45c47a7,host name,env setting,vague Environment: multihomed environments,/,Allow configurable bind-host for JournalNode's HTTP and RPC servers to allow overriding the hostname for which the server accepts connections.,/,yes
HDFS-13018,dfs.cblock.iscsi.advertised.ip,Block Storage: make the iscsi target addres configurable for discovery.,https://issues.apache.org/jira/browse/HDFS-13018,https://github.com/apache/hadoop/commit/fb09d7537961935f3ed2506dca1d6e2cd2829a80,Ip address,env setting,concrete Environment: Kubernetes,uxepected result: won't work,For example in kubernetes the iscsi server could run behind a service where the address (where the jscsi server is available from the cluster) could be different from the targetAddress where the server is listening.,/,yes
HDFS-13018,dfs.cblock.iscsi.advertised.port,Block Storage: make the iscsi target addres configurable for discovery.,https://issues.apache.org/jira/browse/HDFS-13018,https://github.com/apache/hadoop/commit/fb09d7537961935f3ed2506dca1d6e2cd2829a80,port,env setting,concrete Environment: Kubernetes,uxepected result: won't work,For example in kubernetes the iscsi server could run behind a service where the address (where the jscsi server is available from the cluster) could be different from the targetAddress where the server is listening.,/,yes
HBASE-21126,permittedZookeeperFailures,Add ability for HBase Canary to ignore a configurable number of ZooKeeper down nodes,https://issues.apache.org/jira/browse/HBASE-21126,https://github.com/apache/hbase/commit/5cca61c4d0e7f1cf03fb7a80c7611bde4a49214b,count,manageability,concrete task: gauge server health,/,"If we use the Canary to gauge server health, and alert accordingly, this can be too strict. For example, in a 5-node ZooKeeper cluster, having one node down is safe and expected in rolling upgrades/patches.",/,no
HBASE-20004,hbase.rest.http.allow.options.method,Client is not able to execute REST queries in a secure cluster,https://issues.apache.org/jira/browse/HBASE-20004,https://github.com/apache/hbase/commit/c60578d9829f29cf77b250d238a9e38dc0b513d7#,feature selection,manageability,concrete task: In secure mode,/,But the problem here was the browser client negotiate with server by first sending a HTTP request with method type as OPTIONS and this method we have added as a constraint in the security handler.,"Balancing flexibility and simplicity: I added that first then removed, thinking that it works as it is and adding one more new configuration which I felt whose value will not be changed from its default, made no sense to me.",no
SPARK-23765,lineSep,[SQL] Supports custom line separator for json datasource,https://issues.apache.org/jira/browse/SPARK-23765,https://github.com/apache/spark/commit/34c4b9c57e114cdb390e4dbc7383284d82fea317,regex,manageability,unknow,/,"Spark csv and text readers always use default CR, LF or CRLF line terminators without an option to configure a custom delimiter.",/,no
SPARK-29175,spark.sql.additionalRemoteRepositories,Make maven central repository in IsolatedClientLoader configurable,https://issues.apache.org/jira/browse/SPARK-29175,https://github.com/apache/spark/commit/ada3ad34c6ab7f3d83ee9dd0b5e945e8ae50def,,manageability,vague,/,it can be used as the additional remote repositories for the default maven central repo.,/,no
SPARK-22366,spark.sql.files.ignoreMissingFiles,Support ignoring missing files,https://issues.apache.org/jira/browse/SPARK-22366,https://github.com/apache/spark/commit/8e9863531bebbd4d83eafcbc2b359b8bd0ac5734#,feature selection,manageability,vague usecase: some replication scenarios,/,"There's an existing flag ""spark.sql.files.ignoreCorruptFiles"" that will quietly ignore attempted reads from files that have been corrupted, but it still allows the query to fail on missing files. Being able to ignore missing files too is useful in some replication scenarios.",/,no
HDFS-12357,dfs.namenode.inode.attributes.provider.bypass.users,Let NameNode to bypass external attribute provider for configured users.,https://issues.apache.org/jira/browse/HDFS-12357,https://github.com/apache/hadoop/commit/d77ed238a911fc85d6f4bbce606cac7ec44f557f,name list,manageability,concrete Task: do distcp from one cluster to another,uxepected result:  avoid saving metadata,"We want to avoid saving metadata from external provider to HDFS, so we want to bypass external provider when doing the distcp (or hadoop fs -cp) operation",/,no
HBASE-18387,serverPrincipal,"Thrift] Make principal CONFIGurable in DemoClient.javaAdded OPTIONal (fourth) parameter ""server-principal""",https://issues.apache.org/jira/browse/HBASE-18387,https://github.com/apache/hbase/commit/8da77b414657f0ee3b093913de5f92eba17ecd2a,principle,manageability,concrete Task :using hbase-thrift to separate the names from those of backend servers,uxepected result: won't work,"This will only work when the Thrift server is started with the hbase principal. Often this may deviate, for example I am using hbase-thrift to separate the names from those of backend servers. What we need is either an additional command line option to specify the name, or a property that can be set with -D and can be passed at runtime. I prefer the former, as the latter is making this a little convoluted.",Balancing flexibility and simplicity: as the latter is making this a little convoluted.,no
HBASE-18994,hbase.systemtables.compacting.memstore.type,Decide if META/System tables should use Compacting Memstore or Default Memstore,https://issues.apache.org/jira/browse/HBASE-18994,https://github.com/apache/hbase/commit/0acfba0e35933d1008c9ade721055dafc137b50f#,feature selection,manageability,vague use case: bug,uxepected result:the comparators passed to the CellChunkImmutableSegment and CellArrayImmutableSegment are sometimes wrong,"That simplifies the issue but it doesn't bring any reason. Perhaps someone will raise another issue about ""all system tables should be assigned to compacting memstore"". As i see it, a workaround is to make it configurable, and the default memstore is the default setting.",/,no
SPARK-27963,spark.dynamicAllocation.shuffleTracking.enabled,Allow dynamic allocation without an external shuffle service,https://issues.apache.org/jira/browse/SPARK-27963,https://github.com/apache/spark/commit/2ddeff97d7329942a98ef363991eeabc3fa71a7,feature selection,manageability,"concrete task: One immediate use case is the ability to use dynamic allocation on Kubernetes, which doesn't yet have that service.",/,"It would be useful for users to be able to enable dynamic allocation without the need to provision an external shuffle service. One immediate use case is the ability to use dynamic allocation on Kubernetes, which doesn't yet have that service.",/,yes
HDFS-13418,dfs.net.topology.impl,NetworkTopology should be configurable when enable DFSNetworkTopology,https://issues.apache.org/jira/browse/HDFS-13418,https://github.com/apache/hadoop/commit/0725953efec89b35b7586b846abb01f7c5963b37,topology implementation,manageability,unknow,/,I think we should still make the NetworkTopology configurable rather than hard code the implementation since we may need another NetworkTopology impl.,/,yes
HDFS-13634,dfs.federation.router.connection.creator.queue-size,RBF: Configurable value in xml for async connection request queue size.,https://issues.apache.org/jira/browse/HDFS-13634,https://github.com/apache/hadoop/commit/a0ebb6b39f2932d3ea2fb5e287f52b841e108428,size,manageability,unknow,/,"This a very critical parameter for routers, admins would like to change this without doing a new build.",/,yes
HDFS-14113,dfs.namenode.ec.userdefined.policy.allowed,EC : Add Configuration to restrict UserDefined Policies,https://issues.apache.org/jira/browse/HDFS-14113,https://github.com/apache/hadoop/commit/c03024a5305bea1a40c87a4abc0793802bea5c75,feature selection,manageability,unknow,/,By default addition of erasure coding policies is enabled for users.We need to add configuration whether to allow addition of new User Defined policies or not.Which can be configured in for of a Boolean value at the server side.,/,yes
HDFS-12106,dfs.storage.policy.satisfier.retry.max.attempts,Improve storage policy satisfier configuration,https://issues.apache.org/jira/browse/HDFS-12106,https://github.com/apache/hadoop/commit/c561cb316e365ef674784cd6cf0b12c0fbc271a3,threshold,manageability,vague Environment: not enough storage,/,"if file is not able to satisfy for some reasons like not enough storages etc, we may keep retry",/,yes
HDFS-9049,dfs.datanode.http.internal-proxy.port,Make Datanode Netty reverse proxy port to be configurable,https://issues.apache.org/jira/browse/HDFS-9049,https://github.com/apache/hadoop/commit/09efdfe9e13c9695867ce4034aa6ec970c2032f1,port,manageability,vague Task : control port,/,"In some of the deployments, customers want to have the control over all listening ports. So that they logically separates server/listening ports from client ports. For this they prefer no random ports to be allocated for listening.",/,yes
CASSANDRA-13959,enable_materialized_views,Add flag to disable materialized views and warnings on creation,https://issues.apache.org/jira/browse/CASSANDRA-13959,https://github.com/apache/cassandra/commit/b8697441d7a051e7ff68def6aa9cf14bd92ace9e,feature selection,manageability,vague task: for developing task,/,Materialized views are considered experimental and are not recommended for production use.,/,yes
SPARK-29603,spark.yarn.priority,Support application priority for YARN priority scheduling,https://issues.apache.org/jira/browse/SPARK-29603,https://github.com/apache/spark/commit/4615769736f4c052ae1a2de26e715e229154cd2,priority,manageability,"vague usecase: can set priority to an application for YARN to define pending applications ordering policy, those with higher priority have a better opportunity to be activated.",/,"We can set priority to an application for YARN to define pending applications ordering policy, those with higher priority have a better opportunity to be activated. YARN CapacityScheduler only.",/,yes
SPARK-29436,spark.kubernetes.executor.scheduler.name,Support executor for selecting scheduler through scheduler name in the case of k8s multi-scheduler scenario,https://issues.apache.org/jira/browse/SPARK-29436,https://github.com/apache/spark/commit/f800fa383131559c4e841bf062c9775d0919093,name,manageability,vague usecase: support executor for selecting scheduler through scheduler name.,/,"In the case of k8s multi-scheduler, support executor for selecting scheduler through scheduler name.",/,yes
SPARK-29444,spark.sql.jsonGenerator.struct.ignore.null,Add CONFIGURATION to support JacksonGenrator to keep fields with null values,https://issues.apache.org/jira/browse/SPARK-29444,https://github.com/apache/spark/commit/78b0cbe265c4e8cc3d4d8bf5d734f2998c04d37,feature selection,manageability,"concrete task: it is better to keep null data in some scenarios. Such as sparkmagic, which is widely used in jupyter with livy, we use toJSON to toJSON is used to get sql results. with toJSON sparkmagic may return empty results, which confused users.",Failure: may cause parsing exceptions or loss of entire column data.,"DataSet.toJSON will lost some column when field data is null. Maybe it is better to keep null data in some scenarios. Such as sparkmagic, which is widely used in jupyter with livy, we use toJSON to toJSON is used to get sql results. with toJSON sparkmagic may return empty results, which confused users.Maybe adding a config is the best choice. This configuration retains the current semantics and will remain fields with null until the configuration is modified.",/,yes
SPARK-24690,spark.sql.cbo.planStats.enabled,Add a CONFIG to control plan stats computation in LogicalRelation,https://issues.apache.org/jira/browse/SPARK-24690,https://github.com/apache/spark/commit/3f3a18fff116a02ff7996d45a1061f48a2de310,feature selection,manageability,vague: if CBO disabled,uxepected result: cannot currently enable StarSchemaDetection.reorderStarJoins,We cannot currently enable StarSchemaDetection.reorderStarJoins because we need to turn off CBO to enable it but StarSchemaDetection internally references LogicalPlan.stats.rowCount. This rowCount is used in LogicalRelation if CBO disabled.This ticket is to propose a new separate config so that LogicalRelation can use rowCount to compute data statistics.,/,yes
HDFS-13857,dfs.federation.router.default.nameservice.enable,RBF: Choose to enable the default nameservice to read/write files,https://issues.apache.org/jira/browse/HDFS-13857,https://github.com/apache/hadoop/commit/54f2044595206455484284b43e5976c8a1982aaf,feature selection,manageability,concrete Task: we need all files to be written in the location from the MountTableEntry,uxepected result: files written in unknown location,"as cluster administrator, It is not better to happen some files are written in some unknown location.",Balancing flexibility and simplicity: as cluster administrator,yes
SPARK-20950,spark.shuffle.spill.diskWriteBufferSize,add a new config to diskWriteBufferSize which is hard coded before,https://issues.apache.org/jira/browse/SPARK-20950,https://github.com/apache/spark/commit/565e7a8d4ae7879ee704fb94ae9b3da31e202d7e#,size,performance,concrete performance comparison,/,With spark.shuffle.spill.diskWriteBufferSize configure diskWriteBufferSize of ShuffleExternalSorter. when change the size of the diskWriteBufferSize to test `forceSorterToSpill` The average performance of running 10 times is as follows,Balancing flexibility and simplicity: What's the use case for exposing these as more config options? what would cause the user to need to set these separately? ,no
HBASE-15982,hbase.replication.wait.on.endpoint.seconds,Interface ReplicationEndpoint extends Guava's Service,https://issues.apache.org/jira/browse/HBASE-15982,https://github.com/apache/hbase/commit/6e7baa07f0b1f5841379545acaf23d36f50de2c2,timeout,performance,unknow,/,unknow,/,no
HDFS-14114,DFS_ROUTER_NAMENODE_CONNECTION_MIN_ACTIVE_RATIO (dfs.federation.router.connection.min-active-ratio),RBF: MIN_ACTIVE_RATIO should be configurable,https://issues.apache.org/jira/browse/HDFS-14114,https://github.com/apache/hadoop/commit/6aa7aabff801905f55a18d117f927d0125057d78,threshold,performance,unknow,/,It affects cleanup and creating Connections. Maybe it should be configurable so that we can reconfig it to improve performance,/,no
SPARK-20101,spark.sql.columnVector.offheap.enable,"[SQL] Use OffHeapColumnVector when ""spark.sql.columnVector.offheap.enable"" is set to ""true""",https://issues.apache.org/jira/browse/SPARK-20101,https://github.com/apache/spark/commit/3c3eebc8734e36e61f4627e2c517fbbe342b3b42,feature selection,performance,unknow,/,"While ColumnVector has two implementations OnHeapColumnVector and OffHeapColumnVector, only OnHeapColumnVector is used.",/,no
SPARK-22160,spark.sql.execution.rangeExchange.sampleSizePerPartition,[SQL] Make sample points per partition (in range partitioner) CONFIGurable and bump the default value up to 100,https://issues.apache.org/jira/browse/SPARK-22160,https://github.com/apache/spark/commit/323806e68f91f3c7521327186a37ddd1436267d0,size,performance,unknow,/,"Spark's RangePartitioner hard codes the number of sampling points per partition to be 20. This is sometimes too low. This ticket makes it configurable, via spark.sql.execution.rangeExchange.sampleSizePerPartition, and raises the default in Spark SQL to be 100.",/,no
SPARK-23188,spark.sql.parquet.columnarReaderBatchSize,[SQL] Make vectorized columar reader batch size CONFIGurable,https://issues.apache.org/jira/browse/SPARK-23188,https://github.com/apache/spark/commit/cc41245fa3f954f961541bf4b4275c28473042b8,size,performance,unknow,/,Make the capacity of `VectorizedParquetRecordReader` configurable,/,no
SPARK-23188,spark.sql.orc.columnarReaderBatchSize,[SQL] Make vectorized columar reader batch size CONFIGurable,https://issues.apache.org/jira/browse/SPARK-23188,https://github.com/apache/spark/commit/cc41245fa3f954f961541bf4b4275c28473042b8#,size,performance,unknow,/,Make the capacity of `OrcColumnarBatchReader` configurable,/,no
SPARK-28355,spark.broadcast.UDFCompressionThreshold,Use Spark conf for threshold at which UDF is compressed by broadcast,https://issues.apache.org/jira/browse/SPARK-28355,https://github.com/apache/spark/commit/79e204770300dab4a669b9f8e2421ef905236e7,threshold,performance,unknow,/,The _prepare_for_python_RDD method currently broadcasts a pickled command if its length is greater than the hardcoded value 1 << 20 (1M). We would like to set this value as a Spark conf instead.,/,no
HBASE-18646,hbase.backup.logroll.pool.thread.number,LogRollMasterProcedureManager: make procedure timeout thread pool size configurable,https://issues.apache.org/jira/browse/HBASE-18646,https://github.com/apache/hbase/commit/19bb4ef487407cdf59a3f3c5bbef6dd8b917b682,thread/task number,performance,vague scale: large deployment,/,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployments,/,no
HBASE-18646,hbase.backup.logroll.timeout.millis ,LogRollMasterProcedureManager: make procedure timeout thread pool size configurable,https://issues.apache.org/jira/browse/HBASE-18646,https://github.com/apache/hbase/commit/19bb4ef487407cdf59a3f3c5bbef6dd8b917b682,timeout,performance,vague scale: large deployment,/,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployments,/,no
HBASE-21287,hbase.master.init.timeout.localHBaseCluster,Allow configuring test master initialization wait time.,https://issues.apache.org/jira/browse/HBASE-21287,https://github.com/apache/hbase/commit/db9a5b7da76d0876c9515720a8a60acb299ab217,timeout,performance,vague Task: some experiment,/,"We can configure how long the local cluster threads will wait for master to come up and become active, but not how long we allow initialization to take. Being able to tune this would improve my test loop on some experiment I am running.",/,no
HBASE-15519,hbase.regionserver.user.metrics.enabled,Add per-user metrics with lossy counting,https://issues.apache.org/jira/browse/HBASE-15519,https://github.com/apache/hbase/commit/59fc163e942c0981376478f5b3fe4ff14ae998d,feature selection,performance,vague workload: multi-tenant cases,/,"to disable user metrics in case it accounts for any performance issues, Per-user metrics will be useful in multi-tenant cases where we can emit number of requests, operations, num RPCs etc at the per-user aggregate level per regionserver. We currently have throttles per user, but no way to monitor resource usage per-user.",/,no
HBASE-18533,hbase.bucketcache.single.factor,Expose BucketCache values to be CONFIGuredBefore this commit BucketCache always used the default values.This commit adds the ability to CONFIGure these values,https://issues.apache.org/jira/browse/HBASE-18533,https://github.com/apache/hbase/commit/0e32869f01697abf29292aa786d0cdcca10213c6,factor,performance,"vague workload: single access, multi access, and in-memory access.",/,"BucketCache always uses the default values for all cache configuration. However, this doesn't work for all use cases. In particular, users want to be able to configure the percentage of the cache that is single access, multi access, and in-memory access.",/,no
HBASE-18533,hbase.bucketcache.multi.factor,Expose BucketCache values to be CONFIGuredBefore this commit BucketCache always used the default values.This commit adds the ability to CONFIGure these values,https://issues.apache.org/jira/browse/HBASE-18533,https://github.com/apache/hbase/commit/0e32869f01697abf29292aa786d0cdcca10213c6,factor,performance,"vague workload: single access, multi access, and in-memory access.",/,"BucketCache always uses the default values for all cache configuration. However, this doesn't work for all use cases. In particular, users want to be able to configure the percentage of the cache that is single access, multi access, and in-memory access.",/,no
HBASE-18533,hbase.bucketcache.memory.factor,Expose BucketCache values to be CONFIGuredBefore this commit BucketCache always used the default values.This commit adds the ability to CONFIGure these values,https://issues.apache.org/jira/browse/HBASE-18533,https://github.com/apache/hbase/commit/0e32869f01697abf29292aa786d0cdcca10213c6,factor,performance,"vague workload: single access, multi access, and in-memory access.",/,"BucketCache always uses the default values for all cache configuration. However, this doesn't work for all use cases. In particular, users want to be able to configure the percentage of the cache that is single access, multi access, and in-memory access.",/,no
HBASE-18533,hbase.bucketcache.extrafreefactor,Expose BucketCache values to be CONFIGuredBefore this commit BucketCache always used the default values.This commit adds the ability to CONFIGure these values,https://issues.apache.org/jira/browse/HBASE-18533,https://github.com/apache/hbase/commit/0e32869f01697abf29292aa786d0cdcca10213c6,factor,performance,"vague workload: single access, multi access, and in-memory access.",/,"BucketCache always uses the default values for all cache configuration. However, this doesn't work for all use cases. In particular, users want to be able to configure the percentage of the cache that is single access, multi access, and in-memory access.",/,no
HBASE-18533,hbase.bucketcache.minfactor,Expose BucketCache values to be CONFIGuredBefore this commit BucketCache always used the default values.This commit adds the ability to CONFIGure these values,https://issues.apache.org/jira/browse/HBASE-18533,https://github.com/apache/hbase/commit/0e32869f01697abf29292aa786d0cdcca10213c6,factor,performance,"vague workload: single access, multi access, and in-memory access.",/,"BucketCache always uses the default values for all cache configuration. However, this doesn't work for all use cases. In particular, users want to be able to configure the percentage of the cache that is single access, multi access, and in-memory access.",/,no
HBASE-18533,hbase.bucketcache.acceptfactor,Expose BucketCache values to be CONFIGuredBefore this commit BucketCache always used the default values.This commit adds the ability to CONFIGure these values,https://issues.apache.org/jira/browse/HBASE-18533,https://github.com/apache/hbase/commit/0e32869f01697abf29292aa786d0cdcca10213c6,factor,performance,"vague workload: single access, multi access, and in-memory access.",/,"BucketCache always uses the default values for all cache configuration. However, this doesn't work for all use cases. In particular, users want to be able to configure the percentage of the cache that is single access, multi access, and in-memory access.",/,no
HBASE-19024,hbase.wal.hsync,Provide a CONFIGurable param to hsync WAL edits to the disk for better durability,https://issues.apache.org/jira/browse/HBASE-19024,https://github.com/apache/hbase/commit/17ac2fe9c13d2b567c6bc06690e1491dfba83090,feature selection,performance,concrete performance comparison,perf: 10-15% latency impact,At present we do not have an option to hsync WAL edits to the disk for better durability. In our local tests we see 10-15% latency impact of using hsync instead of hflush which is not very high.,/,no
SPARK-23312,spark.sql.inMemoryColumnarStorage.enableVectorizedReader,[SQL] add a CONFIG to turn off vectorized cache reader,https://issues.apache.org/jira/browse/SPARK-23312,https://github.com/apache/spark/commit/b9503fcbb3f4a3ce263164d1f11a8e99b9ca5710,feature selection,performance,concrete workload: 10.4GB input from hive orc files /18.8 GB cached/5592 partitions,perf: a performance regression,I was testing spark 2.3 rc2 and I am seeing a performance regression in sql queries on cached data.,/,no
CASSANDRA-12526,cfs.getMaximumCompactionThreshold,Bump SSTable level instead of rewriting SSTable completely during single-sstable compactions,https://issues.apache.org/jira/browse/CASSANDRA-12526,https://github.com/apache/cassandra/commit/53c0ef171424454c47d64a9326b0ba83cd743a50,threshold,performance,concrete workload: a ton of newly inserted partitions that will never merge in compactions,perf: affect the overall compaction throughput,a ton of newly inserted partitions that will never merge in compactions,Balancing flexibility and simplicity: which is probably the worst kind of workload for LCS,no
HBASE-21764,hbase.regionserver.inmemory.compaction.pool.size,Size of in-memory compaction thread pool should be configurable,https://issues.apache.org/jira/browse/HBASE-21764,https://github.com/apache/hbase/commit/81ebe6d40d485a3aee4451acc340cf6db62609b7,thread/task number,performance,concrete workload: many regions in one region server,perf: better read performance,"introduced an new config key in this issue: hbase.regionserver.inmemory.compaction.pool.size. the default value would be 10. you can configure this to set the pool size of in-memory compaction pool. Note that all memstores in one region server will share the same pool, so if you have many regions in one region server, you need to set this larger to compact faster for better read performance.",/,no
HBASE-18478,hbase.master.balancer.uselocality,Allow users to remove RegionFinder from LoadBalancer calculations if no locality possible,https://issues.apache.org/jira/browse/HBASE-18478,https://github.com/apache/hbase/commit/8301734594011e7f94f992e6f8c815580a6b62d8,feature selection,performance,concrete Environment: FileSystems that do not surface locality ,perf: cluster start time reduction,BaseLoadBalancer should have the option to remove RegionFinder(quickly lookup cloud and region infomation of application deployment) from load balancing. This provides significant cluster start time reduction for FileSystems that do not surface locality such as Amazon S3.,/,no
SPARK-24519,spark.shuffle.minNumPartitionsToHighlyCompress,Make the threshold for highly compressed map status configurable,https://issues.apache.org/jira/browse/SPARK-24519,https://github.com/apache/spark/commit/39dfaf2fd167cafc84ec9cc637c114ed54a331e3,threshold,performance,vague task: easily tune their jobs,"perf: could be increasing # of output files when you don't want to be, affects the # of tasks needs and thus executors to run in parallel, etc.",MapStatus uses hardcoded value of 2000 partitions to determine if it should use highly compressed map status. We should make it configurable to allow users to more easily tune their jobs with respect to this without having for them to modify their code to change the number of partitions. Note we can leave this as an internal/undocumented config for now until we have more advise for the users on how to set this config.,Balancing flexibility and simplicity: Note we can leave this as an internal/undocumented config for now until we have more advise for the users on how to set this config.,no
CASSANDRA-13418,unsafe_aggressive_sstable_expiration,Allow TWCS to ignore overlaps when dropping fully expired sstables,https://issues.apache.org/jira/browse/CASSANDRA-13418,https://github.com/apache/cassandra/commit/14d67d81c57d6387c77bd85c57b342d285880835,feature selection,performance,vague task: read-repairs,perf: CPU intensive and time consuming,"you really want read-repairs, looking for fully expired ones",/,no
SPARK-28595,spark.sql.legacy.bucketedTableScan.outputOrdering,explain should not trigger partition listing,https://issues.apache.org/jira/browse/SPARK-28595,https://github.com/apache/spark/commit/469423f33887a966aaa33eb75f5e7974a0a97be,feature selection,performance,concrete Workload: a) all the bucket columns are read. b) there is only one file in each bucket. ,perf: get stuck for a while,"Sometimes when you explain a query, you will get stuck for a while. What's worse, you will get stuck again if you explain again. The outputOrdering is not very useful. We can only apply it if a) all the bucket columns are read. b) there is only one file in each bucket. This condition is really hard to meet, and even if we meet, sorting an already sorted file is pretty fast and avoiding the sort is not that useful. I think it's worth to give up this optimization so that explain don't need to get stuck.",/,no
SPARK-25850,spark.sql.codegen.methodSplitThreshold,Make the split threshold for the code generated method configurable,https://issues.apache.org/jira/browse/SPARK-25850,https://github.com/apache/spark/commit/e017cb39642a5039abd8ce8127ad41712901bdbc,threshold,performance,concrete performance comparison,perf: have a big performance impact for the client operations,"When the generated Java function source code exceeds `spark.sql.codegen.methodSplitThreshold`, it will be split into multiple small functions. This is useful for performance tuning","Balancing flexibility and simplicity:(test), but fianlly become general because ""It should be useful for performance tuning. - from a Contributor""",no
HBASE-23223,hbase.bucketcache.offsetlock.usestrongref,Support the offsetLock of bucketCache to use strong ref,https://issues.apache.org/jira/browse/HBASE-23223,https://github.com/apache/hbase/commit/4ea792246fbbc5adf9f4cede38c0513e2669b45,feature selection,performance,vague workload: mixed gc occurs,perf: lead to long puase,"The offsetLock of bucketCache use soft reference based impl by default,it could reclaim mem when not enough,but a lot of soft reference object would lead to long puase when mixed gc occurs,so if the count of offset is limited and you have enough mem(most of the time it is true),then the strong reference is better choice.",/,no
SPARK-17310,spark.sql.parquet.recordLevelFilter.enabled,[SQL] Add an option to disable record-level filter in Parquet-side,https://issues.apache.org/jira/browse/SPARK-17310,https://github.com/apache/spark/commit/673c67046598d33b9ecf864024ca7a937c1998d6,feature selection,performance,concrete performance comparison,perf: much faster,It seems Spark-side codegen row-by-row filtering might be faster than Parquet's one in general due to type-boxing and virtual function calls which Spark's one tries to avoid. Maybe we should perform a benchmark and disable this.,/,no
SPARK-24978,spark.sql.codegen.aggregate.fastHashMap.capacityBit,Add spark.sql.fast.hash.aggregate.row.max.capacity to configure the capacity of fast aggregation.,https://issues.apache.org/jira/browse/SPARK-24978,https://github.com/apache/spark/commit/6193a202aab0271b4532ee4b740318290f2c44a1,count,performance,concrete performance comparison,perf: nontrivial overheads,"give performance comparison, this pr add a configuration parameter to configure the capacity of fast aggregation.",/,no
CASSANDRA-14654,key_cache_migrate_during_compaction,Reduce heap pressure during compactions,https://issues.apache.org/jira/browse/CASSANDRA-14654,https://github.com/apache/cassandra/commit/7df67eff2d66dba4bed2b4f6aeabf05144d9b057,feature selection,performance,concrete workload: Small partition compactions(KeyCacheKey dominating heap pressure),perf: painfully slow,Small partition compactions,/,no
SPARK-27870,spark.sql.pandas.udf.buffer.size,Add a runtime buffer size CONFIGURation for Pandas UDFs,https://issues.apache.org/jira/browse/SPARK-27870,https://github.com/apache/spark/commit/26998b86c13e79582a3df31f6184f825cde45e73#,size,performance,vague scale: when multiple pandas UDF plans are pipelined.,"perf: the donwstream UDF's pipeline will lag behind too much, which may increase the total processing time.",This could improve performance when multiple pandas UDF plans are pipelined.,"Balancing flexibility and simplicity:Since these don't occur during normal operation, we can use rather lengthy timeouts, and don't need to make them configurable",no
HBASE-15482,hbase.TableSnapshotInputFormat.locality.enabled,Provide an option to skip calculating block locations for SnapshotInputFormat,https://issues.apache.org/jira/browse/HBASE-15482,https://github.com/apache/hbase/commit/5e7d16a3ceaeec5057474f9bae2d40d306f6dd8e#,feature selection,performance,vague workload: large snapshot,perf: this process may take a long time,"When a MR job is reading from SnapshotInputFormat, it needs to calculate the splits based on the block locations in order to get best locality. However, this process may take a long time for large snapshots.",/,no
HBASE-18083,hbase.regionserver.hfilecleaner.large.thread.count,Make large/small file clean thread number configurable in HFileCleaner,https://issues.apache.org/jira/browse/HBASE-18083,https://github.com/apache/hbase/commit/4fe73857679ecba89a7edd3c17d9f92e4c0e2164,thread/task number,performance,concrete workload: 1.8PB write pressure,perf: time consuming,"Currently we have only one thread for both large and small file cleaning, but when write pressure is huge we might need more cleaner threads, so we need to make the thread number configurable",/,no
HBASE-18083,hbase.regionserver.hfilecleaner.small.thread.count,Make large/small file clean thread number configurable in HFileCleaner,https://issues.apache.org/jira/browse/HBASE-18083,https://github.com/apache/hbase/commit/4fe73857679ecba89a7edd3c17d9f92e4c0e2164,thread/task number,performance,concrete workload: 1.8PB write pressure,perf: time consuming,"Currently we have only one thread for both large and small file cleaning, but when write pressure is huge we might need more cleaner threads, so we need to make the thread number configurable",/,no
CASSANDRA-12526,single_sstable_uplevel,Bump SSTable level instead of rewriting SSTable completely during single-sstable compactions,https://issues.apache.org/jira/browse/CASSANDRA-12526,https://github.com/apache/cassandra/commit/53c0ef171424454c47d64a9326b0ba83cd743a50,feature selection,performance,concrete Workload:a ton of newly inserted partitions that will never merge in compactions,perf: time consuming,a ton of newly inserted partitions that will never merge in compactions,Balancing flexibility and simplicity:which is probably the worst kind of workload for LCS,no
SPARK-24727,spark.sql.codegen.cache.maxEntries,[SQL] Add a static CONFIG to control cache size for generated classes,https://issues.apache.org/jira/browse/SPARK-24727,https://github.com/apache/spark/commit/b2deef64f604ddd9502a31105ed47cb63470ec85,threshold,performance,vague Task:realtime streaming calculation,perf: time consuming,"The cache 100 in CodeGenerator is too small for realtime streaming calculation, although is ok for offline calculation. Because realtime streaming calculation is mostly more complex in one driver, and performance sensitive.",/,no
HBASE-20401,hbase.regionserver.hfilecleaner.thread.check.interval.msec,Make MAX_WAIT and waitIfNotFinished in CleanerContext configurable,https://issues.apache.org/jira/browse/HBASE-20401,https://github.com/apache/hbase/commit/4613f3e8c740e881505fcd2c661a0e28462eb459,interval,performance,vague Environment: high latency storage,perf: wait too long,"1. While deleting a oldwal never complete (strange but possible), then delete file task needs to wait for a max of 60 seconds. Here, 60 seconds might be too long, or the opposite way is to increase more than 60 seconds in the use cases of slow file delete. 2. The check and wait of a file delete is set to default in the period of 500 milliseconds, one might want to tune this checking period to a short interval to check more frequently or to a longer interval to avoid checking too often to manage their delete file task checking period (the longer interval may be use to avoid checking too fast while using a high latency storage).",/,no
HBASE-20401,hbase.oldwals.cleaner.thread.check.interval.msec,Make MAX_WAIT and waitIfNotFinished in CleanerContext configurable,https://issues.apache.org/jira/browse/HBASE-20401,https://github.com/apache/hbase/commit/4613f3e8c740e881505fcd2c661a0e28462eb459,interval,performance,vague Environment: high latency storage,perf: wait too long,"1. While deleting a oldwal never complete (strange but possible), then delete file task needs to wait for a max of 60 seconds. Here, 60 seconds might be too long, or the opposite way is to increase more than 60 seconds in the use cases of slow file delete. 2. The check and wait of a file delete is set to default in the period of 500 milliseconds, one might want to tune this checking period to a short interval to check more frequently or to a longer interval to avoid checking too often to manage their delete file task checking period (the longer interval may be use to avoid checking too fast while using a high latency storage).",/,no
HBASE-20401,hbase.regionserver.hfilecleaner.thread.timeout.msec,Make MAX_WAIT and waitIfNotFinished in CleanerContext configurable,https://issues.apache.org/jira/browse/HBASE-20401,https://github.com/apache/hbase/commit/4613f3e8c740e881505fcd2c661a0e28462eb459,timeout,performance,vague workload: file delete,perf: wait too long,"1. While deleting a oldwal never complete (strange but possible), then delete file task needs to wait for a max of 60 seconds. Here, 60 seconds might be too long, or the opposite way is to increase more than 60 seconds in the use cases of slow file delete. 2. The check and wait of a file delete is set to default in the period of 500 milliseconds, one might want to tune this checking period to a short interval to check more frequently or to a longer interval to avoid checking too often to manage their delete file task checking period (the longer interval may be use to avoid checking too fast while using a high latency storage).",/,no
SPARK-22233,spark.files.ignoreEmptySplits,[CORE] Allow user to filter out empty split in HadoopRDD,https://issues.apache.org/jira/browse/SPARK-22233,https://github.com/apache/spark/commit/014dc8471200518d63005eed531777d30d8a6639,feature selection,performance,concrete workload: Hive will create an empty table with many empty files,Resouce overuse: Spark will spends much more resources,"Sometimes, Hive will create an empty table with many empty files, Spark use the InputFormat stored in Hive Meta Store and will not combine the empty files and therefore generate many tasks to handle this empty files. Hive use CombineHiveInputFormat(hive.input.format) by default. So, in this case, Spark will spends much more resources than hive.",/,no
SPARK-21923,spark.storage.unrollMemoryGrowthFactor,[CORE] Avoid calling reserveUnrollMemoryForThisTask for every record,https://issues.apache.org/jira/browse/SPARK-21923,https://github.com/apache/spark/commit/a11db942aaf4c470a85f8a1b180f034f7a584254,factor,performance,concrete Task: When Spark persist data to Unsafe memory,resource overuse: cause unnecessary synchronize,"When Spark persist data to Unsafe memory, we call the method `MemoryStore.putIteratorAsBytes`, which need synchronize the `memoryManager` for every record write. This implementation is not necessary, we can apply for more memory at a time to reduce unnecessary synchronization.",/,no
SPARK-21923,spark.storage.unrollMemoryCheckPeriod,[CORE] Avoid calling reserveUnrollMemoryForThisTask for every record,https://issues.apache.org/jira/browse/SPARK-21923,https://github.com/apache/spark/commit/a11db942aaf4c470a85f8a1b180f034f7a584254,interval,performance,concrete Task: When Spark persist data to Unsafe memory,resource overuse: cause unnecessary synchronize,"When Spark persist data to Unsafe memory, we call the method `MemoryStore.putIteratorAsBytes`, which need synchronize the `memoryManager` for every record write. This implementation is not necessary, we can apply for more memory at a time to reduce unnecessary synchronization.",/,no
HBASE-21582,hbase.snapshot.sentinels.cleanup.timeoutMillis,If call HBaseAdmin#snapshotAsync but forget call isSnapshotFinished then SnapshotHFileCleaner will skip to run every time,https://issues.apache.org/jira/browse/HBASE-21582,https://github.com/apache/hbase/commit/f32d2618430f70e1b0db92785294b2c7892cc02b,timeout,performance,concrete task: remove the SnapshotSentinel from snapshotHandlers in SnapshotManage,uxepected result: skip to run every time,"If call HBaseAdmin#snapshotAsync but forget call isSnapshotFinished, then SnapshotHFileCleaner will skip to run every time",/,no
HBASE-20449,hbase.normalizer.min.region.count,The minimun number of region should be CONFIGURable,https://issues.apache.org/jira/browse/HBASE-20449,https://github.com/apache/hbase/commit/80cbc0d1fefdba1492d7ec6e580ad54a2960cbdb,threshold,performance,unknow,/,the minimun number of region should be configurable in normalizable,/,yes
HDFS-12482,dfs.datanode.ec.reconstruction.xmits.weight,Provide a configuration to adjust the weight of EC recovery tasks to adjust the speed of recovery,https://issues.apache.org/jira/browse/HDFS-12482,https://github.com/apache/hadoop/commit/9367c25dbdfedf60cdbd65611281cf9c667829e6,weights,performance,unknow,/,we can add a coefficient for user to tune the weight of EC recovery tasks.,/,yes
SPARK-25233,spark.streaming.kafka.minRatePerPartition,Give the user the option of specifying a minimum message per partition per batch when using kafka direct API with backpressure,https://issues.apache.org/jira/browse/SPARK-25233,https://github.com/apache/spark/commit/135ff16a3510a4dfb3470904004dae9848005019,threshold,performance,unknow,/,It would be better if the user has the option of setting the minimum instead of just a hard coded 1 limit.,/,yes
SPARK-29939,spark.shuffle.mapStatus.compression.codec,Add spark.shuffle.mapStatus.compression.codec conf,https://issues.apache.org/jira/browse/SPARK-29939,https://github.com/apache/spark/commit/456cfe6e4693efd26d64f089d53c4e01bf8150a,feature selection,performance,unknow,/,"All the other compressions have conf. Could we do it for this too? (We already have this functionality for broadcast/rdd/shuffle/shuflleSpill, so it might be better to have the same functionality for MapStatus as well.)",/,yes
HDFS-14553,dfs.namenode.blockreport.queue.size,Make queue size of BlockReportProcessingThread configurable,https://issues.apache.org/jira/browse/HDFS-14553,https://github.com/apache/hadoop/commit/bd46bdf9f9244f3f3474d316255ac98717ed5719,size,performance,vague usecase: Block report queue is full,Failure: Block report queue is full,"Worth noting that we had a few experience with ""Block report queue is full"" and client would fail to close files. ",/,yes
HDFS-12947,dfs.namenode.snapshot.max.limit,Limit the number of Snapshots allowed to be created for a Snapshottable Directory,https://issues.apache.org/jira/browse/HDFS-12947,https://github.com/apache/hadoop/commit/4d1bf33d0fa7689a6f83a9c6e5adaac665c0d5d5,count,performance,vague workload: large number snapshot deletion,Failure: namenode crash,"A snapshot table directory is able to accommodate 65,536 snapshots. In case a directory has large no of snapshots , deletion of any of the earlier snapshots take a lot of time which might lead to namenode crash (HDFS-11225)",/,yes
HDFS-14655,dfs.qjournal.parallel-read.num-threads,[SBN Read] Namenode crashes if one of The JN is down.,https://issues.apache.org/jira/browse/HDFS-14655,https://github.com/apache/hadoop/commit/eb96a3093ea34a7749410a63c72b6d0a9636d80,count,performance,vague: one of The JN is down,Failure: Namenode crashes,"""I don't think we want a fixed thread pool"", ""Hey guys, discussed this with Chen. It seems that we need a pool of only 3 threads, which can be reused for each iteration of tailing.""",/,yes
CASSANDRA-14580,periodic_commitlog_sync_lag_block_in_ms,Make PeriodicCommitLogService.blockWhenSyncLagsNanos configurable,https://issues.apache.org/jira/browse/CASSANDRA-14580,https://github.com/apache/cassandra/commit/176d4bac22c356c80e275dcb4040bc5cbd0da1c2,interval,performance,vague workload:a lag in flushing to disc,perf: block for up to 15 seconds,"When using the default values for periodic commitlog, the sync is every ten seconds and if there's a lad in flushing to disc, we can block for up to 15 seconds (sync time * 1.5). However, if you lower the sync time to 1 second, for example, the block time is only 1.5 seconds (not acceptable in all situations). Admittedly this is only an expert-level setting, but useful in some cases.","Balancing flexibility and simplicity: Admittedly this is only an expert-level setting, but useful in some cases.",yes
SPARK-29654,spark.metrics.static.sources.enabled,Add CONFIGURATION to allow disabling registration of static sources to the metrics system,https://issues.apache.org/jira/browse/SPARK-29654,https://github.com/apache/spark/commit/2888009d6660183674de7da456b54247c1b8ca2,feature selection,performance,vague: in the cases when the metrics in question are not needed,perf: can produce a significant amount of data,"The Spark metrics system produces many different metrics and not all of them are used at the same time. This proposes to introduce a configuration parameter to allow disabling the registration of metrics in the ""static sources"" category",/,yes
SPARK-29189,spark.sql.sources.ignore.datalocality,Add an OPTION to ignore block locations when listing file,https://issues.apache.org/jira/browse/SPARK-29189,https://github.com/apache/spark/commit/64fe82b519bdc854fcbef40e906ac1fb181534c,feature selection,performance,"concrete Environment: in a PROD environment, a table can be so huge that even fetching all these location informations need take tens of seconds.",perf: data locality is never reachable.,"n our PROD env, we have a pure Spark cluster, I think this is also pretty common, where computation is separated from storage layer. In such deploy mode, data locality is never reachable. And there are some configurations in Spark scheduler to reduce waiting time for data locality(e.g. ""spark.locality.wait""). While, problem is that, in listing file phase, the location informations of all the files, with all the blocks inside each file, are all fetched from the distributed file system. Actually, in a PROD environment, a table can be so huge that even fetching all these location informations need take tens of seconds. To improve such scenario, Spark need provide an option, where data locality can be totally ignored, all we need in the listing file phase are the files locations, without any block location informations.",/,yes
HDFS-12528,dfs.domain.socket.disable.interval.seconds,Add an option to not disable short-circuit reads on failures,https://issues.apache.org/jira/browse/HDFS-12528,https://github.com/apache/hadoop/commit/2e7331ca264dd366b975f3c8e610cf84eb8cc155,interval,performance,concrete Task: data ingestion makes use of the -appendToFile operation to add new data to existing HDFS files,perf: have a big performance impact for the client operations,"It might also be interesting to be able to control how long SCR is disabled for in the ""unknown response"" case. 10 minutes seems a bit to long and not being able to change that is a problem.",/,yes
HDFS-13831,dfs.namenode.block.deletion.increment,Make block increment deletion number configurable,https://issues.apache.org/jira/browse/HDFS-13831,https://github.com/apache/hadoop/commit/b9b964d25335943fb15cdfcf369d123bbd7e454a,count,performance,vague workload: large directory deletion,perf: hold the write lock long time,"When NN deletes a large directory, it will hold the write lock long time. For improving this, we remove the blocks in a batch way. So that other waiters have a chance to get the lock. But right now, the batch number is a hard-coded value. We can make this value configurable, so that we can control the frequency of other waiters to get the lock chance.",/,yes
HDFS-13820,dfs.namenode.caching.enabled,Add an ability to disable CacheReplicationMonitor.,https://issues.apache.org/jira/browse/HDFS-13820,https://github.com/apache/hadoop/commit/335a8139f5b9004414b2942eeac5a008283a6f75,feature selection,performance,concrete performance comparison,perf: nontrivial overheads in processing,"we have seen from the FSNamesystem metrics added in HDFS-10872 that CacheReplicationMonitor can have nontrivial overheads even on a cluster with no caching enabled, and currently there is no way to disable it completely..",/,yes
HDFS-14858,dfs.namenode.state.context.enabled,[SBN read] Allow CONFIGURably enable/disable AlignmentContext on NameNode.,https://issues.apache.org/jira/browse/HDFS-14858,https://github.com/apache/hadoop/commit/1303255aee75e5109433f937592a890e8d274ce,feature selection,performance,concrete workload: Ran a few benchmarks (NNThroughputBenchmark) and profiler (VisualVM) today on an Observer-enabled cluster. Would like to share the results with the community. The cluster has 1 Observer node.,perf: One potential overhead of SBN read,"As brought up under HDFS-14277, we should make sure SBN read has no performance impact when it is not enabled. One potential overhead of SBN read is maintaining and updating additional state status on NameNode. Specifically, this is done by creating/updating/checking a GlobalStateIdContext instance. Currently, even without enabling SBN read, this logic is still be checked. We can make this configurable so that when SBN read is not enabled, there is no such overhead and everything works as-is.",/,yes
HDFS-14844,dfs.client.block.reader.remote.buffer.size,Make buffer of BlockReaderRemote#newBlockReader#BufferedOutputStream configurable.,https://issues.apache.org/jira/browse/HDFS-14844,https://github.com/apache/hadoop/commit/3f223bebfa6b382a762edcc518fcbae310ce22e,size,performance,"vague: It is recommended to adjust the value according to the workload, which can reduce unnecessary memory usage and the frequency of the garbage collection. A value of 512 might be reasonable.",perf: Reducing the size of the buffer will also reduce the frequency of gc and improve read performance.,/,/,yes
HDFS-13488,dfs.federation.router.client.reject.overload,RBF: Reject requests when a Router is overloaded.,https://issues.apache.org/jira/browse/HDFS-13488,https://github.com/apache/hadoop/commit/37269261d1232bc71708f30c76193188258ef4bd,feature selection,performance,vague environment: a slow subcluster,perf: router overloaded,A Router might be overloaded when handling special cases (e.g. a slow subcluster),/,yes
HDFS-12191,dfs.namenode.snapshot.skip.capture.accesstime-only-change,Provide option to not capture the accessTime change of a file to snapshot if no other modification has been done to this file,https://issues.apache.org/jira/browse/HDFS-12191,https://github.com/apache/hadoop/commit/cf93d60d3f032000e5b78a08d320793d78799f3d,feature selection,performance,"concrete usecase: if the accessTime of a file changed before a snapshot is taken,  a lot of files to be examined.",perf: snapshotDiff to slow down quite a lot,"Currently, if the accessTime of a file changed before a snapshot is taken, this accessTime will be captured in the snapshot, even if there is no other modifications made to this file. This jira is to provide an option to skip capturing accessTime only change to snapshot. Thus snapshotDiff can be done faster.",/,yes
HDFS-14201,dfs.ha.nn.not-become-active-in-safemode,Ability to disallow safemode NN to become active,https://issues.apache.org/jira/browse/HDFS-14201,https://github.com/apache/hadoop/commit/3ab77d9bc9eacfdb218b68988235a921c810b0d1#,feature selection,performance,vague workload:are large number of files and blocks in HDFS,perf: the cluster will be not fully functioning for quite a while,"It can take tens of minutes for a cold started Namenode to get out of safemode, especially when there are large number of files and blocks in HDFS, that means if a Namenode in safemode become active, the cluster will be not fully functioning for quite a while, even if it can while there is some Namenode not in safemode.",/,yes
HDFS-12544,dfs.namenode.snapshotdiff.allow.snap-root-descendant,SnapshotDiff - support diff generation on any snapshot root descendant directory.,https://issues.apache.org/jira/browse/HDFS-12544,https://github.com/apache/hadoop/commit/075dd45a24398dcdcddd60da995f0dc152eee321,feature selection,performance,vague environment: snapshot root directory is very huge,perf: the snapshot diff report generation can take minutes,"The command today only accepts the path that is a snapshot root. There are many deployments where the snapshot root is configured at the higher level directory but the diff report needed is only for a specific directory under the snapshot root. In these cases, the diff report can be filtered for changes pertaining to the directory we are interested in. But when the snapshot root directory is very huge, the snapshot diff report generation can take minutes even if we are interested to know the changes only in a small directory. So, it would be highly performant if the diff report calculation can be limited to only the interesting sub-directory of the snapshot root instead of the whole snapshot root.",/,yes
HDFS-13821,dfs.federation.router.mount-table.cache.enable,RBF: Add dfs.federation.router.mount-table.cache.enable so that users can disable cache.,https://issues.apache.org/jira/browse/HDFS-13821,https://github.com/apache/hadoop/commit/81847392badcd58d934333e7c3b5bf14b4fa1f3f,feature selection,performance,concrete workload: hundreds of millions file access,perf: time consuming,"Disabling Cache is efficient when hundreds of millions files are access, in such scenes ProxyAvgTime only costs 0.0x ms by directly computing remote path,  It caches the request paths, and return the remote location. Maybe request paths are hundreds of millions and the cache is inefficient",/,yes
CASSANDRA-12245,concurrent_materialized_view_builders,Parallelize initial materialized view build,https://issues.apache.org/jira/browse/CASSANDRA-12245,https://github.com/apache/cassandra/commit/4c80eeece37d79f434078224a0504400ae10a20d,thread/task number,performance,concrete Workload: node with lots of data (~3TB),perf: time consuming(building materialized view),"On a node with lots of data (~3TB) building a materialized view takes several weeks, which is not ideal. It's doing this in a single thread.",/,yes
HDFS-13796,dfs.provided.aliasmap.inmemory.server.log,Allow verbosity of InMemoryLevelDBAliasMapServer to be configurable.,https://issues.apache.org/jira/browse/HDFS-13796,https://github.com/apache/hadoop/commit/b1a59b164412fbd9f641a7e992a7d1a3fd0f1a10,feature selection,performance,vague Scale: large numer of nodes,perf: too many log messages,This leads to too many log messages when running with large number of nodes/rpcs to the InMemoryLevelDBAliasMapServer,/,yes
HBASE-6028,hbase.regionserver.compaction.enabled,Start/Stop compactions at region server level,https://issues.apache.org/jira/browse/HBASE-6028,https://github.com/apache/hbase/commit/950d6e6fb0fdcf923aea66e17e617c06ecd5a628#,feature selection,performance,vague workload: when too many regions became eligible for compactions at once,Resource overuse: extremely expensive to run periodic minor / major compactions,"Depending on current server load, it can be extremely expensive to run periodic minor / major compactions. It would be helpful to have a feature where a user could use the shell or a client tool to explicitly cancel an in-progress compactions. This would allow a system to recover when too many regions became eligible for compactions at once",/,yes
HBASE-18846,hbase.regionserver.admin.executorService,Accommodate the hbase-indexer/lily/SEP consumer deploy-typePatch to start a standalone RegionServer that register's itself andoptionally stands up Services.,https://issues.apache.org/jira/browse/HBASE-18846,https://github.com/apache/hbase/commit/456057ef90f152315a7f244141f3fca4ff748336,feature selection,Reliability,concrete usecase: solve bug,/,"This patch adds configuration to disable RegionServer internal Services, Managers, Caches, etc., starting up. By default a RegionServer starts up an Admin and Client Service. ",/,no
HBASE-18846,hbase.regionserver.client.executorService,Accommodate the hbase-indexer/lily/SEP consumer deploy-typePatch to start a standalone RegionServer that register's itself andoptionally stands up Services.,https://issues.apache.org/jira/browse/HBASE-18846,https://github.com/apache/hbase/commit/456057ef90f152315a7f244141f3fca4ff748336,feature selection,Reliability,concrete usecase: solve bug,/,"This patch adds configuration to disable RegionServer internal Services, Managers, Caches, etc., starting up. By default a RegionServer starts up an Admin and Client Service. ",/,no
HBASE-18248,hbase.taskmonitor.max.tasks,Warn if monitored RPC task has been tied up beyond a configurable threshold,https://issues.apache.org/jira/browse/HBASE-18248,https://github.com/apache/hbase/commit/d0941127d424bc76688aec7952388f858d917b14,threshold,Reliability,vague task: rpc task,/,Warn if monitored task has been tied up beyond a configurable threshold. We especially want to do this for RPC tasks,/,no
HBASE-18248,hbase.taskmonitor.expiration.time,Warn if monitored RPC task has been tied up beyond a configurable threshold,https://issues.apache.org/jira/browse/HBASE-18248,https://github.com/apache/hbase/commit/d0941127d424bc76688aec7952388f858d917b14,timeout,Reliability,vague task: rpc task,/,Warn if monitored task has been tied up beyond a configurable threshold. We especially want to do this for RPC tasks.,Balancing flexibility and simplicity: We could restrict this change to them so its completely optional.,no
HBASE-18675,hbase.zookeeper.property.minSessionTimeout,Making {max min}SessionTimeout configurable for MiniZooKeeperCluster,https://issues.apache.org/jira/browse/HBASE-18675,https://github.com/apache/hbase/commit/b5942241cadd79147a5d65457f93aaa39aa7118e,timeout,Reliability,vague when the laptop goes to sleep ,Failure: cluster keep crashing,Right now the mini cluster on application developers laptops keep crashing when the laptop goes to sleep because Zookeeper times out.,/,no
HBASE-18675,hbase.zookeeper.property.maxSessionTimeout,Making {max min}SessionTimeout configurable for MiniZooKeeperCluster,https://issues.apache.org/jira/browse/HBASE-18675,https://github.com/apache/hbase/commit/b5942241cadd79147a5d65457f93aaa39aa7118e,timeout,Reliability,vague when the laptop goes to sleep ,Failure: cluster keep crashing,Right now the mini cluster on application developers laptops keep crashing when the laptop goes to sleep because Zookeeper times out.,/,no
SPARK-23997,spark.sql.bucketing.maxBuckets,Configurable maximum number of buckets,https://issues.apache.org/jira/browse/SPARK-23997,https://github.com/apache/spark/commit/de46df549acee7fda56bb0871f444d2f3b49e582,threshold,Reliability,concrete Workload:save 80TB data,"Failure: driver fail over will not work, java.lang.IllegalArgumentException","We have an 80TB workload and to keep partitions ""manageable"" we do need to use a large number of buckets. While it might seem a lot today it is expected that workloads will continue to increase in size...",/,no
HBASE-19289,hbase.unsafe.stream.capability.enforce,Add flag to disable stream capability enforcement,https://issues.apache.org/jira/browse/HBASE-19289,https://github.com/apache/hbase/commit/2c9ef8a471148ece655b881cc490b6b685d634f4,feature selection,Reliability,concrete Environment: FileSystem doesn't support flush/sync,Failure: java.io.IOException,Sounds like LocalFileSystem doesn't support flush/sync. solutions: telling CommonFSUtils to ignore the check on LocalFileSystem,/,no
SPARK-23182,io.enableTcpKeepAlive,Allow enabling TCP keep alive on the RPC connections,https://issues.apache.org/jira/browse/SPARK-23182,https://github.com/apache/spark/commit/c01152dd22093e9f5d2aa533598e4d4209d30922#,feature selection,Reliability,concrete workload: We run a cluster of ~1000 cores in GCE using preemptible VMs for executors / workers and a standard (non-preemptible) master VM,"Failure: master errors with ""Too many open files"" and can not accept connections anymore","It's currently possible to enable TCP keep alive on the worker / executor, but is not possible to configure on other RPC connections. It's unclear to me why this could be the case. Keep alive is more important for the master to protect it against suddenly departing workers / executors, thus I think it's very important to have it.",/,no
CASSANDRA-14096,repair_session_max_tree_depth,Improve merkle tree size and time on heap,https://issues.apache.org/jira/browse/CASSANDRA-14096,https://github.com/apache/cassandra/commit/b30c8c98a594a5682f6ea1f0b5511463b700b6e8,threshold,Reliability,concrete Environment: memory limit,Failure: OOM,We get out of memory errors while repairing (incremental or full) our keyspace,Balancing flexibility and simplicity: It seems like the simplest solution might be to just repair smaller subranges if you’re ooming on huge merkle trees during repair,no
HBASE-19418,hbase.regionserver.periodicmemstoreflusher.rangeofdelayseconds,configurable range of delay in PeriodicMemstoreFlusher,https://issues.apache.org/jira/browse/HBASE-19418,https://github.com/apache/hbase/commit/668a17988222e7657ff21f945b6dd023bda4276c,interval,Reliability,vague workload: man'y regions and CFs,Failure: overwhelming the filesystem with too many flushes at once,"When RSs(RegionServer) have a LOT of regions and CFs(column family), flushing everything within 5 minutes is not always doable. It might be interesting to be able to increase the RANGE_OF_DELAY. we might end up overwhelming the filesystem with too many flushes at once",/,no
SPARK-26601,spark.sql.broadcastExchange.maxThreadThreshold,Make broadcast-exchange thread pool keepalivetime and maxThreadNumber configurable,https://issues.apache.org/jira/browse/SPARK-26601,https://github.com/apache/spark/commit/126310ca68f2f248ea8b312c4637eccaba2fdc2b,threshold,Reliability,vague use case: slow GC speed,Failure: server(driver) OOM,Thead object do not GC quickly,/,no
CASSANDRA-15260,allocate_tokens_for_local_replication_factor,Add `allocate_tokens_for_local_rf` yaml option for token allocation that doesn't require keyspace knowledge/existence,https://issues.apache.org/jira/browse/CASSANDRA-15260,https://github.com/apache/cassandra/commit/068d2d37c6fbdb60546821c4d408a84161fd1cb6,factor,Reliability,concrete task: adding new datacenters.,Failure: The real keyspace can not be used,"This is problematic in a number of ways. The real keyspace can not be used when adding new datacenters as, in practice, all its nodes need to be up and running before it has the capacity to replicate data into it. New datacenters (or lift-and-shifting a cluster via datacenter migration) therefore has to be done using a dummy keyspace that duplicates the replication strategy+factor of the real keyspace. This gets even more difficult come version 4.0, as the replica factor can not even be defined in new datacenters before those datacenters are up and running. These issues are removed by avoiding the keyspace definition and lookup, and presuming the replica strategy is by datacenter, ie NTS. This can be done with the use of an allocate_tokens_for_dc_rf option.",/,no
HDFS-14652,ha.health-monitor.rpc.connect.max.retries,HealthMonitor connection retry times should be configurable.,https://issues.apache.org/jira/browse/HDFS-14652,https://github.com/apache/hadoop/commit/d086d058d87ecb94fc750ba6f3ccae522658ac8,count,Reliability,vague: some client's burst requests,Failure: This error caused a failover and affects the availability of that cluster,"On our production HDFS cluster, some client's burst requests cause the tcp kernel queue full on NameNode's host,  since the configuration value of ""net.ipv4.tcp_syn_retries"" in our environment is 1, so after 3 seconds, the ZooKeeper Healthmonitor got an connection error",/,no
HBASE-23293,replication.source.shipedits.timeout,[REPLICATION] make ship edits timeout configurable,https://issues.apache.org/jira/browse/HBASE-23293,https://github.com/apache/hbase/commit/17468efb88a99c6b5237575b784b8c105fc5c46,timeout,Reliability,vague workload: some HFile is big enough,Failure: timeout exception may be occurred.,"The default rpc timeout for ReplicationSourceShipper#shipEdits is 60s, when bulkload replication enabled, timeout exception may be occurred. when there is slow N/w and some file copy fails always to copy within the configured time. For example, if some HFile is big enough which may take more than 60 sec to be copied then it will always timedout.",Balancing flexibility and simplicity: Then main goal of this change is providing an alternative valve for increasing the timeout?,no
HBASE-20305,sync.table.do.deletes doDeletes(cmd line option),adding options to skip deletes/puts on target when running SyncTable,https://issues.apache.org/jira/browse/HBASE-20305,https://github.com/apache/hbase/commit/f574fd478211f795aa4a3365ae2ba2d368fea54d,feature selection,Reliability,concrete usecase: clusters with active-active replication got out of sync,uxepected result: data curruption,"We had a situation where two clusters with active-active replication got out of sync, but both had data that should be kept. The tables in question never have data deleted, but ingestion had happened on the two different clusters, some rows had been even updated. This could be solved by adding an option to skip deletes for SyncTable.",/,no
HBASE-20305,sync.table.do.puts doPuts(cmd line option),adding options to skip deletes/puts on target when running SyncTable,https://issues.apache.org/jira/browse/HBASE-20305,https://github.com/apache/hbase/commit/f574fd478211f795aa4a3365ae2ba2d368fea54d,feature selection,Reliability,concrete usecase: clusters with active-active replication got out of sync,uxepected result: data curruption,"We had a situation where two clusters with active-active replication got out of sync, but both had data that should be kept. The tables in question never have data deleted, but ingestion had happened on the two different clusters, some rows had been even updated. This could be solved by adding an option to skip deletes for SyncTable.",/,no
HBASE-12091,hbase.replication.drop.on.deleted.table,Optionally ignore edits for dropped tables for replication.,https://issues.apache.org/jira/browse/HBASE-12091,https://github.com/apache/hbase/commit/0c4fbcc32973e9fb3840cb2b5e397155044c2b0c,feature selection,Reliability,concrete usecase: dropped a table from both the source and the sink,uxepected result: Now all replication is backed up behind these unreplicatable edits,"We just ran into a scenario where we dropped a table from both the source and the sink, but the source still has outstanding edits that now it could not get rid of. Now all replication is backed up behind these unreplicatable edits. We should have an option to ignore edits for tables dropped at the source.",/,no
SPARK-21456,spark.mesos.driver.failoverTimeout,Make the driver failover_timeout CONFIGurable,https://issues.apache.org/jira/browse/SPARK-21456,https://github.com/apache/spark/commit/c42ef953343073a50ef04c5ce848b574ff7f2238,timeout,Reliability,unknow,/,"Instead of setting the driver framework failover_timeout to zero, the failover_timeout will be configurable via a spark-submit config option. The default value will still be zero if the configuration is not set.",/,yes
SPARK-25694,spark.sql.defaultUrlStreamHandlerFactory.enabled,Add a CONFIG for `URL.setURLStreamHandlerFactory`,https://issues.apache.org/jira/browse/SPARK-25694,https://github.com/apache/spark/commit/ee3bd6d76887ccc4961fd520c5d03f7edd3742a,feature selection,Reliability,concrete task: when the users tries to use another custom factories or 3rd party library (trying to set this).,Failure:  This causes exceptions,"URL.setURLStreamHandlerFactory() in SharedState causes URL.openConnection() returns FsUrlConnection object, which is not compatible with HttpURLConnection. This will cause exception when using some third party http library (e.g. scalaj.http).",/,yes
HDFS-13174,dfs.balancer.max-iteration-time,hdfs mover -p /path times out after 20 min.,https://issues.apache.org/jira/browse/HDFS-13174,https://github.com/apache/hadoop/commit/c966a3837af1c1a1c4a441f491b0d76d5c9e5d78,interval,Reliability,concrete usecase: block move was enqueued,Failure: Block move timed out,"Mover could have fail after 20+ minutes if a block move was enqueued for this long, between two DataNodes due to an internal constant that was introduced for Balancer, but affected Mover as well.",/,yes
SPARK-20640,spark.shuffle.registration.timeout,Make rpc timeout and retry for shuffle registration CONFIGurable.,https://issues.apache.org/jira/browse/SPARK-20640,https://github.com/apache/spark/commit/d107b3b910d8f434fb15b663a9db4c2dfe0a9f43#,timeout,Reliability,vague workload: shuffle service is busy transferring large amount of data,"Failure: fail to register with the shuffle service, eventually failing the job","This works well for small workloads but under heavy workload when the shuffle service is busy transferring large amount of data we see significant delay in responding to the registration request, as a result we often see the executors fail to register with the shuffle service, eventually failing the job. We need to make these two parameters configurable.",/,yes
SPARK-20640,spark.shuffle.registration.maxAttempts,Make rpc timeout and retry for shuffle registration CONFIGurable.,https://issues.apache.org/jira/browse/SPARK-20640,https://github.com/apache/spark/commit/d107b3b910d8f434fb15b663a9db4c2dfe0a9f43#,count,Reliability,vague workload: shuffle service is busy transferring large amount of data,"Failure: fail to register with the shuffle service, eventually failing the job","This works well for small workloads but under heavy workload when the shuffle service is busy transferring large amount of data we see significant delay in responding to the registration request, as a result we often see the executors fail to register with the shuffle service, eventually failing the job. We need to make these two parameters configurable.",/,yes
HDFS-13735,dfs.qjournal.http.open.timeout.ms ,Make QJM HTTP URL connection timeout configurable,https://issues.apache.org/jira/browse/HDFS-13735,https://github.com/apache/hadoop/commit/5326a7906de7c86a236d948012cabf3a9ba82310,timeout,Reliability,vague workload:  huge RPC spike,Failure: huge RPC spike could occur with the timeout.,"The HTTP connection is opened while holding the NN read/write lock (see readOp() inside FSEditLogLoaderand how it calls EditLogFileInputStream#init() to open HTTP connection with JNs.), so huge RPC spike could occur with the timeout.",/,yes
HDFS-13735,dfs.qjournal.http.read.timeout.ms,Make QJM HTTP URL connection timeout configurable,https://issues.apache.org/jira/browse/HDFS-13735,https://github.com/apache/hadoop/commit/5326a7906de7c86a236d948012cabf3a9ba82310,timeout,Reliability,vague workload:  huge RPC spike,Failure: huge RPC spike could occur with the timeout.,"The HTTP connection is opened while holding the NN read/write lock (see readOp() inside FSEditLogLoaderand how it calls EditLogFileInputStream#init() to open HTTP connection with JNs.), so huge RPC spike could occur with the timeout.",/,yes
HDFS-12210,dfs.cblock.rpc.timeout.seconds,Block Storage: volume creation times out while creating 3TB volume because of too many containers.,https://issues.apache.org/jira/browse/HDFS-12210,https://github.com/apache/hadoop/commit/9ff136bb02907366352a77a70248cb2e7720883a,timeout,Reliability,concrete workload: creating 3TB volume,Failure: SocketTimeoutException,Volume creation times out while creating 3TB volume because of too many containers. Looking into the logs it can be seen that the volume 614 containers were created before the timeout.,/,yes
SPARK-27023,spark.kubernetes.submission.requestTimeout,Make k8s client timeouts configurable,https://issues.apache.org/jira/browse/SPARK-27023,https://github.com/apache/spark/commit/e9e8bb33ef9ad785473ded168bc85867dad4ee70,timeout,Reliability,vague Environment: in Kubernetes mode,Failure: SocketTimeoutException,SparkSubmit closes with SocketTimeoutException in kubernetes mode,/,yes
SPARK-27023,spark.kubernetes.submission.connectionTimeout,Make k8s client timeouts configurable,https://issues.apache.org/jira/browse/SPARK-27023,https://github.com/apache/spark/commit/e9e8bb33ef9ad785473ded168bc85867dad4ee70,timeout,Reliability,vague Environment: in Kubernetes mode,Failure: SocketTimeoutException,SparkSubmit closes with SocketTimeoutException in kubernetes mode,/,yes
SPARK-27023,spark.kubernetes.driver.requestTimeout,Make k8s client timeouts configurable,https://issues.apache.org/jira/browse/SPARK-27023,https://github.com/apache/spark/commit/e9e8bb33ef9ad785473ded168bc85867dad4ee70,timeout,Reliability,vague Environment: in Kubernetes mode,Failure: SocketTimeoutException,SparkSubmit closes with SocketTimeoutException in kubernetes mode,/,yes
SPARK-27023,spark.kubernetes.driver.connectionTimeout,Make k8s client timeouts configurable,https://issues.apache.org/jira/browse/SPARK-27023,https://github.com/apache/spark/commit/e9e8bb33ef9ad785473ded168bc85867dad4ee70,timeout,Reliability,vague Environment: in Kubernetes mode,Failure: SocketTimeoutException,SparkSubmit closes with SocketTimeoutException in kubernetes mode,/,yes
CASSANDRA-14358,internode_tcp_connect_timeout_in_ms,Partitioned outbound internode TCP connections can occur when nodes restart,https://issues.apache.org/jira/browse/CASSANDRA-14358,https://github.com/apache/cassandra/commit/bfbc5274f2b3a5af2cbbe9679f0e78f1066ef638,timeout,Reliability,concrete Environment: AWS networks (and other stateful firewalls),Failure: UnavailableExceptions,"This should be exceedingly rare, but AWS networks (and other stateful firewalls) apparently suffer from this issue. Defensive settings for protecting Cassandra from true network partitions",Balancing flexibility and simplicity: It's a very rare bug ,yes
SPARK-26118,spark.ui.requestHeaderSize,Make Jetty's requestHeaderSize configurable in Spark,https://issues.apache.org/jira/browse/SPARK-26118,https://github.com/apache/spark/commit/ab61ddb34d58ab5701191c8fd3a24a62f6ebf37b,size,Reliability,concrete Environment: user is a member of many Active Directory user groups,Failure:HTTP 413,too large header size will cause HTTP 413. This issue may occur if the user is a member of many Active Directory user groups. The header size increases together with the number of user groups,Balancing flexibility and simplicity: this is the first time we've heard of someone hitting this limit,yes
HDFS-14082,dfs.federation.router.client.allow-partial-listing,RBF: Add option to fail operations when a subcluster is unavailable,https://issues.apache.org/jira/browse/HDFS-14082,https://github.com/apache/hadoop/commit/f4bd1114ff529e971f9b496ad62a7edca37fdf8d,feature selection,Reliability,vague use case: subcluster is unavailable,uxepected result:  succeed operations like getListing(),"When a subcluster is unavailable, we succeed operations like getListing(). We should add an option to fail the operation if one of the subclusters is unavailable.",/,yes
HDFS-11754,dfs.client.server-defaults.validity.period.ms,Make FsServerDefaults cache configurable,https://issues.apache.org/jira/browse/HDFS-11754,https://github.com/apache/hadoop/commit/53509f295b5274059541565d7216bf98aa35347d,interval,Reliability,vague use case: lockless rpc,uxepected result: lockless rpc,"""On the original jira some concerns were expressed about the lockless rpc for server defaults (I wouldn't care if it was 1min...), so as a middle ground, I'd suggest making the server defaults refresh interval configurable.""",Balancing flexibility and simplicity: so as a middle ground,yes
HDFS-12420,dfs.reformat.disabled,Add an option to disallow 'namenode format -force'.,https://issues.apache.org/jira/browse/HDFS-12420,https://github.com/apache/hadoop/commit/b6942cbe9b8c9469e8c2b64c3268d671f5a43e75,feature selection,Reliability,concrete Task: format a cluster,uxepected result: losing data,"We are just trying to avoid losing data by operator mistake. But from a sysadmin/developer hat on, I do like that fact that I can format a cluster with data. I do that when I test and develop",Balancing flexibility and simplicity: from a sysadmin/developer hat on,yes
SPARK-21247,spark.sql.caseSensitive,[SQL] Type comparison should respect case-sensitive SQL conf,https://issues.apache.org/jira/browse/SPARK-21247,https://github.com/apache/spark/commit/6412ea1759d39a2380c572ec24cfd8ae4f2d81f7,feature selection,Reliability,concrete Task: case-insensitive type comparisions,uxepected result: SET operations fail,"This issue aims to support case-insensitive type comparisions in Set operation. Currently, SET operations fail due to case-sensitive type comparision failure .",/,yes
SPARK-29105,spark.driver.log.allowErasureCoding,SHS may delete driver log file of in progress application,https://issues.apache.org/jira/browse/SPARK-29105,https://github.com/apache/spark/commit/276aaaae8d404975f8701089e9f4dfecd16e0d9,feature selection,Reliability,concrete task: if you just keep writing to the file,uxepected result: SHS may delete driver log file,"HDFS doesn't update the file size reported by the NM if you just keep writing to the file; this makes the SHS believe the file is inactive, and so it may delete it after the configured max age for log files.",/,yes
SPARK-22791,spark.sql.redaction.string.regex,[SQL][SS] Redact Output of Explain,https://issues.apache.org/jira/browse/SPARK-22791,https://github.com/apache/spark/commit/28315714ddef3ddcc192375e98dd5207cf4ecc98,regex,security,concrete task: redact seneitive information.,/,"When calling explain on a query, the output can contain sensitive information. We should provide an admin/user to redact such information.",/,no
HBASE-18875,hbase.thrift.readonly,Thrift server supports read-only mode,https://issues.apache.org/jira/browse/HBASE-18875,https://github.com/apache/hbase/commit/cfb6a54f69b1d847142afab56bdc1504638f118d,feature selection,security,unknow,/,"Provide option for thrift server to support read-only mode.To start the thrift server, use the -ro option or set hbase.thrift.readonly to true.",/,no
HDFS-13996,DFS_WEBHDFS_ACL_PERMISSION_PATTERN_KEY (dfs.webhdfs.acl.provider.permission.pattern),Make HttpFS' ACLs RegEx configurable,https://issues.apache.org/jira/browse/HDFS-13996,https://github.com/apache/hadoop/commit/8fe85af63b37f2f61269e8719e5b6287f30bb0b3,ACLs RegEx,security,unknow,/,Part of HDFS-5608 added support for GET/SET ACLs over WebHDFS. This currently identifies the passed arguments via a hard-coded regex that mandates certain group and user naming styles.,/,yes
SPARK-28055,REDACTION_REPLACEMENT_TEXT,Add delegation token custom AdminClient configuration,https://issues.apache.org/jira/browse/SPARK-28055,https://github.com/apache/spark/commit/d47c219f94f478b4b90bf6f74f78762ea301ebf,regex,security,unknow,/,/,/,yes
SPARK-29975,CONFIG_DIM,introduce --CONFIG_DIM directive,https://issues.apache.org/jira/browse/SPARK-29975,https://github.com/apache/spark/commit/e2f056f4a89b1bd9864be8c111d39af6558c839,feature selection,testability,unknow,/,allow the sql test files to specify different dimensions of config sets during testing.,/,no
SPARK-23361,spark.security.credentials.renewalRatio,[YARN] Allow AM to restart after initial tokens expire.,https://issues.apache.org/jira/browse/SPARK-23361,https://github.com/apache/spark/commit/5fa438471110afbf4e2174df449ac79e292501f8,ratio,testability,vague task: helps with testing,/,"I also made two currently hardcoded values (the renewal time ratio, and the retry wait) configurable; while this probably never needs to be set the retry wait) configurable; while this probably never needs to be set by anyone in a production environment, it helps with testing; that's also why they're not documented.",Balancing flexibility and simplicity:while this probably never needs to be set by anyone in a production environment,no
SPARK-23361,spark.security.credentials.retryWait,[YARN] Allow AM to restart after initial tokens expire.,https://issues.apache.org/jira/browse/SPARK-23361,https://github.com/apache/spark/commit/5fa438471110afbf4e2174df449ac79e292501f8,timeout,testability,vague task: helps with testing,/,"I also made two currently hardcoded values (the renewal time ratio, and the retry wait) configurable; while this probably never needs to be set the retry wait) configurable; while this probably never needs to be set by anyone in a production environment, it helps with testing; that's also why they're not documented.",Balancing flexibility and simplicity: while this probably never needs to be set by anyone in a production environment,no
SPARK-22554,spark.python.use.daemon,[PYTHON] Add a CONFIG to control if PySpark should use daemon or not for workers,https://issues.apache.org/jira/browse/SPARK-22554,https://github.com/apache/spark/commit/57c5514de9dba1c14e296f85fb13fef23ce8c73f,feature selection,testability,vague task: test Windows specific issue.,/,It makes easier to test Windows specific issue.,/,no
HDFS-12496,dfs.qjm.operations.timeout,Make QuorumJournalManager timeout properties configurable,https://issues.apache.org/jira/browse/HDFS-12496,https://github.com/apache/hadoop/commit/b9e423fa8d30ea89244f6ec018a8064cc87d94a9,timeout,unknow,unknow,/,Make QuorumJournalManager timeout properties configurable using a common key.,"Balancing flexibility and simplicity: Since these don't occur during normal operation, we can use rather lengthy timeouts, and don't need to make them configurable",yes
HDFS-13181,dfs.disk.balancer.plan.valid.interval,Add a configuration for valid plan hours instead of constant 24 hours in the code.,https://issues.apache.org/jira/browse/HDFS-13181,https://github.com/apache/hadoop/commit/1cc9a58ddad8a02db0ec5a014f9de417eec1b8dd,interval,unknow,unknow,/,"Add a configuration for valid plan hours, instead of constant 24 hours in the code.",/,yes